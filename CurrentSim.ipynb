{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y-7MqF5_1Je"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Old Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jsdd3do_1Jf"
      },
      "outputs": [],
      "source": [
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model = 'gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T/n\n",
        "        self.model = model\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # Use the Heston volatility path\n",
        "            vol_path = self.vol(self.sigma)\n",
        "            innovations = np.random.normal(0, 1, self.n)\n",
        "            stock_prices = np.zeros(self.n)\n",
        "            stock_prices[0] = self.s0\n",
        "\n",
        "            for i in range(1, self.n):\n",
        "                stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                    (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "                )\n",
        "            return stock_prices # Implement Heston model volatility here\n",
        "\n",
        "\n",
        "    def heston_model_sim(S0, v0, rho, kappa, theta, sigma,T, N, M):\n",
        "\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "         - S0, v0: initial parameters for asset and variance\n",
        "         - rho   : correlation between asset returns and variance\n",
        "         - kappa : rate of mean reversion in variance process\n",
        "         - theta : long-term mean of variance process\n",
        "         - sigma : vol of vol / volatility of variance process\n",
        "         - T     : time of simulation\n",
        "         - N     : number of time steps\n",
        "         - M     : number of scenarios / simulations\n",
        "\n",
        "        Outputs:\n",
        "        - asset prices over time (numpy array)\n",
        "        - variance over time (numpy array)\n",
        "        \"\"\"\n",
        "        # initialise other parameters\n",
        "        dt = T/N\n",
        "        mu = np.array([0,0])\n",
        "        cov = np.array([[1,rho],\n",
        "                        [rho,1]])\n",
        "\n",
        "        # arrays for storing prices and variances\n",
        "        S = np.full(shape=(N+1,M), fill_value=S0)\n",
        "        v = np.full(shape=(N+1,M), fill_value=v0)\n",
        "\n",
        "        # sampling correlated brownian motions under risk-neutral measure\n",
        "        Z = np.random.multivariate_normal(mu, cov, (N,M))\n",
        "\n",
        "        for i in range(1,N+1):\n",
        "            S[i] = S[i-1] * np.exp( (r - 0.5*v[i-1])*dt + np.sqrt(v[i-1] * dt) * Z[i-1,:,0] )\n",
        "            v[i] = np.maximum(v[i-1] + kappa*(theta-v[i-1])*dt + sigma*np.sqrt(v[i-1]*dt)*Z[i-1,:,1],0)\n",
        "\n",
        "        return S, v\n",
        "\n",
        "    def simulate(self):\n",
        "        innovations = np.random.normal(0, 1, self.n)\n",
        "        stock_prices = np.zeros(self.n)\n",
        "        stock_prices[0] = self.s0\n",
        "\n",
        "        for i in range(1, self.n):\n",
        "            stock_prices[i] = stock_prices[i-1] * np.exp((self.r - 0.5 * self.sigma**2) * self.dt + self.sigma * np.sqrt(self.dt) * innovations[i])\n",
        "        return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N51TCa4m_1Jf",
        "outputId": "4c0419b4-0097-4a3a-8e7b-5f18cfca562c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QLBS Option Price at t=0: -0.0\n"
          ]
        }
      ],
      "source": [
        "class simulation():\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Parameters for simulation\n",
        "    T_steps = 50         # Number of time steps (T)\n",
        "    K_paths = 1000       # Number of Monte Carlo paths (K)\n",
        "    T_total = 1.0        # Total time horizon (years)\n",
        "    dt = T_total / T_steps\n",
        "    S0 = 100             # Initial stock price\n",
        "    r = 0.05             # Risk-free rate\n",
        "    sigma = 0.2          # Volatility\n",
        "    strike = 100         # Strike price (Z in pseudocode)\n",
        "    lambda_param = 0.5   # λ parameter\n",
        "\n",
        "    def simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt):\n",
        "      \"\"\"\n",
        "      Simulate stock prices using a geometric Brownian motion.\n",
        "      Returns an array S of shape (T_steps+1, K_paths).\n",
        "      \"\"\"\n",
        "      S = np.zeros((T_steps + 1, K_paths))\n",
        "      S[0] = S0\n",
        "      for t in range(1, T_steps + 1):\n",
        "          z = np.random.standard_normal(K_paths)\n",
        "          S[t] = S[t - 1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z)\n",
        "      return S\n",
        "\n",
        "    # Simulate the stock paths\n",
        "    S = simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt)\n",
        "\n",
        "    # %% [code]\n",
        "    # Compute the state variable X.\n",
        "    # For example, one may take X as the log of S (common in QLBS literature).\n",
        "    X = np.log(S)\n",
        "\n",
        "    # %% [code]\n",
        "    # Define N basis functions; here we use a simple polynomial basis (constant, linear, quadratic)\n",
        "    def basis_function_1(x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def basis_function_2(x):\n",
        "        return x\n",
        "\n",
        "    def basis_function_3(x):\n",
        "        return x**2\n",
        "\n",
        "    basis_functions = [basis_function_1, basis_function_2, basis_function_3]\n",
        "    N_basis = len(basis_functions)\n",
        "\n",
        "    # Create the feature matrix phi with dimensions (T_steps+1, K_paths, N_basis)\n",
        "    phi = np.zeros((T_steps + 1, K_paths, N_basis))\n",
        "    for t in range(T_steps + 1):\n",
        "        for n, func in enumerate(basis_functions):\n",
        "            phi[t, :, n] = func(X[t])\n",
        "\n",
        "    # %% [code]\n",
        "    # Initialize arrays for the variables computed in the backward recursion\n",
        "    # a_star, Pi, R_star, and Q_star each have shape (T_steps+1, K_paths)\n",
        "    a_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Pi = np.zeros((T_steps + 1, K_paths))\n",
        "    R_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Q_star = np.zeros((T_steps + 1, K_paths))\n",
        "\n",
        "    # Terminal conditions at t = T_steps\n",
        "    # Compute the option payoff: max(strike - S_T, 0)\n",
        "    Pi[T_steps] = np.maximum(strike - S[T_steps], 0)\n",
        "    # Center the terminal portfolio (subtracting the mean)\n",
        "    Pi_hat_T = Pi[T_steps] - np.mean(Pi[T_steps])\n",
        "    # Set terminal action to zero\n",
        "    a_star[T_steps] = 0\n",
        "    # Terminal risk measure (here using the variance of the payoff; note that var is a scalar)\n",
        "    R_star[T_steps] = -lambda_param * np.var(Pi[T_steps])\n",
        "    # Terminal Q-value (again, note that the λ·Var term is constant across paths)\n",
        "    Q_star[T_steps] = -Pi[T_steps] - lambda_param * np.var(Pi[T_steps])\n",
        "\n",
        "    # %% [code]\n",
        "    # Backward recursion from t = T_steps-1 to t = 0\n",
        "    for t in range(T_steps - 1, -1, -1):\n",
        "        # === Compute a_star[t] as in (44) ===\n",
        "        # Placeholder: in a full implementation, you would estimate a regression of the continuation value\n",
        "        # on the features phi[t]. Here we set it to zero.\n",
        "        a_star[t] = 0  # Replace with actual computation using phi[t]\n",
        "\n",
        "        # === Compute Pi[t] as in (29) ===\n",
        "        # Placeholder: here we mimic the terminal payoff but an actual update rule may be more complex.\n",
        "        Pi[t] = np.maximum(strike - S[t], 0)  # Replace with your QLBS update rule\n",
        "\n",
        "        # === Compute R_star[t] as in (41) ===\n",
        "        # Placeholder: you might compute a risk measure update here.\n",
        "        R_star[t] = R_star[t + 1]  # Replace with actual computation\n",
        "\n",
        "        # === Compute Q_star[t] as in (45) ===\n",
        "        # Placeholder: combine the immediate cost and risk measure.\n",
        "        Q_star[t] = -Pi[t] - lambda_param * np.var(Pi[t])  # Replace with the proper formula\n",
        "\n",
        "    # %% [code]\n",
        "    # Calculate the QLBS option price at t = 0:\n",
        "    # QLBS_price = - (1/K_paths) * sum_{k=1}^{K_paths} Q_star[0, k]\n",
        "    QLBS_price = -np.mean(Q_star[0])\n",
        "    print(\"QLBS Option Price at t=0:\", QLBS_price)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## stock process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtbPF1rEB7NS",
        "outputId": "27a57fa4-92f7-4cd6-84e4-95b96c95f1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Option Price (Heston): 10.328763121630008\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model='gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T / n\n",
        "        self.model = model\n",
        "        self.sigma = sigma  # For GBM, sigma is constant; for Heston, sigma is the initial volatility.\n",
        "        # initial variance for Heston model\n",
        "        if self.model == 'heston':\n",
        "            self.kappa = 2.0         # speed of mean reversion\n",
        "            self.theta = sigma**2    # long-run variance (theta)\n",
        "            self.xi = 0.1            # volatility of volatility\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # initialize variance process\n",
        "            v = np.zeros(self.n)\n",
        "            v[0] = sigma**2 \n",
        "            for i in range(1, self.n):\n",
        "                # Euler-Maruyama update for variance\n",
        "                dv = self.kappa * (self.theta - v[i-1]) * self.dt + self.xi * np.sqrt(max(v[i-1], 0)) * np.sqrt(self.dt) * np.random.normal()\n",
        "                v[i] = v[i-1] + dv\n",
        "                # Ensure non-negativity (using full truncation)\n",
        "                v[i] = max(v[i], 0)\n",
        "            # Return the volatility (sqrt of variance)\n",
        "            return np.sqrt(v)\n",
        "\n",
        "    def simulate(self):\n",
        "        # no need to check model here, as vol() handles it\n",
        "        vol_path = self.vol(self.sigma)\n",
        "        innovations = np.random.normal(0, 1, self.n)\n",
        "        stock_prices = np.zeros(self.n)\n",
        "        stock_prices[0] = self.s0\n",
        "\n",
        "        for i in range(1, self.n):\n",
        "            stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "            )\n",
        "        return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        # European call option payoff at maturity\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Using the Heston model\n",
        "    heston_stock = stock(s0=100, r=0.05, sigma=0.2, T=1.0, n=250, model='heston')\n",
        "    n_sims = 10000\n",
        "    option_prices = []\n",
        "    for i in range(n_sims):\n",
        "        price_path = heston_stock.simulate()\n",
        "        option_prices.append(heston_stock.option_price(K=100))\n",
        "    price = np.mean(option_prices)\n",
        "\n",
        "    print(\"Option Price (Heston):\", price)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "ZciZCY_VbG0J",
        "outputId": "f3c4d260-2013-430f-bec7-5666a63430e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/261.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-5.2.1\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'redis' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-849d351a06e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Connect to Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRedis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localhost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6379\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set a value in Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'redis' is not defined"
          ]
        }
      ],
      "source": [
        "# Install a pip package in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install redis\n",
        "\n",
        "# Connect to Redis\n",
        "r = redis.Redis(host='localhost', port=6379, db=0)\n",
        "\n",
        "# Set a value in Redis\n",
        "r.set('Stock', 'NVDA', 'Price', '99.60')\n",
        "\n",
        "# Get a value from Redis\n",
        "stock = r.get('stock')\n",
        "print(stock)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## env setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRBVQTcZu20Z",
        "outputId": "5ba5ca8c-39eb-43f7-e9fb-365e1749ad9e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "##############################\n",
        "# Environment Implementation #\n",
        "##############################\n",
        "\n",
        "class HedgingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom hedging environment.\n",
        "\n",
        "    - Dynamics: Underlying asset follows a geometric Brownian motion.\n",
        "      If stochastic_vol==True, volatility updates via a simplified SABR model.\n",
        "    - Reward: At each timestep the reward is the change in wealth adjusted by transaction costs,\n",
        "      and a risk penalty scaling the variance (approximated here via the squared change).\n",
        "    - At t=0, the agent buys the replicating portfolio.\n",
        "    - At maturity, the terminal payoff of a European call is subtracted.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 T=1.0,            # time horizon (e.g. 1 year)\n",
        "                 n_steps=50,       # number of timesteps per episode\n",
        "                 S0=100.0,         # initial asset price\n",
        "                 sigma0=0.2,       # initial volatility\n",
        "                 kappa=0.001,      # transaction cost parameter\n",
        "                 risk_aversion=0.01,  # risk–penalty parameter lambda\n",
        "                 strike=100.0,     # strike price of the option (European call)\n",
        "                 nu=0.1,           # vol-of-vol (for SABR dynamics)\n",
        "                 rho=-0.3,         # correlation between asset and volatility shocks\n",
        "                 stochastic_vol=True):  # whether to use stochastic volatility dynamics\n",
        "        super(HedgingEnv, self).__init__()\n",
        "        self.T = T\n",
        "        self.n_steps = n_steps\n",
        "        self.dt = T / n_steps\n",
        "        self.S0 = S0\n",
        "        self.sigma0 = sigma0\n",
        "        self.kappa = kappa\n",
        "        self.risk_aversion = risk_aversion\n",
        "        self.strike = strike\n",
        "        self.nu = nu\n",
        "        self.rho = rho\n",
        "        self.stochastic_vol = stochastic_vol\n",
        "\n",
        "        # Continuous action: hedge position. (We assume it can be any real number.)\n",
        "        # I think it makes sense to bound the action space to a more reasonable range, presumably between (-1 and 1) for the ranges of delta\n",
        "        self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
        "        # State: we use [S, sigma, previous hedge, normalized time]\n",
        "        # probably should be bounded as well\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.S = self.S0\n",
        "        self.sigma = self.sigma0\n",
        "        self.a_prev = 0.0  # initial hedge (no position)\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, 0.0], dtype=np.float32)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        action = float(action)  # ensure scalar\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        # Compute hedge adjustment cost and change in wealth\n",
        "        trade = self.a_prev - action  # change in hedge\n",
        "        delta_w = self.S * trade - self.kappa * abs(self.S * trade)\n",
        "        risk_penalty = self.risk_aversion * (delta_w ** 2)\n",
        "        reward = delta_w - risk_penalty\n",
        "\n",
        "        if self.t == 0:\n",
        "            # At initial step, buying replicating portfolio\n",
        "            reward = - self.S * action - self.kappa * abs(self.S * action)\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        # Update underlying dynamics:\n",
        "        Z1 = np.random.normal()\n",
        "        # it looks to me like in either case we've forgotten to have a risk-free rate in the dynamics\n",
        "        if self.stochastic_vol:\n",
        "            # Stochastic volatility update using a simplified SABR-like model\n",
        "            Z2 = np.random.normal()\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            self.sigma = self.sigma + self.nu * self.sigma * np.sqrt(self.dt) * (\n",
        "                self.rho * Z1 + np.sqrt(1 - self.rho ** 2) * Z2)\n",
        "            # Ensure sigma stays positive\n",
        "            self.sigma = max(self.sigma, 1e-3)\n",
        "        else:\n",
        "            # Constant volatility dynamics\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            # sigma remains constant\n",
        "            self.sigma = self.sigma0\n",
        "\n",
        "        self.a_prev = action\n",
        "        time_frac = self.t / self.n_steps\n",
        "        if self.t >= self.n_steps:\n",
        "            # Terminal reward: liquidate position and subtract option payoff.\n",
        "            option_payoff = max(self.S - self.strike, 0)\n",
        "            final_delta = self.S * action - self.kappa * abs(self.S * action) - option_payoff\n",
        "            reward = final_delta\n",
        "            done = True\n",
        "\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, time_frac], dtype=np.float32)\n",
        "        return self.state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# behavioral cloning using BSM model analytical solution\n",
        "env = HedgingEnv(stochastic_vol=False)\n",
        "n_paths = 1000\n",
        "expert_data = []\n",
        "for _ in range(n_paths):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        S, sigma, a_prev, t = state\n",
        "        tau = 1.0 - t\n",
        "        d1 = (np.log(S / env.strike) + 0.5 * sigma**2 * tau) / (sigma * np.sqrt(tau) + 1e-8) # term is there for numerical stability        \n",
        "        expert_action = stats.norm.cdf(d1)\n",
        "        \n",
        "        expert_data.append((state.copy(), np.array([expert_action], dtype=np.float32)))\n",
        "        state, _, done, _ = env.step(expert_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BehavioralCloningModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(BehavioralCloningModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_behavioral_cloning(model, data, epochs=100, batch_size=32, lr=0.001):\n",
        "    state_dim = data[0][0].shape[0]\n",
        "    action_dim = data[0][1].shape[0]\n",
        "    model = BehavioralCloningModel(state_dim, action_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    dataset = [(torch.tensor(state, dtype=torch.float32), torch.tensor(action, dtype=torch.float32)) for state, action in data]\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(dataset)\n",
        "        for i in range(0, len(dataset), batch_size):\n",
        "            batch = dataset[i:i + batch_size]\n",
        "            states, actions = zip(*batch)\n",
        "            states = torch.stack(states)\n",
        "            actions = torch.stack(actions)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(states)\n",
        "            loss = loss_fn(outputs, actions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item()}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# can then use the weights from the behavioral cloning model to initialize the actor in the RL agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################################\n",
        "# Helper: Monte Carlo Simulation #\n",
        "##################################\n",
        "\n",
        "def simulate_paths(env, N=1000):\n",
        "    \"\"\"\n",
        "    Simulate N Monte Carlo paths using the environment dynamics.\n",
        "    Each episode represents one path.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "    returns = []\n",
        "    for _ in range(N):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_states = [state]\n",
        "        while not done:\n",
        "            # For simulation here, use a fixed policy (e.g., do nothing, a=0)\n",
        "            action = 0.0\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_states.append(state)\n",
        "        paths.append(episode_states)\n",
        "        returns.append(episode_reward)\n",
        "    return paths, returns\n",
        "\n",
        "#####################################\n",
        "# Tabular Q-Learning Implementation #\n",
        "#####################################\n",
        "\n",
        "class TabularQAgent:\n",
        "    def __init__(self, env, price_bins=50, action_low=-1.0, action_high=1.0, action_step=0.01,\n",
        "                 alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.price_bins = np.linspace(0, 2 * env.S0, price_bins)\n",
        "        self.n_time = env.n_steps + 1  # include terminal step\n",
        "        self.actions = np.arange(action_low, action_high + action_step, action_step)\n",
        "        self.n_actions = len(self.actions)\n",
        "        # Q-table indexed by (price_index, time_index)\n",
        "        self.Q = np.zeros((price_bins, self.n_time, self.n_actions))\n",
        "        # Hyperparameters\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def discretize_state(self, state):\n",
        "        # For tabular Q-learning we use only the price S and discrete time.\n",
        "        S, _, _, t_frac = state\n",
        "        price_idx = np.digitize(S, self.price_bins) - 1\n",
        "        price_idx = np.clip(price_idx, 0, len(self.price_bins)-1)\n",
        "        time_idx = int(t_frac * self.env.n_steps)\n",
        "        time_idx = np.clip(time_idx, 0, self.n_time-1)\n",
        "        return price_idx, time_idx\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        price_idx, time_idx = self.discretize_state(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explore: choose a random discrete action index.\n",
        "            a_idx = np.random.randint(self.n_actions)\n",
        "        else:\n",
        "            a_idx = np.argmax(self.Q[price_idx, time_idx])\n",
        "        return self.actions[a_idx], a_idx\n",
        "\n",
        "    def update(self, state, action_idx, reward, next_state, done):\n",
        "        s_idx = self.discretize_state(state)\n",
        "        next_s_idx = self.discretize_state(next_state)\n",
        "        price_idx, time_idx = s_idx\n",
        "        n_price_idx, n_time_idx = next_s_idx\n",
        "\n",
        "        best_next = 0 if done else np.max(self.Q[n_price_idx, n_time_idx])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.Q[price_idx, time_idx, action_idx]\n",
        "        self.Q[price_idx, time_idx, action_idx] += self.alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "def train_tabular_q_learning(env, n_episodes=1000):\n",
        "    agent = TabularQAgent(env)\n",
        "    rewards_per_episode = []\n",
        "    for ep in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            action, action_idx = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update(state, action_idx, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        agent.decay_epsilon()\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[Tabular Q] Episode {ep}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###################################\n",
        "# DDPG Implementation (Lillicrap 2019) #\n",
        "###################################\n",
        "\n",
        "# Define Actor and Critic networks for DDPG\n",
        "class DDPGActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = self.out(x)\n",
        "        return action\n",
        "\n",
        "class DDPGCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.fc1(torch.cat([state, action], dim=1)))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_value = self.out(x)\n",
        "        return q_value\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "def train_ddpg(env, n_episodes=500, batch_size=64, gamma=0.99, tau=0.005):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = DDPGActor(state_dim, action_dim)\n",
        "    critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor = DDPGActor(state_dim, action_dim)\n",
        "    target_critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "    buffer = ReplayBuffer()\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action = actor(state_tensor).detach().numpy()[0]\n",
        "            # Add exploratory noise (Gaussian)\n",
        "            action = action + np.random.normal(0, 0.1, size=action_dim)\n",
        "            next_state, reward, done, _ = env.step(action[0])\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Update if enough samples are available\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states_tensor = torch.FloatTensor(states)\n",
        "                actions_tensor = torch.FloatTensor(actions)\n",
        "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "                next_states_tensor = torch.FloatTensor(next_states)\n",
        "                dones_tensor = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "                # Critic update\n",
        "                next_actions = target_actor(next_states_tensor)\n",
        "                target_q = target_critic(next_states_tensor, next_actions)\n",
        "                expected_q = rewards_tensor + gamma * (1 - dones_tensor) * target_q\n",
        "                current_q = critic(states_tensor, actions_tensor)\n",
        "                critic_loss = nn.MSELoss()(current_q, expected_q.detach())\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                # Actor update\n",
        "                actor_loss = -critic(states_tensor, actor(states_tensor)).mean()\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                # Update target networks\n",
        "                for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "                for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[DDPG] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###########################################\n",
        "# PPO Implementation (Schulman et al. 2017) #\n",
        "###########################################\n",
        "\n",
        "class PPOActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(PPOActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Log_std parameter for Gaussian exploration\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "class PPOCritic(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_size=64):\n",
        "        super(PPOCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        value = self.out(x)\n",
        "        return value\n",
        "\n",
        "def train_ppo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4, update_epochs=5):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        # Compute rewards-to-go\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        # Normalize returns\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # Convert trajectory to tensors\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        # Multiple update epochs on the same trajectory\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            # PPO clipped objective\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[PPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "#############################################\n",
        "# GRPO Implementation (DeepSeek 2024 variant)#\n",
        "#############################################\n",
        "\n",
        "# For this demonstration, we assume GRPO augments the PPO update with an additional gradient–reward regularization term.\n",
        "# The implementation here is similar in structure to PPO with an extra term that penalizes large deviations in the gradient.\n",
        "def train_grpo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4,\n",
        "               update_epochs=5, grad_reg_coeff=0.1):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            # Additional gradient regularization (a penalty on the norm of the actor gradient)\n",
        "            optimizer.zero_grad()\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "            loss.backward()\n",
        "            grad_norm = 0.0\n",
        "            for param in actor.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm += param.grad.data.norm(2)\n",
        "            # Add penalty term\n",
        "            loss = loss + grad_reg_coeff * grad_norm\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[GRPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "####################################\n",
        "# Main comparison and experiment   #\n",
        "####################################\n",
        "\n",
        "def main():\n",
        "    # Number of Monte Carlo paths is given by the number of episodes we simulate.\n",
        "    n_mc = 500  # for demonstration we use 500 episodes per algorithm\n",
        "    print(\"======== Running Tabular Q-Learning (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_q = train_tabular_q_learning(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ddpg = train_ddpg(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ppo = train_ppo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_grpo = train_grpo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for constant volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Constant Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Now repeat with stochastic volatility dynamics.\n",
        "    print(\"\\n======== Running Tabular Q-Learning (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_q_stoch = train_tabular_q_learning(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ddpg_stoch = train_ddpg(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ppo_stoch = train_ppo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_grpo_stoch = train_grpo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for stochastic volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q_stoch, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg_stoch, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo_stoch, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo_stoch, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Stochastic Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fin580",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
