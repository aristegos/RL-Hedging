{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y-7MqF5_1Je"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Old Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jsdd3do_1Jf"
      },
      "outputs": [],
      "source": [
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model = 'gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T/n\n",
        "        self.model = model\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # Use the Heston volatility path\n",
        "            vol_path = self.vol(self.sigma)\n",
        "            innovations = np.random.normal(0, 1, self.n)\n",
        "            stock_prices = np.zeros(self.n)\n",
        "            stock_prices[0] = self.s0\n",
        "\n",
        "            for i in range(1, self.n):\n",
        "                stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                    (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "                )\n",
        "            return stock_prices # Implement Heston model volatility here\n",
        "\n",
        "\n",
        "    def heston_model_sim(S0, v0, rho, kappa, theta, sigma,T, N, M):\n",
        "\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "         - S0, v0: initial parameters for asset and variance\n",
        "         - rho   : correlation between asset returns and variance\n",
        "         - kappa : rate of mean reversion in variance process\n",
        "         - theta : long-term mean of variance process\n",
        "         - sigma : vol of vol / volatility of variance process\n",
        "         - T     : time of simulation\n",
        "         - N     : number of time steps\n",
        "         - M     : number of scenarios / simulations\n",
        "\n",
        "        Outputs:\n",
        "        - asset prices over time (numpy array)\n",
        "        - variance over time (numpy array)\n",
        "        \"\"\"\n",
        "        # initialise other parameters\n",
        "        dt = T/N\n",
        "        mu = np.array([0,0])\n",
        "        cov = np.array([[1,rho],\n",
        "                        [rho,1]])\n",
        "\n",
        "        # arrays for storing prices and variances\n",
        "        S = np.full(shape=(N+1,M), fill_value=S0)\n",
        "        v = np.full(shape=(N+1,M), fill_value=v0)\n",
        "\n",
        "        # sampling correlated brownian motions under risk-neutral measure\n",
        "        Z = np.random.multivariate_normal(mu, cov, (N,M))\n",
        "\n",
        "        for i in range(1,N+1):\n",
        "            S[i] = S[i-1] * np.exp( (r - 0.5*v[i-1])*dt + np.sqrt(v[i-1] * dt) * Z[i-1,:,0] )\n",
        "            v[i] = np.maximum(v[i-1] + kappa*(theta-v[i-1])*dt + sigma*np.sqrt(v[i-1]*dt)*Z[i-1,:,1],0)\n",
        "\n",
        "        return S, v\n",
        "\n",
        "    def simulate(self):\n",
        "        innovations = np.random.normal(0, 1, self.n)\n",
        "        stock_prices = np.zeros(self.n)\n",
        "        stock_prices[0] = self.s0\n",
        "\n",
        "        for i in range(1, self.n):\n",
        "            stock_prices[i] = stock_prices[i-1] * np.exp((self.r - 0.5 * self.sigma**2) * self.dt + self.sigma * np.sqrt(self.dt) * innovations[i])\n",
        "        return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N51TCa4m_1Jf",
        "outputId": "4c0419b4-0097-4a3a-8e7b-5f18cfca562c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QLBS Option Price at t=0: -0.0\n"
          ]
        }
      ],
      "source": [
        "class simulation():\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Parameters for simulation\n",
        "    T_steps = 50         # Number of time steps (T)\n",
        "    K_paths = 1000       # Number of Monte Carlo paths (K)\n",
        "    T_total = 1.0        # Total time horizon (years)\n",
        "    dt = T_total / T_steps\n",
        "    S0 = 100             # Initial stock price\n",
        "    r = 0.05             # Risk-free rate\n",
        "    sigma = 0.2          # Volatility\n",
        "    strike = 100         # Strike price (Z in pseudocode)\n",
        "    lambda_param = 0.5   # λ parameter\n",
        "\n",
        "    def simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt):\n",
        "      \"\"\"\n",
        "      Simulate stock prices using a geometric Brownian motion.\n",
        "      Returns an array S of shape (T_steps+1, K_paths).\n",
        "      \"\"\"\n",
        "      S = np.zeros((T_steps + 1, K_paths))\n",
        "      S[0] = S0\n",
        "      for t in range(1, T_steps + 1):\n",
        "          z = np.random.standard_normal(K_paths)\n",
        "          S[t] = S[t - 1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z)\n",
        "      return S\n",
        "\n",
        "    # Simulate the stock paths\n",
        "    S = simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt)\n",
        "\n",
        "    # %% [code]\n",
        "    # Compute the state variable X.\n",
        "    # For example, one may take X as the log of S (common in QLBS literature).\n",
        "    X = np.log(S)\n",
        "\n",
        "    # %% [code]\n",
        "    # Define N basis functions; here we use a simple polynomial basis (constant, linear, quadratic)\n",
        "    def basis_function_1(x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def basis_function_2(x):\n",
        "        return x\n",
        "\n",
        "    def basis_function_3(x):\n",
        "        return x**2\n",
        "\n",
        "    basis_functions = [basis_function_1, basis_function_2, basis_function_3]\n",
        "    N_basis = len(basis_functions)\n",
        "\n",
        "    # Create the feature matrix phi with dimensions (T_steps+1, K_paths, N_basis)\n",
        "    phi = np.zeros((T_steps + 1, K_paths, N_basis))\n",
        "    for t in range(T_steps + 1):\n",
        "        for n, func in enumerate(basis_functions):\n",
        "            phi[t, :, n] = func(X[t])\n",
        "\n",
        "    # %% [code]\n",
        "    # Initialize arrays for the variables computed in the backward recursion\n",
        "    # a_star, Pi, R_star, and Q_star each have shape (T_steps+1, K_paths)\n",
        "    a_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Pi = np.zeros((T_steps + 1, K_paths))\n",
        "    R_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Q_star = np.zeros((T_steps + 1, K_paths))\n",
        "\n",
        "    # Terminal conditions at t = T_steps\n",
        "    # Compute the option payoff: max(strike - S_T, 0)\n",
        "    Pi[T_steps] = np.maximum(strike - S[T_steps], 0)\n",
        "    # Center the terminal portfolio (subtracting the mean)\n",
        "    Pi_hat_T = Pi[T_steps] - np.mean(Pi[T_steps])\n",
        "    # Set terminal action to zero\n",
        "    a_star[T_steps] = 0\n",
        "    # Terminal risk measure (here using the variance of the payoff; note that var is a scalar)\n",
        "    R_star[T_steps] = -lambda_param * np.var(Pi[T_steps])\n",
        "    # Terminal Q-value (again, note that the λ·Var term is constant across paths)\n",
        "    Q_star[T_steps] = -Pi[T_steps] - lambda_param * np.var(Pi[T_steps])\n",
        "\n",
        "    # %% [code]\n",
        "    # Backward recursion from t = T_steps-1 to t = 0\n",
        "    for t in range(T_steps - 1, -1, -1):\n",
        "        # === Compute a_star[t] as in (44) ===\n",
        "        # Placeholder: in a full implementation, you would estimate a regression of the continuation value\n",
        "        # on the features phi[t]. Here we set it to zero.\n",
        "        a_star[t] = 0  # Replace with actual computation using phi[t]\n",
        "\n",
        "        # === Compute Pi[t] as in (29) ===\n",
        "        # Placeholder: here we mimic the terminal payoff but an actual update rule may be more complex.\n",
        "        Pi[t] = np.maximum(strike - S[t], 0)  # Replace with your QLBS update rule\n",
        "\n",
        "        # === Compute R_star[t] as in (41) ===\n",
        "        # Placeholder: you might compute a risk measure update here.\n",
        "        R_star[t] = R_star[t + 1]  # Replace with actual computation\n",
        "\n",
        "        # === Compute Q_star[t] as in (45) ===\n",
        "        # Placeholder: combine the immediate cost and risk measure.\n",
        "        Q_star[t] = -Pi[t] - lambda_param * np.var(Pi[t])  # Replace with the proper formula\n",
        "\n",
        "    # %% [code]\n",
        "    # Calculate the QLBS option price at t = 0:\n",
        "    # QLBS_price = - (1/K_paths) * sum_{k=1}^{K_paths} Q_star[0, k]\n",
        "    QLBS_price = -np.mean(Q_star[0])\n",
        "    print(\"QLBS Option Price at t=0:\", QLBS_price)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# New Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtbPF1rEB7NS",
        "outputId": "27a57fa4-92f7-4cd6-84e4-95b96c95f1f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulated Stock Prices (Heston):\n",
            "[100.          97.77855336  97.0877602   97.67846846  98.39851349\n",
            "  98.40933976  98.54160439  98.21772185  98.01932248  96.80721411\n",
            "  97.82238949  98.77386293  99.2083249   98.91912334  97.78240088\n",
            "  98.69030889  99.10407195 100.05665888  99.93942357 101.22210555\n",
            " 101.6959107  100.87876677 102.06812112 101.16878383 103.01358198\n",
            " 103.63787722 101.76168824 100.93179638 101.47626939 100.19106042\n",
            "  98.80214381  99.49271029 100.32509691 100.07353023  99.78374195\n",
            " 100.68868727 100.37989557  99.45404241  97.85176198  96.03437564\n",
            "  97.29843464  96.36157797  96.78790962  97.54963739  97.13237741\n",
            "  97.6834944   97.4505998   98.04957137  96.37569084  96.51931495\n",
            "  97.17080744  97.20214034 100.07574224  99.90159287  99.05192823\n",
            "  99.18569936 100.89541339 101.08200815 100.31649377  99.27158892\n",
            "  99.97726763 100.93913613 100.78704549 100.54582744 100.56533894\n",
            "  99.66350235  99.11911625  98.09001765  97.31603319  94.44446949\n",
            "  94.70459822  93.8704828   94.14549325  94.45616267  95.14806521\n",
            "  93.25791893  94.11931011  95.41786084  97.18040605  97.26087604\n",
            "  96.56780599  96.0471745   94.85556678  94.95158528  93.90257591\n",
            "  90.85027235  90.87427808  90.46347599  89.92271543  92.300464\n",
            "  92.2098654   91.19476706  90.83401519  91.90388887  90.13693283\n",
            "  90.14221684  88.62868948  88.29674152  89.24174782  90.1343805\n",
            "  90.18873629  90.04745439  89.66767267  89.91771164  89.76340601\n",
            "  88.98665198  87.86837616  88.08308652  89.86783181  89.01802726\n",
            "  89.67929613  89.01446968  88.68776477  88.60979951  89.4029444\n",
            "  91.48267212  91.50945271  92.35176768  92.46737295  90.32121258\n",
            "  90.42869514  90.88255285  92.76821698  95.60460088  95.10617314\n",
            "  95.09979405  93.57325844  92.85615122  93.51957975  93.02159981\n",
            "  93.95368371  92.91508905  93.75245811  92.05591894  92.78880324\n",
            "  93.15895678  91.8876584   92.50846412  92.10186775  93.30756014\n",
            "  94.78883722  95.81573113  95.65843541  93.06548312  94.24045328\n",
            "  94.10295909  94.42362581  93.46538184  91.51758697  92.07498534\n",
            "  89.89747358  90.1258827   91.84170016  90.94913846  90.00169808\n",
            "  89.21790928  89.83734911  89.8737914   91.51611435  91.44643864\n",
            "  92.97400762  92.71124726  92.93956402  93.71176643  94.23440802\n",
            "  93.13063762  92.13514921  89.61092935  89.47965917  90.79448303\n",
            "  89.87962891  90.43032781  89.68402972  89.35976168  90.15396523\n",
            "  92.56600847  91.7136053   91.93199426  91.94897802  91.08016658\n",
            "  91.62933213  90.8738909   89.68677681  91.8118237   93.92311434\n",
            "  92.19779536  95.03789642  93.75336181  93.70684964  94.44582699\n",
            "  95.85688217  94.94695716  95.5002528   95.97691266  96.08182101\n",
            "  96.20078241  93.96163883  95.43149742  94.10347573  95.1983521\n",
            "  95.23388638  96.25919188  97.60275907  98.1071053   98.8343156\n",
            "  99.11061244  99.92466516  99.64153184 100.64738377 100.86229355\n",
            " 101.08655522 101.54386053  99.34838576  98.52739381  99.71351448\n",
            "  98.92684725  96.61339416  94.74034374  95.5315853   96.27748931\n",
            "  96.86597329  95.45648331  95.27727395  95.10343398  95.86597481\n",
            "  95.02978842  94.90106341  94.24761228  93.87833205  93.25414526\n",
            "  92.46601792  90.53236374  89.6469236   89.70052538  91.72404492\n",
            "  91.16810447  90.74630512  91.96926084  89.52686481  88.85975588\n",
            "  87.81813914  88.97238185  87.65173891  86.55733261  86.86263994\n",
            "  88.40058099  88.7534766   89.86430765  90.94618442  90.60108983]\n",
            "\n",
            "Option Price (Heston): 36.32224967850638\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model='gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T / n\n",
        "        self.model = model\n",
        "        self.sigma = sigma  # For GBM, sigma is constant; for Heston, sigma is the initial volatility.\n",
        "        # initial variance for Heston model\n",
        "        if self.model == 'heston':\n",
        "            self.kappa = 2.0         # speed of mean reversion\n",
        "            self.theta = sigma**2    # long-run variance (theta)\n",
        "            self.xi = 0.1            # volatility of volatility\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # initialize variance process\n",
        "            v = np.zeros(self.n)\n",
        "            v[0] = sigma**2 \n",
        "            for i in range(1, self.n):\n",
        "                # Euler-Maruyama update for variance\n",
        "                dv = self.kappa * (self.theta - v[i-1]) * self.dt + self.xi * np.sqrt(max(v[i-1], 0)) * np.sqrt(self.dt) * np.random.normal()\n",
        "                v[i] = v[i-1] + dv\n",
        "                # Ensure non-negativity (using full truncation)\n",
        "                v[i] = max(v[i], 0)\n",
        "            # Return the volatility (sqrt of variance)\n",
        "            return np.sqrt(v)\n",
        "\n",
        "    def simulate(self):\n",
        "        # no need to check model here, as vol() handles it\n",
        "        vol_path = self.vol(self.sigma)\n",
        "        innovations = np.random.normal(0, 1, self.n)\n",
        "        stock_prices = np.zeros(self.n)\n",
        "        stock_prices[0] = self.s0\n",
        "\n",
        "        for i in range(1, self.n):\n",
        "            stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "            )\n",
        "        return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        # European call option payoff at maturity\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Using the Heston model\n",
        "    heston_stock = stock(s0=100, r=0.05, sigma=0.2, T=1.0, n=250, model='heston')\n",
        "    simulated_prices = heston_stock.simulate()\n",
        "    price = heston_stock.option_price(K=100)\n",
        "\n",
        "    print(\"Simulated Stock Prices (Heston):\")\n",
        "    print(simulated_prices)\n",
        "    print(\"\\nOption Price (Heston):\", price)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "ZciZCY_VbG0J",
        "outputId": "f3c4d260-2013-430f-bec7-5666a63430e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/261.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-5.2.1\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'redis' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-849d351a06e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Connect to Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRedis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localhost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6379\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set a value in Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'redis' is not defined"
          ]
        }
      ],
      "source": [
        "# Install a pip package in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install redis\n",
        "\n",
        "# Connect to Redis\n",
        "r = redis.Redis(host='localhost', port=6379, db=0)\n",
        "\n",
        "# Set a value in Redis\n",
        "r.set('Stock', 'NVDA', 'Price', '99.60')\n",
        "\n",
        "# Get a value from Redis\n",
        "stock = r.get('stock')\n",
        "print(stock)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRBVQTcZu20Z",
        "outputId": "5ba5ca8c-39eb-43f7-e9fb-365e1749ad9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Running Tabular Q-Learning (Constant Volatility) ========\n",
            "[Tabular Q] Episode 0, Total Reward: -2260.85, Epsilon: 0.995\n",
            "[Tabular Q] Episode 50, Total Reward: -2682.09, Epsilon: 0.774\n",
            "[Tabular Q] Episode 100, Total Reward: -5369.04, Epsilon: 0.603\n",
            "[Tabular Q] Episode 150, Total Reward: -3221.20, Epsilon: 0.469\n",
            "[Tabular Q] Episode 200, Total Reward: -2881.90, Epsilon: 0.365\n",
            "[Tabular Q] Episode 250, Total Reward: -4953.80, Epsilon: 0.284\n",
            "[Tabular Q] Episode 300, Total Reward: -3136.11, Epsilon: 0.221\n",
            "[Tabular Q] Episode 350, Total Reward: -3065.78, Epsilon: 0.172\n",
            "[Tabular Q] Episode 400, Total Reward: -1207.62, Epsilon: 0.134\n",
            "[Tabular Q] Episode 450, Total Reward: -719.54, Epsilon: 0.104\n",
            "[Tabular Q] Episode 500, Total Reward: -754.68, Epsilon: 0.081\n",
            "[Tabular Q] Episode 550, Total Reward: -1046.49, Epsilon: 0.063\n",
            "[Tabular Q] Episode 600, Total Reward: -849.06, Epsilon: 0.049\n",
            "[Tabular Q] Episode 650, Total Reward: -1896.88, Epsilon: 0.038\n",
            "[Tabular Q] Episode 700, Total Reward: -253.64, Epsilon: 0.030\n",
            "[Tabular Q] Episode 750, Total Reward: -33.20, Epsilon: 0.023\n",
            "[Tabular Q] Episode 800, Total Reward: -632.65, Epsilon: 0.018\n",
            "[Tabular Q] Episode 850, Total Reward: -531.08, Epsilon: 0.014\n",
            "[Tabular Q] Episode 900, Total Reward: -225.86, Epsilon: 0.011\n",
            "[Tabular Q] Episode 950, Total Reward: -575.47, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1000, Total Reward: -649.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1050, Total Reward: -37.07, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1100, Total Reward: -1358.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1150, Total Reward: -236.39, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1200, Total Reward: -243.28, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1250, Total Reward: 43.85, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1300, Total Reward: -1249.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1350, Total Reward: -858.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1400, Total Reward: -269.15, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1450, Total Reward: -1797.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1500, Total Reward: -2207.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1550, Total Reward: -1935.33, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1600, Total Reward: -451.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1650, Total Reward: -1259.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1700, Total Reward: -1078.18, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1750, Total Reward: -1158.99, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1800, Total Reward: -3005.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1850, Total Reward: -1354.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1900, Total Reward: -2707.19, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1950, Total Reward: -2857.99, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2000, Total Reward: -1847.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2050, Total Reward: -751.07, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2100, Total Reward: -3437.95, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2150, Total Reward: -3015.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2200, Total Reward: -2008.57, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2250, Total Reward: -1625.74, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2300, Total Reward: -1030.69, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2350, Total Reward: -3780.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2400, Total Reward: -2795.21, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2450, Total Reward: -3793.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2500, Total Reward: -2317.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2550, Total Reward: -861.75, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2600, Total Reward: -2120.76, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2650, Total Reward: -964.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2700, Total Reward: -2011.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2750, Total Reward: -1434.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2800, Total Reward: -1787.88, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2850, Total Reward: -1662.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2900, Total Reward: -992.08, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2950, Total Reward: -1924.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3000, Total Reward: -2370.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3050, Total Reward: -2046.15, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3100, Total Reward: -1123.55, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3150, Total Reward: -2984.78, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3200, Total Reward: -1180.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3250, Total Reward: -2115.56, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3300, Total Reward: -1466.04, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3350, Total Reward: -1379.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3400, Total Reward: -915.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3450, Total Reward: -2076.79, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3500, Total Reward: -3173.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3550, Total Reward: -2318.37, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3600, Total Reward: -2330.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3650, Total Reward: -5800.54, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3700, Total Reward: -2850.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3750, Total Reward: -1530.11, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3800, Total Reward: -3653.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3850, Total Reward: -1636.97, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3900, Total Reward: -1756.55, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3950, Total Reward: -1993.89, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4000, Total Reward: -1021.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4050, Total Reward: -3416.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4100, Total Reward: -2875.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4150, Total Reward: -795.31, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4200, Total Reward: -3712.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4250, Total Reward: -1174.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4300, Total Reward: -2328.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4350, Total Reward: -462.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4400, Total Reward: -1709.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4450, Total Reward: -3318.98, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4500, Total Reward: -2942.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4550, Total Reward: -1270.11, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4600, Total Reward: -1780.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4650, Total Reward: -3568.24, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4700, Total Reward: -990.30, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4750, Total Reward: -2346.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4800, Total Reward: -2377.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4850, Total Reward: -1234.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4900, Total Reward: -1237.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4950, Total Reward: -4481.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5000, Total Reward: -1223.23, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5050, Total Reward: -2741.82, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5100, Total Reward: -1908.10, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5150, Total Reward: -1849.76, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5200, Total Reward: -561.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5250, Total Reward: -438.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5300, Total Reward: -2743.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5350, Total Reward: -660.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5400, Total Reward: -1278.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5450, Total Reward: -2535.60, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5500, Total Reward: -2299.96, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5550, Total Reward: -1336.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5600, Total Reward: -1244.87, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5650, Total Reward: -7.64, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5700, Total Reward: -2649.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5750, Total Reward: -5325.01, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5800, Total Reward: -2232.18, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5850, Total Reward: -1722.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5900, Total Reward: -2259.91, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5950, Total Reward: -1241.49, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6000, Total Reward: -788.04, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6050, Total Reward: -1892.65, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6100, Total Reward: -665.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6150, Total Reward: -2153.61, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6200, Total Reward: -3364.83, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6250, Total Reward: 149.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6300, Total Reward: -2119.24, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6350, Total Reward: -1562.63, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6400, Total Reward: -2116.21, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6450, Total Reward: -582.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6500, Total Reward: -523.12, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6550, Total Reward: -472.05, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6600, Total Reward: -2958.23, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6650, Total Reward: -1739.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6700, Total Reward: -880.65, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6750, Total Reward: -4722.32, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6800, Total Reward: -937.29, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6850, Total Reward: -358.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6900, Total Reward: -1172.83, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6950, Total Reward: -1714.53, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7000, Total Reward: -280.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7050, Total Reward: -2379.35, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7100, Total Reward: -530.10, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7150, Total Reward: -1434.97, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7200, Total Reward: 63.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7250, Total Reward: -1346.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7300, Total Reward: -46.79, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7350, Total Reward: -418.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7400, Total Reward: -585.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7450, Total Reward: -527.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7500, Total Reward: 63.57, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7550, Total Reward: -353.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7600, Total Reward: -620.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7650, Total Reward: -2752.81, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7700, Total Reward: -568.58, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7750, Total Reward: -2169.78, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7800, Total Reward: -402.02, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7850, Total Reward: -602.56, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7900, Total Reward: -826.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7950, Total Reward: -1423.49, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8000, Total Reward: 42.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8050, Total Reward: -2109.68, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8100, Total Reward: -900.73, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8150, Total Reward: -794.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8200, Total Reward: -658.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8250, Total Reward: -309.44, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8300, Total Reward: -1894.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8350, Total Reward: -876.31, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8400, Total Reward: -2353.93, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8450, Total Reward: -1286.44, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8500, Total Reward: -710.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8550, Total Reward: -420.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8600, Total Reward: 36.40, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8650, Total Reward: -3801.19, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8700, Total Reward: -2.71, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8750, Total Reward: -358.33, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8800, Total Reward: -336.48, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8850, Total Reward: 129.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8900, Total Reward: -92.30, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8950, Total Reward: -463.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9000, Total Reward: -3004.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9050, Total Reward: -3649.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9100, Total Reward: -1592.92, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9150, Total Reward: 94.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9200, Total Reward: -207.08, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9250, Total Reward: -222.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9300, Total Reward: 114.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9350, Total Reward: -1526.32, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9400, Total Reward: -325.64, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9450, Total Reward: 96.92, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9500, Total Reward: -111.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9550, Total Reward: -278.98, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9600, Total Reward: -791.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9650, Total Reward: -321.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9700, Total Reward: -3298.17, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9750, Total Reward: -816.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9800, Total Reward: -1716.89, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9850, Total Reward: -141.61, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9900, Total Reward: 67.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9950, Total Reward: -470.67, Epsilon: 0.010\n",
            "\n",
            "======== Running DDPG (Constant Volatility) ========\n",
            "[DDPG] Episode 0, Total Reward: -162.65\n",
            "[DDPG] Episode 20, Total Reward: -34249.82\n",
            "[DDPG] Episode 40, Total Reward: -37419.99\n",
            "[DDPG] Episode 60, Total Reward: -9521.75\n",
            "[DDPG] Episode 80, Total Reward: -22137.08\n",
            "[DDPG] Episode 100, Total Reward: -17903.48\n",
            "[DDPG] Episode 120, Total Reward: -8689.85\n",
            "[DDPG] Episode 140, Total Reward: -22162.90\n",
            "[DDPG] Episode 160, Total Reward: -4404.14\n",
            "[DDPG] Episode 180, Total Reward: -10883.51\n",
            "[DDPG] Episode 200, Total Reward: -7462.49\n",
            "[DDPG] Episode 220, Total Reward: -17484.27\n",
            "[DDPG] Episode 240, Total Reward: -6433.27\n",
            "[DDPG] Episode 260, Total Reward: -5798.70\n",
            "[DDPG] Episode 280, Total Reward: -8445.03\n",
            "[DDPG] Episode 300, Total Reward: -17433.42\n",
            "[DDPG] Episode 320, Total Reward: -10286.36\n",
            "[DDPG] Episode 340, Total Reward: -12457.67\n",
            "[DDPG] Episode 360, Total Reward: -10084.60\n",
            "[DDPG] Episode 380, Total Reward: -9484.26\n",
            "[DDPG] Episode 400, Total Reward: -9739.51\n",
            "[DDPG] Episode 420, Total Reward: -8270.05\n",
            "[DDPG] Episode 440, Total Reward: -2238.75\n",
            "[DDPG] Episode 460, Total Reward: -3484.02\n",
            "[DDPG] Episode 480, Total Reward: -1777.63\n",
            "[DDPG] Episode 500, Total Reward: -195.24\n",
            "[DDPG] Episode 520, Total Reward: -288.00\n",
            "[DDPG] Episode 540, Total Reward: -2198.73\n",
            "[DDPG] Episode 560, Total Reward: -85458.86\n",
            "[DDPG] Episode 580, Total Reward: -248646.30\n",
            "[DDPG] Episode 600, Total Reward: -33235.52\n",
            "[DDPG] Episode 620, Total Reward: -66204.60\n",
            "[DDPG] Episode 640, Total Reward: -33974.92\n",
            "[DDPG] Episode 660, Total Reward: -32641.69\n",
            "[DDPG] Episode 680, Total Reward: -17492.34\n",
            "[DDPG] Episode 700, Total Reward: -6906.51\n",
            "[DDPG] Episode 720, Total Reward: -16163.68\n",
            "[DDPG] Episode 740, Total Reward: -17828.27\n",
            "[DDPG] Episode 760, Total Reward: -104582.80\n",
            "[DDPG] Episode 780, Total Reward: -117257.44\n",
            "[DDPG] Episode 800, Total Reward: -25415.48\n",
            "[DDPG] Episode 820, Total Reward: -82194.61\n",
            "[DDPG] Episode 840, Total Reward: -14063.03\n",
            "[DDPG] Episode 860, Total Reward: -2877.31\n",
            "[DDPG] Episode 880, Total Reward: -2126.73\n",
            "[DDPG] Episode 900, Total Reward: -15162.42\n",
            "[DDPG] Episode 920, Total Reward: -79476.24\n",
            "[DDPG] Episode 940, Total Reward: -159669.29\n",
            "[DDPG] Episode 960, Total Reward: -277042.03\n",
            "[DDPG] Episode 980, Total Reward: -73943.30\n",
            "[DDPG] Episode 1000, Total Reward: -39015.76\n",
            "[DDPG] Episode 1020, Total Reward: -22603.29\n",
            "[DDPG] Episode 1040, Total Reward: -22206.40\n",
            "[DDPG] Episode 1060, Total Reward: -33888.92\n",
            "[DDPG] Episode 1080, Total Reward: -42160.10\n",
            "[DDPG] Episode 1100, Total Reward: -57816.25\n",
            "[DDPG] Episode 1120, Total Reward: -80167.67\n",
            "[DDPG] Episode 1140, Total Reward: -81982.42\n",
            "[DDPG] Episode 1160, Total Reward: -68066.27\n",
            "[DDPG] Episode 1180, Total Reward: -37696.80\n",
            "[DDPG] Episode 1200, Total Reward: -65341.57\n",
            "[DDPG] Episode 1220, Total Reward: -54184.37\n",
            "[DDPG] Episode 1240, Total Reward: -409889.12\n",
            "[DDPG] Episode 1260, Total Reward: -42673.74\n",
            "[DDPG] Episode 1280, Total Reward: -56196.82\n",
            "[DDPG] Episode 1300, Total Reward: -71562.77\n",
            "[DDPG] Episode 1320, Total Reward: -20534.45\n",
            "[DDPG] Episode 1340, Total Reward: -55999.61\n",
            "[DDPG] Episode 1360, Total Reward: -4295480.53\n",
            "[DDPG] Episode 1380, Total Reward: -18336.17\n",
            "[DDPG] Episode 1400, Total Reward: -13937.70\n",
            "[DDPG] Episode 1420, Total Reward: -4011.91\n",
            "[DDPG] Episode 1440, Total Reward: -11510.11\n",
            "[DDPG] Episode 1460, Total Reward: -12277.87\n",
            "[DDPG] Episode 1480, Total Reward: -6517.06\n",
            "[DDPG] Episode 1500, Total Reward: -13578.39\n",
            "[DDPG] Episode 1520, Total Reward: -1779.47\n",
            "[DDPG] Episode 1540, Total Reward: -18097.41\n",
            "[DDPG] Episode 1560, Total Reward: -51949.42\n",
            "[DDPG] Episode 1580, Total Reward: -8433403.70\n",
            "[DDPG] Episode 1600, Total Reward: -5225361.56\n",
            "[DDPG] Episode 1620, Total Reward: -61831.35\n",
            "[DDPG] Episode 1640, Total Reward: -80659.91\n",
            "[DDPG] Episode 1660, Total Reward: -7057.13\n",
            "[DDPG] Episode 1680, Total Reward: -9368.26\n",
            "[DDPG] Episode 1700, Total Reward: -712.34\n",
            "[DDPG] Episode 1720, Total Reward: -1609.21\n",
            "[DDPG] Episode 1740, Total Reward: -1440.71\n",
            "[DDPG] Episode 1760, Total Reward: -1092.29\n",
            "[DDPG] Episode 1780, Total Reward: -471.38\n",
            "[DDPG] Episode 1800, Total Reward: -1271.59\n",
            "[DDPG] Episode 1820, Total Reward: -1702.54\n",
            "[DDPG] Episode 1840, Total Reward: -463.08\n",
            "[DDPG] Episode 1860, Total Reward: -218.61\n",
            "[DDPG] Episode 1880, Total Reward: -1130.89\n",
            "[DDPG] Episode 1900, Total Reward: -611.37\n",
            "[DDPG] Episode 1920, Total Reward: -284.71\n",
            "[DDPG] Episode 1940, Total Reward: -662.55\n",
            "[DDPG] Episode 1960, Total Reward: -365.32\n",
            "[DDPG] Episode 1980, Total Reward: -1066.24\n",
            "[DDPG] Episode 2000, Total Reward: -102.67\n",
            "[DDPG] Episode 2020, Total Reward: -941.71\n",
            "[DDPG] Episode 2040, Total Reward: -232.66\n",
            "[DDPG] Episode 2060, Total Reward: -317.70\n",
            "[DDPG] Episode 2080, Total Reward: -824.90\n",
            "[DDPG] Episode 2100, Total Reward: -80.10\n",
            "[DDPG] Episode 2120, Total Reward: -197.33\n",
            "[DDPG] Episode 2140, Total Reward: -169.47\n",
            "[DDPG] Episode 2160, Total Reward: -590.77\n",
            "[DDPG] Episode 2180, Total Reward: -314.08\n",
            "[DDPG] Episode 2200, Total Reward: -263.25\n",
            "[DDPG] Episode 2220, Total Reward: -979.11\n",
            "[DDPG] Episode 2240, Total Reward: -264.99\n",
            "[DDPG] Episode 2260, Total Reward: -279.58\n",
            "[DDPG] Episode 2280, Total Reward: -328.27\n",
            "[DDPG] Episode 2300, Total Reward: -694.10\n",
            "[DDPG] Episode 2320, Total Reward: -895.40\n",
            "[DDPG] Episode 2340, Total Reward: -2582.90\n",
            "[DDPG] Episode 2360, Total Reward: -3548.33\n",
            "[DDPG] Episode 2380, Total Reward: -4668.36\n",
            "[DDPG] Episode 2400, Total Reward: -2926.27\n",
            "[DDPG] Episode 2420, Total Reward: -1308.78\n",
            "[DDPG] Episode 2440, Total Reward: -387.22\n",
            "[DDPG] Episode 2460, Total Reward: -681.02\n",
            "[DDPG] Episode 2480, Total Reward: -248.37\n",
            "[DDPG] Episode 2500, Total Reward: -240.55\n",
            "[DDPG] Episode 2520, Total Reward: -128.37\n",
            "[DDPG] Episode 2540, Total Reward: -349.71\n",
            "[DDPG] Episode 2560, Total Reward: -289.47\n",
            "[DDPG] Episode 2580, Total Reward: -165.17\n",
            "[DDPG] Episode 2600, Total Reward: -120.84\n",
            "[DDPG] Episode 2620, Total Reward: -383.94\n",
            "[DDPG] Episode 2640, Total Reward: -96.80\n",
            "[DDPG] Episode 2660, Total Reward: -390.54\n",
            "[DDPG] Episode 2680, Total Reward: -137.89\n",
            "[DDPG] Episode 2700, Total Reward: -420.25\n",
            "[DDPG] Episode 2720, Total Reward: -142.06\n",
            "[DDPG] Episode 2740, Total Reward: -616.61\n",
            "[DDPG] Episode 2760, Total Reward: -386.19\n",
            "[DDPG] Episode 2780, Total Reward: -6822.24\n",
            "[DDPG] Episode 2800, Total Reward: -5025.50\n",
            "[DDPG] Episode 2820, Total Reward: -8553.12\n",
            "[DDPG] Episode 2840, Total Reward: -21707.73\n",
            "[DDPG] Episode 2860, Total Reward: -27120.42\n",
            "[DDPG] Episode 2880, Total Reward: -11173.45\n",
            "[DDPG] Episode 2900, Total Reward: -53964.64\n",
            "[DDPG] Episode 2920, Total Reward: -24239.83\n",
            "[DDPG] Episode 2940, Total Reward: -48882.09\n",
            "[DDPG] Episode 2960, Total Reward: -119962.01\n",
            "[DDPG] Episode 2980, Total Reward: -135828.95\n",
            "[DDPG] Episode 3000, Total Reward: -96993.37\n",
            "[DDPG] Episode 3020, Total Reward: -420944.23\n",
            "[DDPG] Episode 3040, Total Reward: -14543.06\n",
            "[DDPG] Episode 3060, Total Reward: -45419.92\n",
            "[DDPG] Episode 3080, Total Reward: -52243.85\n",
            "[DDPG] Episode 3100, Total Reward: -35460.19\n",
            "[DDPG] Episode 3120, Total Reward: -63594.46\n",
            "[DDPG] Episode 3140, Total Reward: -6009.78\n",
            "[DDPG] Episode 3160, Total Reward: -16965.90\n",
            "[DDPG] Episode 3180, Total Reward: -8905.08\n",
            "[DDPG] Episode 3200, Total Reward: -4720.93\n",
            "[DDPG] Episode 3220, Total Reward: -25018.65\n",
            "[DDPG] Episode 3240, Total Reward: -35318.20\n",
            "[DDPG] Episode 3260, Total Reward: -6264.64\n",
            "[DDPG] Episode 3280, Total Reward: -20356.89\n",
            "[DDPG] Episode 3300, Total Reward: -19930.34\n",
            "[DDPG] Episode 3320, Total Reward: -1280.99\n",
            "[DDPG] Episode 3340, Total Reward: -9636.47\n",
            "[DDPG] Episode 3360, Total Reward: -7284.52\n",
            "[DDPG] Episode 3380, Total Reward: -1894.36\n",
            "[DDPG] Episode 3400, Total Reward: -2935.07\n",
            "[DDPG] Episode 3420, Total Reward: -710.45\n",
            "[DDPG] Episode 3440, Total Reward: -17283.37\n",
            "[DDPG] Episode 3460, Total Reward: -2907.65\n",
            "[DDPG] Episode 3480, Total Reward: -3722.30\n",
            "[DDPG] Episode 3500, Total Reward: -5510.79\n",
            "[DDPG] Episode 3520, Total Reward: -12151.72\n",
            "[DDPG] Episode 3540, Total Reward: -3102.40\n",
            "[DDPG] Episode 3560, Total Reward: -5051.15\n",
            "[DDPG] Episode 3580, Total Reward: -2815.67\n",
            "[DDPG] Episode 3600, Total Reward: -27373.35\n",
            "[DDPG] Episode 3620, Total Reward: -97830.07\n",
            "[DDPG] Episode 3640, Total Reward: -18465.65\n",
            "[DDPG] Episode 3660, Total Reward: -35171.25\n",
            "[DDPG] Episode 3680, Total Reward: -46854.58\n",
            "[DDPG] Episode 3700, Total Reward: -10374.59\n",
            "[DDPG] Episode 3720, Total Reward: -24429.24\n",
            "[DDPG] Episode 3740, Total Reward: -28953.46\n",
            "[DDPG] Episode 3760, Total Reward: -52182.81\n",
            "[DDPG] Episode 3780, Total Reward: -83490.66\n",
            "[DDPG] Episode 3800, Total Reward: -11285.71\n",
            "[DDPG] Episode 3820, Total Reward: -59866.15\n",
            "[DDPG] Episode 3840, Total Reward: -84812.04\n",
            "[DDPG] Episode 3860, Total Reward: -77596.71\n",
            "[DDPG] Episode 3880, Total Reward: -81423.56\n",
            "[DDPG] Episode 3900, Total Reward: -114381.43\n",
            "[DDPG] Episode 3920, Total Reward: -99221.50\n",
            "[DDPG] Episode 3940, Total Reward: -120174.63\n",
            "[DDPG] Episode 3960, Total Reward: -103633.63\n",
            "[DDPG] Episode 3980, Total Reward: -93701.46\n",
            "[DDPG] Episode 4000, Total Reward: -88764.14\n",
            "[DDPG] Episode 4020, Total Reward: -146746.69\n",
            "[DDPG] Episode 4040, Total Reward: -187620.15\n",
            "[DDPG] Episode 4060, Total Reward: -528171.04\n",
            "[DDPG] Episode 4080, Total Reward: -184498.23\n",
            "[DDPG] Episode 4100, Total Reward: -163043.92\n",
            "[DDPG] Episode 4120, Total Reward: -265133.34\n",
            "[DDPG] Episode 4140, Total Reward: -157428.30\n",
            "[DDPG] Episode 4160, Total Reward: -40675.26\n",
            "[DDPG] Episode 4180, Total Reward: -23078.69\n",
            "[DDPG] Episode 4200, Total Reward: -21571.72\n",
            "[DDPG] Episode 4220, Total Reward: -13310.04\n",
            "[DDPG] Episode 4240, Total Reward: -11922.54\n",
            "[DDPG] Episode 4260, Total Reward: -8349.24\n",
            "[DDPG] Episode 4280, Total Reward: -7048.63\n",
            "[DDPG] Episode 4300, Total Reward: -7923.36\n",
            "[DDPG] Episode 4320, Total Reward: -10063.03\n",
            "[DDPG] Episode 4340, Total Reward: -10837.22\n",
            "[DDPG] Episode 4360, Total Reward: -14242.87\n",
            "[DDPG] Episode 4380, Total Reward: -13725.89\n",
            "[DDPG] Episode 4400, Total Reward: -11526.83\n",
            "[DDPG] Episode 4420, Total Reward: -17742.27\n",
            "[DDPG] Episode 4440, Total Reward: -24431.70\n",
            "[DDPG] Episode 4460, Total Reward: -20928.38\n",
            "[DDPG] Episode 4480, Total Reward: -48941.77\n",
            "[DDPG] Episode 4500, Total Reward: -64251.41\n",
            "[DDPG] Episode 4520, Total Reward: -31657.33\n",
            "[DDPG] Episode 4540, Total Reward: -30081.90\n",
            "[DDPG] Episode 4560, Total Reward: -38298.09\n",
            "[DDPG] Episode 4580, Total Reward: -20704.25\n",
            "[DDPG] Episode 4600, Total Reward: -114973.30\n",
            "[DDPG] Episode 4620, Total Reward: -25851.42\n",
            "[DDPG] Episode 4640, Total Reward: -30517.39\n",
            "[DDPG] Episode 4660, Total Reward: -24009.74\n",
            "[DDPG] Episode 4680, Total Reward: -14301.17\n",
            "[DDPG] Episode 4700, Total Reward: -24957.64\n",
            "[DDPG] Episode 4720, Total Reward: -18732.79\n",
            "[DDPG] Episode 4740, Total Reward: -28292.16\n",
            "[DDPG] Episode 4760, Total Reward: -15027.44\n",
            "[DDPG] Episode 4780, Total Reward: -19221.07\n",
            "[DDPG] Episode 4800, Total Reward: -18578.00\n",
            "[DDPG] Episode 4820, Total Reward: -6196.22\n",
            "[DDPG] Episode 4840, Total Reward: -9448.32\n",
            "[DDPG] Episode 4860, Total Reward: -9546.60\n",
            "[DDPG] Episode 4880, Total Reward: -10527.26\n",
            "[DDPG] Episode 4900, Total Reward: -19069.29\n",
            "[DDPG] Episode 4920, Total Reward: -4942.94\n",
            "[DDPG] Episode 4940, Total Reward: -13139.43\n",
            "[DDPG] Episode 4960, Total Reward: -12800.77\n",
            "[DDPG] Episode 4980, Total Reward: -95.90\n",
            "[DDPG] Episode 5000, Total Reward: -115.38\n",
            "[DDPG] Episode 5020, Total Reward: -353.46\n",
            "[DDPG] Episode 5040, Total Reward: -548.17\n",
            "[DDPG] Episode 5060, Total Reward: -704.95\n",
            "[DDPG] Episode 5080, Total Reward: -642.66\n",
            "[DDPG] Episode 5100, Total Reward: -2569.81\n",
            "[DDPG] Episode 5120, Total Reward: -13787.21\n",
            "[DDPG] Episode 5140, Total Reward: -18246.17\n",
            "[DDPG] Episode 5160, Total Reward: -13651.75\n",
            "[DDPG] Episode 5180, Total Reward: -9426.23\n",
            "[DDPG] Episode 5200, Total Reward: -8332.86\n",
            "[DDPG] Episode 5220, Total Reward: -8441.81\n",
            "[DDPG] Episode 5240, Total Reward: -10094.16\n",
            "[DDPG] Episode 5260, Total Reward: -8033.77\n",
            "[DDPG] Episode 5280, Total Reward: -8279.10\n",
            "[DDPG] Episode 5300, Total Reward: -13662.25\n",
            "[DDPG] Episode 5320, Total Reward: -7928.83\n",
            "[DDPG] Episode 5340, Total Reward: -11550.14\n",
            "[DDPG] Episode 5360, Total Reward: -7726.29\n",
            "[DDPG] Episode 5380, Total Reward: -10268.68\n",
            "[DDPG] Episode 5400, Total Reward: -12619.52\n",
            "[DDPG] Episode 5420, Total Reward: -7259.67\n",
            "[DDPG] Episode 5440, Total Reward: -11438.05\n",
            "[DDPG] Episode 5460, Total Reward: -6737.46\n",
            "[DDPG] Episode 5480, Total Reward: -8592.95\n",
            "[DDPG] Episode 5500, Total Reward: -8462.39\n",
            "[DDPG] Episode 5520, Total Reward: -9324.19\n",
            "[DDPG] Episode 5540, Total Reward: -8491.41\n",
            "[DDPG] Episode 5560, Total Reward: -13165.58\n",
            "[DDPG] Episode 5580, Total Reward: -9216.49\n",
            "[DDPG] Episode 5600, Total Reward: -14463.96\n",
            "[DDPG] Episode 5620, Total Reward: -13465.13\n",
            "[DDPG] Episode 5640, Total Reward: -13236.14\n",
            "[DDPG] Episode 5660, Total Reward: -12038.35\n",
            "[DDPG] Episode 5680, Total Reward: -11134.01\n",
            "[DDPG] Episode 5700, Total Reward: -9765.18\n",
            "[DDPG] Episode 5720, Total Reward: -9532.17\n",
            "[DDPG] Episode 5740, Total Reward: -8939.98\n",
            "[DDPG] Episode 5760, Total Reward: -7115.82\n",
            "[DDPG] Episode 5780, Total Reward: -11740.10\n",
            "[DDPG] Episode 5800, Total Reward: -8006.27\n",
            "[DDPG] Episode 5820, Total Reward: -13429.23\n",
            "[DDPG] Episode 5840, Total Reward: -8010.20\n",
            "[DDPG] Episode 5860, Total Reward: -6896.71\n",
            "[DDPG] Episode 5880, Total Reward: -5636.60\n",
            "[DDPG] Episode 5900, Total Reward: -31248.05\n",
            "[DDPG] Episode 5920, Total Reward: -3069.17\n",
            "[DDPG] Episode 5940, Total Reward: -3680.56\n",
            "[DDPG] Episode 5960, Total Reward: -3128.99\n",
            "[DDPG] Episode 5980, Total Reward: -2249.35\n",
            "[DDPG] Episode 6000, Total Reward: -3285.12\n",
            "[DDPG] Episode 6020, Total Reward: -3919.90\n",
            "[DDPG] Episode 6040, Total Reward: -3940.74\n",
            "[DDPG] Episode 6060, Total Reward: -4361.41\n",
            "[DDPG] Episode 6080, Total Reward: -3298.39\n",
            "[DDPG] Episode 6100, Total Reward: -2479.69\n",
            "[DDPG] Episode 6120, Total Reward: -3661.47\n",
            "[DDPG] Episode 6140, Total Reward: -2389.38\n",
            "[DDPG] Episode 6160, Total Reward: -832.05\n",
            "[DDPG] Episode 6180, Total Reward: -1026.70\n",
            "[DDPG] Episode 6200, Total Reward: -66.31\n",
            "[DDPG] Episode 6220, Total Reward: -221853370156713.78\n",
            "[DDPG] Episode 6240, Total Reward: -1783.60\n",
            "[DDPG] Episode 6260, Total Reward: nan\n",
            "[DDPG] Episode 6280, Total Reward: nan\n",
            "[DDPG] Episode 6300, Total Reward: nan\n",
            "[DDPG] Episode 6320, Total Reward: nan\n",
            "[DDPG] Episode 6340, Total Reward: nan\n",
            "[DDPG] Episode 6360, Total Reward: nan\n",
            "[DDPG] Episode 6380, Total Reward: nan\n",
            "[DDPG] Episode 6400, Total Reward: nan\n",
            "[DDPG] Episode 6420, Total Reward: nan\n",
            "[DDPG] Episode 6440, Total Reward: nan\n",
            "[DDPG] Episode 6460, Total Reward: nan\n",
            "[DDPG] Episode 6480, Total Reward: nan\n",
            "[DDPG] Episode 6500, Total Reward: nan\n",
            "[DDPG] Episode 6520, Total Reward: nan\n",
            "[DDPG] Episode 6540, Total Reward: nan\n",
            "[DDPG] Episode 6560, Total Reward: nan\n",
            "[DDPG] Episode 6580, Total Reward: nan\n",
            "[DDPG] Episode 6600, Total Reward: nan\n",
            "[DDPG] Episode 6620, Total Reward: nan\n",
            "[DDPG] Episode 6640, Total Reward: nan\n",
            "[DDPG] Episode 6660, Total Reward: nan\n",
            "[DDPG] Episode 6680, Total Reward: nan\n",
            "[DDPG] Episode 6700, Total Reward: nan\n",
            "[DDPG] Episode 6720, Total Reward: nan\n",
            "[DDPG] Episode 6740, Total Reward: nan\n",
            "[DDPG] Episode 6760, Total Reward: nan\n",
            "[DDPG] Episode 6780, Total Reward: nan\n",
            "[DDPG] Episode 6800, Total Reward: nan\n",
            "[DDPG] Episode 6820, Total Reward: nan\n",
            "[DDPG] Episode 6840, Total Reward: nan\n",
            "[DDPG] Episode 6860, Total Reward: nan\n",
            "[DDPG] Episode 6880, Total Reward: nan\n",
            "[DDPG] Episode 6900, Total Reward: nan\n",
            "[DDPG] Episode 6920, Total Reward: nan\n",
            "[DDPG] Episode 6940, Total Reward: nan\n",
            "[DDPG] Episode 6960, Total Reward: nan\n",
            "[DDPG] Episode 6980, Total Reward: nan\n",
            "[DDPG] Episode 7000, Total Reward: nan\n",
            "[DDPG] Episode 7020, Total Reward: nan\n",
            "[DDPG] Episode 7040, Total Reward: nan\n",
            "[DDPG] Episode 7060, Total Reward: nan\n",
            "[DDPG] Episode 7080, Total Reward: nan\n",
            "[DDPG] Episode 7100, Total Reward: nan\n",
            "[DDPG] Episode 7120, Total Reward: nan\n",
            "[DDPG] Episode 7140, Total Reward: nan\n",
            "[DDPG] Episode 7160, Total Reward: nan\n",
            "[DDPG] Episode 7180, Total Reward: nan\n",
            "[DDPG] Episode 7200, Total Reward: nan\n",
            "[DDPG] Episode 7220, Total Reward: nan\n",
            "[DDPG] Episode 7240, Total Reward: nan\n",
            "[DDPG] Episode 7260, Total Reward: nan\n",
            "[DDPG] Episode 7280, Total Reward: nan\n",
            "[DDPG] Episode 7300, Total Reward: nan\n",
            "[DDPG] Episode 7320, Total Reward: nan\n",
            "[DDPG] Episode 7340, Total Reward: nan\n",
            "[DDPG] Episode 7360, Total Reward: nan\n",
            "[DDPG] Episode 7380, Total Reward: nan\n",
            "[DDPG] Episode 7400, Total Reward: nan\n",
            "[DDPG] Episode 7420, Total Reward: nan\n",
            "[DDPG] Episode 7440, Total Reward: nan\n",
            "[DDPG] Episode 7460, Total Reward: nan\n",
            "[DDPG] Episode 7480, Total Reward: nan\n",
            "[DDPG] Episode 7500, Total Reward: nan\n",
            "[DDPG] Episode 7520, Total Reward: nan\n",
            "[DDPG] Episode 7540, Total Reward: nan\n",
            "[DDPG] Episode 7560, Total Reward: nan\n",
            "[DDPG] Episode 7580, Total Reward: nan\n",
            "[DDPG] Episode 7600, Total Reward: nan\n",
            "[DDPG] Episode 7620, Total Reward: nan\n",
            "[DDPG] Episode 7640, Total Reward: nan\n",
            "[DDPG] Episode 7660, Total Reward: nan\n",
            "[DDPG] Episode 7680, Total Reward: nan\n",
            "[DDPG] Episode 7700, Total Reward: nan\n",
            "[DDPG] Episode 7720, Total Reward: nan\n",
            "[DDPG] Episode 7740, Total Reward: nan\n",
            "[DDPG] Episode 7760, Total Reward: nan\n",
            "[DDPG] Episode 7780, Total Reward: nan\n",
            "[DDPG] Episode 7800, Total Reward: nan\n",
            "[DDPG] Episode 7820, Total Reward: nan\n",
            "[DDPG] Episode 7840, Total Reward: nan\n",
            "[DDPG] Episode 7860, Total Reward: nan\n",
            "[DDPG] Episode 7880, Total Reward: nan\n",
            "[DDPG] Episode 7900, Total Reward: nan\n",
            "[DDPG] Episode 7920, Total Reward: nan\n",
            "[DDPG] Episode 7940, Total Reward: nan\n",
            "[DDPG] Episode 7960, Total Reward: nan\n",
            "[DDPG] Episode 7980, Total Reward: nan\n",
            "[DDPG] Episode 8000, Total Reward: nan\n",
            "[DDPG] Episode 8020, Total Reward: nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n======== Running DDPG (Constant Volatility) ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0menv_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHedgingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstochastic_vol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0mrewards_ddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_const\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n======== Running PPO (Constant Volatility) ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, n_episodes, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Update if enough samples are available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0mstates_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mactions_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##############################\n",
        "# Environment Implementation #\n",
        "##############################\n",
        "\n",
        "class HedgingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom hedging environment.\n",
        "\n",
        "    - Dynamics: Underlying asset follows a geometric Brownian motion.\n",
        "      If stochastic_vol==True, volatility updates via a simplified SABR model.\n",
        "    - Reward: At each timestep the reward is the change in wealth adjusted by transaction costs,\n",
        "      and a risk penalty scaling the variance (approximated here via the squared change).\n",
        "    - At t=0, the agent buys the replicating portfolio.\n",
        "    - At maturity, the terminal payoff of a European call is subtracted.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 T=1.0,            # time horizon (e.g. 1 year)\n",
        "                 n_steps=50,       # number of timesteps per episode\n",
        "                 S0=100.0,         # initial asset price\n",
        "                 sigma0=0.2,       # initial volatility\n",
        "                 kappa=0.001,      # transaction cost parameter\n",
        "                 risk_aversion=0.01,  # risk–penalty parameter lambda\n",
        "                 strike=100.0,     # strike price of the option (European call)\n",
        "                 nu=0.1,           # vol-of-vol (for SABR dynamics)\n",
        "                 rho=-0.3,         # correlation between asset and volatility shocks\n",
        "                 stochastic_vol=True):  # whether to use stochastic volatility dynamics\n",
        "        super(HedgingEnv, self).__init__()\n",
        "        self.T = T\n",
        "        self.n_steps = n_steps\n",
        "        self.dt = T / n_steps\n",
        "        self.S0 = S0\n",
        "        self.sigma0 = sigma0\n",
        "        self.kappa = kappa\n",
        "        self.risk_aversion = risk_aversion\n",
        "        self.strike = strike\n",
        "        self.nu = nu\n",
        "        self.rho = rho\n",
        "        self.stochastic_vol = stochastic_vol\n",
        "\n",
        "        # Continuous action: hedge position. (We assume it can be any real number.)\n",
        "        self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
        "        # State: we use [S, sigma, previous hedge, normalized time]\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.S = self.S0\n",
        "        self.sigma = self.sigma0\n",
        "        self.a_prev = 0.0  # initial hedge (no position)\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, 0.0], dtype=np.float32)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        action = float(action)  # ensure scalar\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        # Compute hedge adjustment cost and change in wealth\n",
        "        trade = self.a_prev - action  # change in hedge\n",
        "        delta_w = self.S * trade - self.kappa * abs(self.S * trade)\n",
        "        risk_penalty = self.risk_aversion * (delta_w ** 2)\n",
        "        reward = delta_w - risk_penalty\n",
        "\n",
        "        if self.t == 0:\n",
        "            # At initial step, buying replicating portfolio\n",
        "            reward = - self.S * action - self.kappa * abs(self.S * action)\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        # Update underlying dynamics:\n",
        "        Z1 = np.random.normal()\n",
        "        if self.stochastic_vol:\n",
        "            # Stochastic volatility update using a simplified SABR-like model\n",
        "            Z2 = np.random.normal()\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            self.sigma = self.sigma + self.nu * self.sigma * np.sqrt(self.dt) * (\n",
        "                self.rho * Z1 + np.sqrt(1 - self.rho ** 2) * Z2)\n",
        "            # Ensure sigma stays positive\n",
        "            self.sigma = max(self.sigma, 1e-3)\n",
        "        else:\n",
        "            # Constant volatility dynamics\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            # sigma remains constant\n",
        "            self.sigma = self.sigma0\n",
        "\n",
        "        self.a_prev = action\n",
        "        time_frac = self.t / self.n_steps\n",
        "        if self.t >= self.n_steps:\n",
        "            # Terminal reward: liquidate position and subtract option payoff.\n",
        "            option_payoff = max(self.S - self.strike, 0)\n",
        "            final_delta = self.S * action - self.kappa * abs(self.S * action) - option_payoff\n",
        "            reward = final_delta\n",
        "            done = True\n",
        "\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, time_frac], dtype=np.float32)\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "##################################\n",
        "# Helper: Monte Carlo Simulation #\n",
        "##################################\n",
        "\n",
        "def simulate_paths(env, N=1000):\n",
        "    \"\"\"\n",
        "    Simulate N Monte Carlo paths using the environment dynamics.\n",
        "    Each episode represents one path.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "    returns = []\n",
        "    for _ in range(N):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_states = [state]\n",
        "        while not done:\n",
        "            # For simulation here, use a fixed policy (e.g., do nothing, a=0)\n",
        "            action = 0.0\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_states.append(state)\n",
        "        paths.append(episode_states)\n",
        "        returns.append(episode_reward)\n",
        "    return paths, returns\n",
        "\n",
        "#####################################\n",
        "# Tabular Q-Learning Implementation #\n",
        "#####################################\n",
        "\n",
        "class TabularQAgent:\n",
        "    def __init__(self, env, price_bins=50, action_low=-1.0, action_high=1.0, action_step=0.01,\n",
        "                 alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.price_bins = np.linspace(0, 2 * env.S0, price_bins)\n",
        "        self.n_time = env.n_steps + 1  # include terminal step\n",
        "        self.actions = np.arange(action_low, action_high + action_step, action_step)\n",
        "        self.n_actions = len(self.actions)\n",
        "        # Q-table indexed by (price_index, time_index)\n",
        "        self.Q = np.zeros((price_bins, self.n_time, self.n_actions))\n",
        "        # Hyperparameters\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def discretize_state(self, state):\n",
        "        # For tabular Q-learning we use only the price S and discrete time.\n",
        "        S, _, _, t_frac = state\n",
        "        price_idx = np.digitize(S, self.price_bins) - 1\n",
        "        price_idx = np.clip(price_idx, 0, len(self.price_bins)-1)\n",
        "        time_idx = int(t_frac * self.env.n_steps)\n",
        "        time_idx = np.clip(time_idx, 0, self.n_time-1)\n",
        "        return price_idx, time_idx\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        price_idx, time_idx = self.discretize_state(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explore: choose a random discrete action index.\n",
        "            a_idx = np.random.randint(self.n_actions)\n",
        "        else:\n",
        "            a_idx = np.argmax(self.Q[price_idx, time_idx])\n",
        "        return self.actions[a_idx], a_idx\n",
        "\n",
        "    def update(self, state, action_idx, reward, next_state, done):\n",
        "        s_idx = self.discretize_state(state)\n",
        "        next_s_idx = self.discretize_state(next_state)\n",
        "        price_idx, time_idx = s_idx\n",
        "        n_price_idx, n_time_idx = next_s_idx\n",
        "\n",
        "        best_next = 0 if done else np.max(self.Q[n_price_idx, n_time_idx])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.Q[price_idx, time_idx, action_idx]\n",
        "        self.Q[price_idx, time_idx, action_idx] += self.alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "def train_tabular_q_learning(env, n_episodes=1000):\n",
        "    agent = TabularQAgent(env)\n",
        "    rewards_per_episode = []\n",
        "    for ep in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            action, action_idx = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update(state, action_idx, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        agent.decay_epsilon()\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[Tabular Q] Episode {ep}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###################################\n",
        "# DDPG Implementation (Lillicrap 2019) #\n",
        "###################################\n",
        "\n",
        "# Define Actor and Critic networks for DDPG\n",
        "class DDPGActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = self.out(x)\n",
        "        return action\n",
        "\n",
        "class DDPGCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.fc1(torch.cat([state, action], dim=1)))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_value = self.out(x)\n",
        "        return q_value\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "def train_ddpg(env, n_episodes=500, batch_size=64, gamma=0.99, tau=0.005):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = DDPGActor(state_dim, action_dim)\n",
        "    critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor = DDPGActor(state_dim, action_dim)\n",
        "    target_critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "    buffer = ReplayBuffer()\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action = actor(state_tensor).detach().numpy()[0]\n",
        "            # Add exploratory noise (Gaussian)\n",
        "            action = action + np.random.normal(0, 0.1, size=action_dim)\n",
        "            next_state, reward, done, _ = env.step(action[0])\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Update if enough samples are available\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states_tensor = torch.FloatTensor(states)\n",
        "                actions_tensor = torch.FloatTensor(actions)\n",
        "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "                next_states_tensor = torch.FloatTensor(next_states)\n",
        "                dones_tensor = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "                # Critic update\n",
        "                next_actions = target_actor(next_states_tensor)\n",
        "                target_q = target_critic(next_states_tensor, next_actions)\n",
        "                expected_q = rewards_tensor + gamma * (1 - dones_tensor) * target_q\n",
        "                current_q = critic(states_tensor, actions_tensor)\n",
        "                critic_loss = nn.MSELoss()(current_q, expected_q.detach())\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                # Actor update\n",
        "                actor_loss = -critic(states_tensor, actor(states_tensor)).mean()\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                # Update target networks\n",
        "                for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "                for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[DDPG] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###########################################\n",
        "# PPO Implementation (Schulman et al. 2017) #\n",
        "###########################################\n",
        "\n",
        "class PPOActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(PPOActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Log_std parameter for Gaussian exploration\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "class PPOCritic(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_size=64):\n",
        "        super(PPOCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        value = self.out(x)\n",
        "        return value\n",
        "\n",
        "def train_ppo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4, update_epochs=5):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        # Compute rewards-to-go\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        # Normalize returns\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # Convert trajectory to tensors\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        # Multiple update epochs on the same trajectory\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            # PPO clipped objective\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[PPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "#############################################\n",
        "# GRPO Implementation (DeepSeek 2024 variant)#\n",
        "#############################################\n",
        "\n",
        "# For this demonstration, we assume GRPO augments the PPO update with an additional gradient–reward regularization term.\n",
        "# The implementation here is similar in structure to PPO with an extra term that penalizes large deviations in the gradient.\n",
        "def train_grpo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4,\n",
        "               update_epochs=5, grad_reg_coeff=0.1):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            # Additional gradient regularization (a penalty on the norm of the actor gradient)\n",
        "            optimizer.zero_grad()\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "            loss.backward()\n",
        "            grad_norm = 0.0\n",
        "            for param in actor.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm += param.grad.data.norm(2)\n",
        "            # Add penalty term\n",
        "            loss = loss + grad_reg_coeff * grad_norm\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[GRPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "####################################\n",
        "# Main comparison and experiment   #\n",
        "####################################\n",
        "\n",
        "def main():\n",
        "    # Number of Monte Carlo paths is given by the number of episodes we simulate.\n",
        "    n_mc = 500  # for demonstration we use 500 episodes per algorithm\n",
        "    print(\"======== Running Tabular Q-Learning (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_q = train_tabular_q_learning(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ddpg = train_ddpg(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ppo = train_ppo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_grpo = train_grpo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for constant volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Constant Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Now repeat with stochastic volatility dynamics.\n",
        "    print(\"\\n======== Running Tabular Q-Learning (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_q_stoch = train_tabular_q_learning(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ddpg_stoch = train_ddpg(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ppo_stoch = train_ppo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_grpo_stoch = train_grpo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for stochastic volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q_stoch, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg_stoch, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo_stoch, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo_stoch, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Stochastic Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fin580",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
