{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y-7MqF5_1Je"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jsdd3do_1Jf"
      },
      "outputs": [],
      "source": [
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model = 'gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T/n\n",
        "        self.model = model\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # Use the Heston volatility path\n",
        "            vol_path = self.vol(self.sigma)\n",
        "            innovations = np.random.normal(0, 1, self.n)\n",
        "            stock_prices = np.zeros(self.n)\n",
        "            stock_prices[0] = self.s0\n",
        "\n",
        "            for i in range(1, self.n):\n",
        "                stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                    (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "                )\n",
        "            return stock_prices # Implement Heston model volatility here\n",
        "\n",
        "\n",
        "    def heston_model_sim(S0, v0, rho, kappa, theta, sigma,T, N, M):\n",
        "\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "         - S0, v0: initial parameters for asset and variance\n",
        "         - rho   : correlation between asset returns and variance\n",
        "         - kappa : rate of mean reversion in variance process\n",
        "         - theta : long-term mean of variance process\n",
        "         - sigma : vol of vol / volatility of variance process\n",
        "         - T     : time of simulation\n",
        "         - N     : number of time steps\n",
        "         - M     : number of scenarios / simulations\n",
        "\n",
        "        Outputs:\n",
        "        - asset prices over time (numpy array)\n",
        "        - variance over time (numpy array)\n",
        "        \"\"\"\n",
        "        # initialise other parameters\n",
        "        dt = T/N\n",
        "        mu = np.array([0,0])\n",
        "        cov = np.array([[1,rho],\n",
        "                        [rho,1]])\n",
        "\n",
        "        # arrays for storing prices and variances\n",
        "        S = np.full(shape=(N+1,M), fill_value=S0)\n",
        "        v = np.full(shape=(N+1,M), fill_value=v0)\n",
        "\n",
        "        # sampling correlated brownian motions under risk-neutral measure\n",
        "        Z = np.random.multivariate_normal(mu, cov, (N,M))\n",
        "\n",
        "        for i in range(1,N+1):\n",
        "            S[i] = S[i-1] * np.exp( (r - 0.5*v[i-1])*dt + np.sqrt(v[i-1] * dt) * Z[i-1,:,0] )\n",
        "            v[i] = np.maximum(v[i-1] + kappa*(theta-v[i-1])*dt + sigma*np.sqrt(v[i-1]*dt)*Z[i-1,:,1],0)\n",
        "\n",
        "        return S, v\n",
        "\n",
        "    def simulate(self):\n",
        "        innovations = np.random.normal(0, 1, self.n)\n",
        "        stock_prices = np.zeros(self.n)\n",
        "        stock_prices[0] = self.s0\n",
        "\n",
        "        for i in range(1, self.n):\n",
        "            stock_prices[i] = stock_prices[i-1] * np.exp((self.r - 0.5 * self.sigma**2) * self.dt + self.sigma * np.sqrt(self.dt) * innovations[i])\n",
        "        return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N51TCa4m_1Jf",
        "outputId": "4c0419b4-0097-4a3a-8e7b-5f18cfca562c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QLBS Option Price at t=0: -0.0\n"
          ]
        }
      ],
      "source": [
        "class simulation():\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Parameters for simulation\n",
        "    T_steps = 50         # Number of time steps (T)\n",
        "    K_paths = 1000       # Number of Monte Carlo paths (K)\n",
        "    T_total = 1.0        # Total time horizon (years)\n",
        "    dt = T_total / T_steps\n",
        "    S0 = 100             # Initial stock price\n",
        "    r = 0.05             # Risk-free rate\n",
        "    sigma = 0.2          # Volatility\n",
        "    strike = 100         # Strike price (Z in pseudocode)\n",
        "    lambda_param = 0.5   # λ parameter\n",
        "\n",
        "    def simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt):\n",
        "      \"\"\"\n",
        "      Simulate stock prices using a geometric Brownian motion.\n",
        "      Returns an array S of shape (T_steps+1, K_paths).\n",
        "      \"\"\"\n",
        "      S = np.zeros((T_steps + 1, K_paths))\n",
        "      S[0] = S0\n",
        "      for t in range(1, T_steps + 1):\n",
        "          z = np.random.standard_normal(K_paths)\n",
        "          S[t] = S[t - 1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * z)\n",
        "      return S\n",
        "\n",
        "    # Simulate the stock paths\n",
        "    S = simulate_stock_prices(S0, r, sigma, T_steps, K_paths, dt)\n",
        "\n",
        "    # %% [code]\n",
        "    # Compute the state variable X.\n",
        "    # For example, one may take X as the log of S (common in QLBS literature).\n",
        "    X = np.log(S)\n",
        "\n",
        "    # %% [code]\n",
        "    # Define N basis functions; here we use a simple polynomial basis (constant, linear, quadratic)\n",
        "    def basis_function_1(x):\n",
        "        return np.ones_like(x)\n",
        "\n",
        "    def basis_function_2(x):\n",
        "        return x\n",
        "\n",
        "    def basis_function_3(x):\n",
        "        return x**2\n",
        "\n",
        "    basis_functions = [basis_function_1, basis_function_2, basis_function_3]\n",
        "    N_basis = len(basis_functions)\n",
        "\n",
        "    # Create the feature matrix phi with dimensions (T_steps+1, K_paths, N_basis)\n",
        "    phi = np.zeros((T_steps + 1, K_paths, N_basis))\n",
        "    for t in range(T_steps + 1):\n",
        "        for n, func in enumerate(basis_functions):\n",
        "            phi[t, :, n] = func(X[t])\n",
        "\n",
        "    # %% [code]\n",
        "    # Initialize arrays for the variables computed in the backward recursion\n",
        "    # a_star, Pi, R_star, and Q_star each have shape (T_steps+1, K_paths)\n",
        "    a_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Pi = np.zeros((T_steps + 1, K_paths))\n",
        "    R_star = np.zeros((T_steps + 1, K_paths))\n",
        "    Q_star = np.zeros((T_steps + 1, K_paths))\n",
        "\n",
        "    # Terminal conditions at t = T_steps\n",
        "    # Compute the option payoff: max(strike - S_T, 0)\n",
        "    Pi[T_steps] = np.maximum(strike - S[T_steps], 0)\n",
        "    # Center the terminal portfolio (subtracting the mean)\n",
        "    Pi_hat_T = Pi[T_steps] - np.mean(Pi[T_steps])\n",
        "    # Set terminal action to zero\n",
        "    a_star[T_steps] = 0\n",
        "    # Terminal risk measure (here using the variance of the payoff; note that var is a scalar)\n",
        "    R_star[T_steps] = -lambda_param * np.var(Pi[T_steps])\n",
        "    # Terminal Q-value (again, note that the λ·Var term is constant across paths)\n",
        "    Q_star[T_steps] = -Pi[T_steps] - lambda_param * np.var(Pi[T_steps])\n",
        "\n",
        "    # %% [code]\n",
        "    # Backward recursion from t = T_steps-1 to t = 0\n",
        "    for t in range(T_steps - 1, -1, -1):\n",
        "        # === Compute a_star[t] as in (44) ===\n",
        "        # Placeholder: in a full implementation, you would estimate a regression of the continuation value\n",
        "        # on the features phi[t]. Here we set it to zero.\n",
        "        a_star[t] = 0  # Replace with actual computation using phi[t]\n",
        "\n",
        "        # === Compute Pi[t] as in (29) ===\n",
        "        # Placeholder: here we mimic the terminal payoff but an actual update rule may be more complex.\n",
        "        Pi[t] = np.maximum(strike - S[t], 0)  # Replace with your QLBS update rule\n",
        "\n",
        "        # === Compute R_star[t] as in (41) ===\n",
        "        # Placeholder: you might compute a risk measure update here.\n",
        "        R_star[t] = R_star[t + 1]  # Replace with actual computation\n",
        "\n",
        "        # === Compute Q_star[t] as in (45) ===\n",
        "        # Placeholder: combine the immediate cost and risk measure.\n",
        "        Q_star[t] = -Pi[t] - lambda_param * np.var(Pi[t])  # Replace with the proper formula\n",
        "\n",
        "    # %% [code]\n",
        "    # Calculate the QLBS option price at t = 0:\n",
        "    # QLBS_price = - (1/K_paths) * sum_{k=1}^{K_paths} Q_star[0, k]\n",
        "    QLBS_price = -np.mean(Q_star[0])\n",
        "    print(\"QLBS Option Price at t=0:\", QLBS_price)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class stock():\n",
        "    def __init__(self, s0, r, sigma, T, n, model='gbm'):\n",
        "        self.s0 = s0\n",
        "        self.r = r\n",
        "        self.T = T\n",
        "        self.n = n\n",
        "        self.dt = T / n\n",
        "        self.model = model\n",
        "        self.sigma = sigma  # For GBM, sigma is constant; for Heston, sigma is the initial volatility.\n",
        "\n",
        "    def vol(self, sigma):\n",
        "        if self.model == 'gbm':\n",
        "            return np.array([sigma] * self.n)\n",
        "        elif self.model == 'heston':\n",
        "            # Heston model parameters\n",
        "            kappa = 2.0         # speed of mean reversion\n",
        "            theta = sigma**2    # long-run variance (theta)\n",
        "            xi = 0.1            # volatility of volatility\n",
        "            v = np.zeros(self.n)\n",
        "            v[0] = sigma**2     # initial variance\n",
        "            for i in range(1, self.n):\n",
        "                # Euler-Maruyama update for variance\n",
        "                dv = kappa * (theta - v[i-1]) * self.dt + xi * np.sqrt(max(v[i-1], 0)) * np.sqrt(self.dt) * np.random.normal()\n",
        "                v[i] = v[i-1] + dv\n",
        "                # Ensure non-negativity (using full truncation)\n",
        "                v[i] = max(v[i], 0)\n",
        "            # Return the volatility (sqrt of variance)\n",
        "            return np.sqrt(v)\n",
        "\n",
        "    def simulate(self):\n",
        "        if self.model == 'gbm':\n",
        "            innovations = np.random.normal(0, 1, self.n)\n",
        "            stock_prices = np.zeros(self.n)\n",
        "            stock_prices[0] = self.s0\n",
        "\n",
        "            for i in range(1, self.n):\n",
        "                stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                    (self.r - 0.5 * self.sigma**2) * self.dt + self.sigma * np.sqrt(self.dt) * innovations[i]\n",
        "                )\n",
        "            return stock_prices\n",
        "        elif self.model == 'heston':\n",
        "            # Use the Heston volatility path\n",
        "            vol_path = self.vol(self.sigma)\n",
        "            innovations = np.random.normal(0, 1, self.n)\n",
        "            stock_prices = np.zeros(self.n)\n",
        "            stock_prices[0] = self.s0\n",
        "\n",
        "            for i in range(1, self.n):\n",
        "                stock_prices[i] = stock_prices[i-1] * np.exp(\n",
        "                    (self.r - 0.5 * vol_path[i]**2) * self.dt + vol_path[i] * np.sqrt(self.dt) * innovations[i]\n",
        "                )\n",
        "            return stock_prices\n",
        "\n",
        "    def option_price(self, K):\n",
        "        stock_prices = self.simulate()\n",
        "        # European call option payoff at maturity\n",
        "        payoff = np.maximum(stock_prices[-1] - K, 0)\n",
        "        return np.exp(-self.r * self.T) * np.mean(payoff)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Using the Heston model\n",
        "    heston_stock = stock(s0=100, r=0.05, sigma=0.2, T=1.0, n=250, model='heston')\n",
        "    simulated_prices = heston_stock.simulate()\n",
        "    price = heston_stock.option_price(K=100)\n",
        "\n",
        "    print(\"Simulated Stock Prices (Heston):\")\n",
        "    print(simulated_prices)\n",
        "    print(\"\\nOption Price (Heston):\", price)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtbPF1rEB7NS",
        "outputId": "27a57fa4-92f7-4cd6-84e4-95b96c95f1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated Stock Prices (Heston):\n",
            "[100.          98.91305202  99.43182792  97.53779972  98.8724\n",
            "  99.19656941 100.44336647  98.9270396   99.84395398  99.15319125\n",
            " 100.8074453  102.1190933  102.11042751 105.22724871 104.58084624\n",
            " 104.61359933 107.26965634 108.05314354 107.45533476 107.2164884\n",
            " 107.62231566 107.47808574 105.11108455 105.79578246 103.20656039\n",
            " 104.8245379  104.60738242 105.43321864 108.32742552 108.91486128\n",
            " 110.2893015  108.09804227 107.31331089 107.30651256 101.71671981\n",
            " 102.25448122 101.88618472 101.15174631 101.09559578 103.94466639\n",
            " 102.04255386 102.11887704 102.01115269 103.09921013 102.14502885\n",
            " 100.64161319 101.59005442  99.3393346  100.95439171 104.68429045\n",
            " 103.99471146 103.58428115 102.77856025 100.57933053  98.96480844\n",
            "  98.45224563 100.6457208  103.18071316 103.36527087 102.59838816\n",
            " 101.35869774 100.80103924 101.88398991 102.85482407 102.20052193\n",
            " 102.32701967  98.38308865  96.76184744  97.4527611   98.41376635\n",
            "  98.20255286  98.54713218  95.32112937  96.13652205  95.71757247\n",
            "  96.70109222  94.90984975  94.87632097  96.42730004  94.97704851\n",
            "  95.11880982  96.08496115  97.88321032  98.05120621  99.28650944\n",
            "  98.17708879  98.66033957  96.85591233  95.0630164   94.50638416\n",
            "  98.30655513  95.7863325   93.39347196  92.5296408   90.23352427\n",
            "  90.58263414  93.41512786  95.82772447  95.48912202  94.30490743\n",
            "  94.0357253   95.55746107  95.40672124  97.15582499  95.33220315\n",
            "  95.09716714  96.81079817  95.33702555  93.36133214  90.5019891\n",
            "  92.2311404   93.86721738  92.90871906  91.00751021  93.56737138\n",
            "  95.32954308  94.08462553  90.59828768  92.15224582  92.62819431\n",
            "  94.13179067  92.21624565  90.75525557  92.81444912  93.22052929\n",
            "  91.22075507  89.5197254   89.20041176  87.79078233  89.18573187\n",
            "  88.4971678   87.21217826  85.8262863   87.22786482  88.91943619\n",
            "  87.09386862  88.39420468  88.62973473  89.02763256  89.21056933\n",
            "  89.9520184   89.811096    89.91824586  88.89533598  90.70893657\n",
            "  91.51722896  93.00873399  92.26731168  90.79211456  88.67922405\n",
            "  90.96877389  90.94646761  89.78866606  91.57349162  92.33803205\n",
            "  90.49896787  89.53998936  89.89383046  91.85415493  92.21992959\n",
            "  94.04940952  95.87604334  93.9393798   93.72539841  93.33689672\n",
            "  94.99487649  93.36906532  96.02196472  95.81191746  96.47631895\n",
            "  96.290682    96.89221651  96.71839434  95.10694503  97.44835634\n",
            "  95.50289345  97.69939647  98.08944419  96.64681638  96.2594159\n",
            "  96.72407284  97.07560032  98.53116647  99.40013248  97.11965255\n",
            "  97.60515902  96.89662762  95.79211615  93.22234303  92.18405541\n",
            "  90.61403117  90.60582044  90.35155492  91.34018721  91.7125848\n",
            "  90.39022676  91.3853969   92.31964371  91.32720943  90.68773835\n",
            "  90.55646666  90.48102687  90.70975398  90.07258416  88.52282175\n",
            "  91.79401655  92.47280477  91.39208745  91.44334447  90.93778446\n",
            "  92.4623098   91.83079856  93.61853332  91.5673446   89.73541678\n",
            "  91.05277398  91.22294836  89.94269808  90.86343724  92.67180248\n",
            "  90.51852256  89.03662711  89.55491033  88.29112375  87.430833\n",
            "  89.34019734  88.74186505  89.84749081  91.8778304   91.61310077\n",
            "  93.87089742  95.11718652  93.48544427  94.28453237  93.52104019\n",
            "  93.08891971  93.88277104  94.12217789  94.99954613  95.67262019\n",
            "  96.4500523   97.86733548  97.89118309  97.5309943   97.52721975\n",
            "  97.22941984  98.6818838   97.61769069  98.44617412  97.574611  ]\n",
            "\n",
            "Option Price (Heston): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a pip package in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install redis\n",
        "\n",
        "# Connect to Redis\n",
        "r = redis.Redis(host='localhost', port=6379, db=0)\n",
        "\n",
        "# Set a value in Redis\n",
        "r.set('Stock', 'NVDA', 'Price', '99.60')\n",
        "\n",
        "# Get a value from Redis\n",
        "stock = r.get('stock')\n",
        "print(stock)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "ZciZCY_VbG0J",
        "outputId": "f3c4d260-2013-430f-bec7-5666a63430e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Downloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/261.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-5.2.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'redis' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-849d351a06e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Connect to Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRedis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'localhost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6379\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set a value in Redis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'redis' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as D\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##############################\n",
        "# Environment Implementation #\n",
        "##############################\n",
        "\n",
        "class HedgingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom hedging environment.\n",
        "\n",
        "    - Dynamics: Underlying asset follows a geometric Brownian motion.\n",
        "      If stochastic_vol==True, volatility updates via a simplified SABR model.\n",
        "    - Reward: At each timestep the reward is the change in wealth adjusted by transaction costs,\n",
        "      and a risk penalty scaling the variance (approximated here via the squared change).\n",
        "    - At t=0, the agent buys the replicating portfolio.\n",
        "    - At maturity, the terminal payoff of a European call is subtracted.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 T=1.0,            # time horizon (e.g. 1 year)\n",
        "                 n_steps=50,       # number of timesteps per episode\n",
        "                 S0=100.0,         # initial asset price\n",
        "                 sigma0=0.2,       # initial volatility\n",
        "                 kappa=0.001,      # transaction cost parameter\n",
        "                 risk_aversion=0.01,  # risk–penalty parameter lambda\n",
        "                 strike=100.0,     # strike price of the option (European call)\n",
        "                 nu=0.1,           # vol-of-vol (for SABR dynamics)\n",
        "                 rho=-0.3,         # correlation between asset and volatility shocks\n",
        "                 stochastic_vol=True):  # whether to use stochastic volatility dynamics\n",
        "        super(HedgingEnv, self).__init__()\n",
        "        self.T = T\n",
        "        self.n_steps = n_steps\n",
        "        self.dt = T / n_steps\n",
        "        self.S0 = S0\n",
        "        self.sigma0 = sigma0\n",
        "        self.kappa = kappa\n",
        "        self.risk_aversion = risk_aversion\n",
        "        self.strike = strike\n",
        "        self.nu = nu\n",
        "        self.rho = rho\n",
        "        self.stochastic_vol = stochastic_vol\n",
        "\n",
        "        # Continuous action: hedge position. (We assume it can be any real number.)\n",
        "        self.action_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
        "        # State: we use [S, sigma, previous hedge, normalized time]\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.S = self.S0\n",
        "        self.sigma = self.sigma0\n",
        "        self.a_prev = 0.0  # initial hedge (no position)\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, 0.0], dtype=np.float32)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        action = float(action)  # ensure scalar\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        # Compute hedge adjustment cost and change in wealth\n",
        "        trade = self.a_prev - action  # change in hedge\n",
        "        delta_w = self.S * trade - self.kappa * abs(self.S * trade)\n",
        "        risk_penalty = self.risk_aversion * (delta_w ** 2)\n",
        "        reward = delta_w - risk_penalty\n",
        "\n",
        "        if self.t == 0:\n",
        "            # At initial step, buying replicating portfolio\n",
        "            reward = - self.S * action - self.kappa * abs(self.S * action)\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        # Update underlying dynamics:\n",
        "        Z1 = np.random.normal()\n",
        "        if self.stochastic_vol:\n",
        "            # Stochastic volatility update using a simplified SABR-like model\n",
        "            Z2 = np.random.normal()\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            self.sigma = self.sigma + self.nu * self.sigma * np.sqrt(self.dt) * (\n",
        "                self.rho * Z1 + np.sqrt(1 - self.rho ** 2) * Z2)\n",
        "            # Ensure sigma stays positive\n",
        "            self.sigma = max(self.sigma, 1e-3)\n",
        "        else:\n",
        "            # Constant volatility dynamics\n",
        "            self.S = self.S * (1 + self.sigma * np.sqrt(self.dt) * Z1)\n",
        "            # sigma remains constant\n",
        "            self.sigma = self.sigma0\n",
        "\n",
        "        self.a_prev = action\n",
        "        time_frac = self.t / self.n_steps\n",
        "        if self.t >= self.n_steps:\n",
        "            # Terminal reward: liquidate position and subtract option payoff.\n",
        "            option_payoff = max(self.S - self.strike, 0)\n",
        "            final_delta = self.S * action - self.kappa * abs(self.S * action) - option_payoff\n",
        "            reward = final_delta\n",
        "            done = True\n",
        "\n",
        "        self.state = np.array([self.S, self.sigma, self.a_prev, time_frac], dtype=np.float32)\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "##################################\n",
        "# Helper: Monte Carlo Simulation #\n",
        "##################################\n",
        "\n",
        "def simulate_paths(env, N=1000):\n",
        "    \"\"\"\n",
        "    Simulate N Monte Carlo paths using the environment dynamics.\n",
        "    Each episode represents one path.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "    returns = []\n",
        "    for _ in range(N):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_states = [state]\n",
        "        while not done:\n",
        "            # For simulation here, use a fixed policy (e.g., do nothing, a=0)\n",
        "            action = 0.0\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_states.append(state)\n",
        "        paths.append(episode_states)\n",
        "        returns.append(episode_reward)\n",
        "    return paths, returns\n",
        "\n",
        "#####################################\n",
        "# Tabular Q-Learning Implementation #\n",
        "#####################################\n",
        "\n",
        "class TabularQAgent:\n",
        "    def __init__(self, env, price_bins=50, action_low=-1.0, action_high=1.0, action_step=0.01,\n",
        "                 alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.env = env\n",
        "        self.price_bins = np.linspace(0, 2 * env.S0, price_bins)\n",
        "        self.n_time = env.n_steps + 1  # include terminal step\n",
        "        self.actions = np.arange(action_low, action_high + action_step, action_step)\n",
        "        self.n_actions = len(self.actions)\n",
        "        # Q-table indexed by (price_index, time_index)\n",
        "        self.Q = np.zeros((price_bins, self.n_time, self.n_actions))\n",
        "        # Hyperparameters\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "    def discretize_state(self, state):\n",
        "        # For tabular Q-learning we use only the price S and discrete time.\n",
        "        S, _, _, t_frac = state\n",
        "        price_idx = np.digitize(S, self.price_bins) - 1\n",
        "        price_idx = np.clip(price_idx, 0, len(self.price_bins)-1)\n",
        "        time_idx = int(t_frac * self.env.n_steps)\n",
        "        time_idx = np.clip(time_idx, 0, self.n_time-1)\n",
        "        return price_idx, time_idx\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        price_idx, time_idx = self.discretize_state(state)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Explore: choose a random discrete action index.\n",
        "            a_idx = np.random.randint(self.n_actions)\n",
        "        else:\n",
        "            a_idx = np.argmax(self.Q[price_idx, time_idx])\n",
        "        return self.actions[a_idx], a_idx\n",
        "\n",
        "    def update(self, state, action_idx, reward, next_state, done):\n",
        "        s_idx = self.discretize_state(state)\n",
        "        next_s_idx = self.discretize_state(next_state)\n",
        "        price_idx, time_idx = s_idx\n",
        "        n_price_idx, n_time_idx = next_s_idx\n",
        "\n",
        "        best_next = 0 if done else np.max(self.Q[n_price_idx, n_time_idx])\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        td_error = td_target - self.Q[price_idx, time_idx, action_idx]\n",
        "        self.Q[price_idx, time_idx, action_idx] += self.alpha * td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "def train_tabular_q_learning(env, n_episodes=1000):\n",
        "    agent = TabularQAgent(env)\n",
        "    rewards_per_episode = []\n",
        "    for ep in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            action, action_idx = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update(state, action_idx, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        agent.decay_epsilon()\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if ep % 50 == 0:\n",
        "            print(f\"[Tabular Q] Episode {ep}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###################################\n",
        "# DDPG Implementation (Lillicrap 2019) #\n",
        "###################################\n",
        "\n",
        "# Define Actor and Critic networks for DDPG\n",
        "class DDPGActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = self.out(x)\n",
        "        return action\n",
        "\n",
        "class DDPGCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(DDPGCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.fc1(torch.cat([state, action], dim=1)))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_value = self.out(x)\n",
        "        return q_value\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "def train_ddpg(env, n_episodes=500, batch_size=64, gamma=0.99, tau=0.005):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = DDPGActor(state_dim, action_dim)\n",
        "    critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor = DDPGActor(state_dim, action_dim)\n",
        "    target_critic = DDPGCritic(state_dim, action_dim)\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "    buffer = ReplayBuffer()\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action = actor(state_tensor).detach().numpy()[0]\n",
        "            # Add exploratory noise (Gaussian)\n",
        "            action = action + np.random.normal(0, 0.1, size=action_dim)\n",
        "            next_state, reward, done, _ = env.step(action[0])\n",
        "            buffer.push(state, action, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            # Update if enough samples are available\n",
        "            if len(buffer) > batch_size:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "                states_tensor = torch.FloatTensor(states)\n",
        "                actions_tensor = torch.FloatTensor(actions)\n",
        "                rewards_tensor = torch.FloatTensor(rewards).unsqueeze(1)\n",
        "                next_states_tensor = torch.FloatTensor(next_states)\n",
        "                dones_tensor = torch.FloatTensor(dones).unsqueeze(1)\n",
        "\n",
        "                # Critic update\n",
        "                next_actions = target_actor(next_states_tensor)\n",
        "                target_q = target_critic(next_states_tensor, next_actions)\n",
        "                expected_q = rewards_tensor + gamma * (1 - dones_tensor) * target_q\n",
        "                current_q = critic(states_tensor, actions_tensor)\n",
        "                critic_loss = nn.MSELoss()(current_q, expected_q.detach())\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                # Actor update\n",
        "                actor_loss = -critic(states_tensor, actor(states_tensor)).mean()\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                # Update target networks\n",
        "                for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "                for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data + (1-tau)*target_param.data)\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[DDPG] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "###########################################\n",
        "# PPO Implementation (Schulman et al. 2017) #\n",
        "###########################################\n",
        "\n",
        "class PPOActor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super(PPOActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Log_std parameter for Gaussian exploration\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "class PPOCritic(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_size=64):\n",
        "        super(PPOCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        value = self.out(x)\n",
        "        return value\n",
        "\n",
        "def train_ppo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4, update_epochs=5):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        # Compute rewards-to-go\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        # Normalize returns\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # Convert trajectory to tensors\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        # Multiple update epochs on the same trajectory\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            # PPO clipped objective\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[PPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "#############################################\n",
        "# GRPO Implementation (DeepSeek 2024 variant)#\n",
        "#############################################\n",
        "\n",
        "# For this demonstration, we assume GRPO augments the PPO update with an additional gradient–reward regularization term.\n",
        "# The implementation here is similar in structure to PPO with an extra term that penalizes large deviations in the gradient.\n",
        "def train_grpo(env, n_episodes=500, clip_epsilon=0.2, gamma=0.99, lr=3e-4,\n",
        "               update_epochs=5, grad_reg_coeff=0.1):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = PPOActor(state_dim, action_dim)\n",
        "    critic = PPOCritic(state_dim)\n",
        "    optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        trajectory = []\n",
        "        total_reward = 0.0\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            mean, std = actor(state_tensor)\n",
        "            dist = D.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            trajectory.append((state, action.item(), reward, log_prob.item()))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for (_, _, r, _) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        states = torch.FloatTensor([s for (s, a, r, lp) in trajectory])\n",
        "        actions = torch.FloatTensor([[a] for (s, a, r, lp) in trajectory])\n",
        "        old_log_probs = torch.FloatTensor([[lp] for (s, a, r, lp) in trajectory])\n",
        "\n",
        "        for _ in range(update_epochs):\n",
        "            mean, std = actor(states)\n",
        "            dist = D.Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            new_log_probs = new_log_probs.unsqueeze(1)\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            values = critic(states)\n",
        "            advantages = returns.unsqueeze(1) - values.detach()\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns.unsqueeze(1))\n",
        "\n",
        "            # Additional gradient regularization (a penalty on the norm of the actor gradient)\n",
        "            optimizer.zero_grad()\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "            loss.backward()\n",
        "            grad_norm = 0.0\n",
        "            for param in actor.parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm += param.grad.data.norm(2)\n",
        "            # Add penalty term\n",
        "            loss = loss + grad_reg_coeff * grad_norm\n",
        "            optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"[GRPO] Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "    return rewards_per_episode\n",
        "\n",
        "####################################\n",
        "# Main comparison and experiment   #\n",
        "####################################\n",
        "\n",
        "def main():\n",
        "    # Number of Monte Carlo paths is given by the number of episodes we simulate.\n",
        "    n_mc = 500  # for demonstration we use 500 episodes per algorithm\n",
        "    print(\"======== Running Tabular Q-Learning (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_q = train_tabular_q_learning(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ddpg = train_ddpg(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_ppo = train_ppo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Constant Volatility) ========\")\n",
        "    env_const = HedgingEnv(stochastic_vol=False)\n",
        "    rewards_grpo = train_grpo(env_const, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for constant volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Constant Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Now repeat with stochastic volatility dynamics.\n",
        "    print(\"\\n======== Running Tabular Q-Learning (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_q_stoch = train_tabular_q_learning(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running DDPG (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ddpg_stoch = train_ddpg(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running PPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_ppo_stoch = train_ppo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    print(\"\\n======== Running GRPO (Stochastic Volatility) ========\")\n",
        "    env_stoch = HedgingEnv(stochastic_vol=True)\n",
        "    rewards_grpo_stoch = train_grpo(env_stoch, n_episodes=n_mc)\n",
        "\n",
        "    # Plot convergence curves for stochastic volatility\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(rewards_q_stoch, label=\"Tabular Q-Learning\")\n",
        "    plt.plot(rewards_ddpg_stoch, label=\"DDPG\")\n",
        "    plt.plot(rewards_ppo_stoch, label=\"PPO\")\n",
        "    plt.plot(rewards_grpo_stoch, label=\"GRPO\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"Performance Comparison (Stochastic Volatility)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRBVQTcZu20Z",
        "outputId": "5ba5ca8c-39eb-43f7-e9fb-365e1749ad9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Running Tabular Q-Learning (Constant Volatility) ========\n",
            "[Tabular Q] Episode 0, Total Reward: -2260.85, Epsilon: 0.995\n",
            "[Tabular Q] Episode 50, Total Reward: -2682.09, Epsilon: 0.774\n",
            "[Tabular Q] Episode 100, Total Reward: -5369.04, Epsilon: 0.603\n",
            "[Tabular Q] Episode 150, Total Reward: -3221.20, Epsilon: 0.469\n",
            "[Tabular Q] Episode 200, Total Reward: -2881.90, Epsilon: 0.365\n",
            "[Tabular Q] Episode 250, Total Reward: -4953.80, Epsilon: 0.284\n",
            "[Tabular Q] Episode 300, Total Reward: -3136.11, Epsilon: 0.221\n",
            "[Tabular Q] Episode 350, Total Reward: -3065.78, Epsilon: 0.172\n",
            "[Tabular Q] Episode 400, Total Reward: -1207.62, Epsilon: 0.134\n",
            "[Tabular Q] Episode 450, Total Reward: -719.54, Epsilon: 0.104\n",
            "[Tabular Q] Episode 500, Total Reward: -754.68, Epsilon: 0.081\n",
            "[Tabular Q] Episode 550, Total Reward: -1046.49, Epsilon: 0.063\n",
            "[Tabular Q] Episode 600, Total Reward: -849.06, Epsilon: 0.049\n",
            "[Tabular Q] Episode 650, Total Reward: -1896.88, Epsilon: 0.038\n",
            "[Tabular Q] Episode 700, Total Reward: -253.64, Epsilon: 0.030\n",
            "[Tabular Q] Episode 750, Total Reward: -33.20, Epsilon: 0.023\n",
            "[Tabular Q] Episode 800, Total Reward: -632.65, Epsilon: 0.018\n",
            "[Tabular Q] Episode 850, Total Reward: -531.08, Epsilon: 0.014\n",
            "[Tabular Q] Episode 900, Total Reward: -225.86, Epsilon: 0.011\n",
            "[Tabular Q] Episode 950, Total Reward: -575.47, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1000, Total Reward: -649.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1050, Total Reward: -37.07, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1100, Total Reward: -1358.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1150, Total Reward: -236.39, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1200, Total Reward: -243.28, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1250, Total Reward: 43.85, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1300, Total Reward: -1249.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1350, Total Reward: -858.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1400, Total Reward: -269.15, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1450, Total Reward: -1797.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1500, Total Reward: -2207.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1550, Total Reward: -1935.33, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1600, Total Reward: -451.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1650, Total Reward: -1259.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1700, Total Reward: -1078.18, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1750, Total Reward: -1158.99, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1800, Total Reward: -3005.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1850, Total Reward: -1354.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1900, Total Reward: -2707.19, Epsilon: 0.010\n",
            "[Tabular Q] Episode 1950, Total Reward: -2857.99, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2000, Total Reward: -1847.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2050, Total Reward: -751.07, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2100, Total Reward: -3437.95, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2150, Total Reward: -3015.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2200, Total Reward: -2008.57, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2250, Total Reward: -1625.74, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2300, Total Reward: -1030.69, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2350, Total Reward: -3780.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2400, Total Reward: -2795.21, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2450, Total Reward: -3793.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2500, Total Reward: -2317.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2550, Total Reward: -861.75, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2600, Total Reward: -2120.76, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2650, Total Reward: -964.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2700, Total Reward: -2011.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2750, Total Reward: -1434.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2800, Total Reward: -1787.88, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2850, Total Reward: -1662.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2900, Total Reward: -992.08, Epsilon: 0.010\n",
            "[Tabular Q] Episode 2950, Total Reward: -1924.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3000, Total Reward: -2370.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3050, Total Reward: -2046.15, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3100, Total Reward: -1123.55, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3150, Total Reward: -2984.78, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3200, Total Reward: -1180.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3250, Total Reward: -2115.56, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3300, Total Reward: -1466.04, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3350, Total Reward: -1379.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3400, Total Reward: -915.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3450, Total Reward: -2076.79, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3500, Total Reward: -3173.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3550, Total Reward: -2318.37, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3600, Total Reward: -2330.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3650, Total Reward: -5800.54, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3700, Total Reward: -2850.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3750, Total Reward: -1530.11, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3800, Total Reward: -3653.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3850, Total Reward: -1636.97, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3900, Total Reward: -1756.55, Epsilon: 0.010\n",
            "[Tabular Q] Episode 3950, Total Reward: -1993.89, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4000, Total Reward: -1021.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4050, Total Reward: -3416.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4100, Total Reward: -2875.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4150, Total Reward: -795.31, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4200, Total Reward: -3712.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4250, Total Reward: -1174.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4300, Total Reward: -2328.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4350, Total Reward: -462.42, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4400, Total Reward: -1709.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4450, Total Reward: -3318.98, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4500, Total Reward: -2942.51, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4550, Total Reward: -1270.11, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4600, Total Reward: -1780.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4650, Total Reward: -3568.24, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4700, Total Reward: -990.30, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4750, Total Reward: -2346.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4800, Total Reward: -2377.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4850, Total Reward: -1234.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4900, Total Reward: -1237.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 4950, Total Reward: -4481.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5000, Total Reward: -1223.23, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5050, Total Reward: -2741.82, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5100, Total Reward: -1908.10, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5150, Total Reward: -1849.76, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5200, Total Reward: -561.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5250, Total Reward: -438.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5300, Total Reward: -2743.84, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5350, Total Reward: -660.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5400, Total Reward: -1278.00, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5450, Total Reward: -2535.60, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5500, Total Reward: -2299.96, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5550, Total Reward: -1336.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5600, Total Reward: -1244.87, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5650, Total Reward: -7.64, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5700, Total Reward: -2649.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5750, Total Reward: -5325.01, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5800, Total Reward: -2232.18, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5850, Total Reward: -1722.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5900, Total Reward: -2259.91, Epsilon: 0.010\n",
            "[Tabular Q] Episode 5950, Total Reward: -1241.49, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6000, Total Reward: -788.04, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6050, Total Reward: -1892.65, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6100, Total Reward: -665.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6150, Total Reward: -2153.61, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6200, Total Reward: -3364.83, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6250, Total Reward: 149.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6300, Total Reward: -2119.24, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6350, Total Reward: -1562.63, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6400, Total Reward: -2116.21, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6450, Total Reward: -582.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6500, Total Reward: -523.12, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6550, Total Reward: -472.05, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6600, Total Reward: -2958.23, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6650, Total Reward: -1739.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6700, Total Reward: -880.65, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6750, Total Reward: -4722.32, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6800, Total Reward: -937.29, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6850, Total Reward: -358.94, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6900, Total Reward: -1172.83, Epsilon: 0.010\n",
            "[Tabular Q] Episode 6950, Total Reward: -1714.53, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7000, Total Reward: -280.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7050, Total Reward: -2379.35, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7100, Total Reward: -530.10, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7150, Total Reward: -1434.97, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7200, Total Reward: 63.14, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7250, Total Reward: -1346.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7300, Total Reward: -46.79, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7350, Total Reward: -418.22, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7400, Total Reward: -585.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7450, Total Reward: -527.20, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7500, Total Reward: 63.57, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7550, Total Reward: -353.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7600, Total Reward: -620.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7650, Total Reward: -2752.81, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7700, Total Reward: -568.58, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7750, Total Reward: -2169.78, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7800, Total Reward: -402.02, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7850, Total Reward: -602.56, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7900, Total Reward: -826.50, Epsilon: 0.010\n",
            "[Tabular Q] Episode 7950, Total Reward: -1423.49, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8000, Total Reward: 42.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8050, Total Reward: -2109.68, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8100, Total Reward: -900.73, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8150, Total Reward: -794.26, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8200, Total Reward: -658.72, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8250, Total Reward: -309.44, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8300, Total Reward: -1894.80, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8350, Total Reward: -876.31, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8400, Total Reward: -2353.93, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8450, Total Reward: -1286.44, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8500, Total Reward: -710.13, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8550, Total Reward: -420.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8600, Total Reward: 36.40, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8650, Total Reward: -3801.19, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8700, Total Reward: -2.71, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8750, Total Reward: -358.33, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8800, Total Reward: -336.48, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8850, Total Reward: 129.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8900, Total Reward: -92.30, Epsilon: 0.010\n",
            "[Tabular Q] Episode 8950, Total Reward: -463.27, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9000, Total Reward: -3004.86, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9050, Total Reward: -3649.45, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9100, Total Reward: -1592.92, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9150, Total Reward: 94.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9200, Total Reward: -207.08, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9250, Total Reward: -222.59, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9300, Total Reward: 114.09, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9350, Total Reward: -1526.32, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9400, Total Reward: -325.64, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9450, Total Reward: 96.92, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9500, Total Reward: -111.67, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9550, Total Reward: -278.98, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9600, Total Reward: -791.52, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9650, Total Reward: -321.62, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9700, Total Reward: -3298.17, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9750, Total Reward: -816.06, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9800, Total Reward: -1716.89, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9850, Total Reward: -141.61, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9900, Total Reward: 67.41, Epsilon: 0.010\n",
            "[Tabular Q] Episode 9950, Total Reward: -470.67, Epsilon: 0.010\n",
            "\n",
            "======== Running DDPG (Constant Volatility) ========\n",
            "[DDPG] Episode 0, Total Reward: -162.65\n",
            "[DDPG] Episode 20, Total Reward: -34249.82\n",
            "[DDPG] Episode 40, Total Reward: -37419.99\n",
            "[DDPG] Episode 60, Total Reward: -9521.75\n",
            "[DDPG] Episode 80, Total Reward: -22137.08\n",
            "[DDPG] Episode 100, Total Reward: -17903.48\n",
            "[DDPG] Episode 120, Total Reward: -8689.85\n",
            "[DDPG] Episode 140, Total Reward: -22162.90\n",
            "[DDPG] Episode 160, Total Reward: -4404.14\n",
            "[DDPG] Episode 180, Total Reward: -10883.51\n",
            "[DDPG] Episode 200, Total Reward: -7462.49\n",
            "[DDPG] Episode 220, Total Reward: -17484.27\n",
            "[DDPG] Episode 240, Total Reward: -6433.27\n",
            "[DDPG] Episode 260, Total Reward: -5798.70\n",
            "[DDPG] Episode 280, Total Reward: -8445.03\n",
            "[DDPG] Episode 300, Total Reward: -17433.42\n",
            "[DDPG] Episode 320, Total Reward: -10286.36\n",
            "[DDPG] Episode 340, Total Reward: -12457.67\n",
            "[DDPG] Episode 360, Total Reward: -10084.60\n",
            "[DDPG] Episode 380, Total Reward: -9484.26\n",
            "[DDPG] Episode 400, Total Reward: -9739.51\n",
            "[DDPG] Episode 420, Total Reward: -8270.05\n",
            "[DDPG] Episode 440, Total Reward: -2238.75\n",
            "[DDPG] Episode 460, Total Reward: -3484.02\n",
            "[DDPG] Episode 480, Total Reward: -1777.63\n",
            "[DDPG] Episode 500, Total Reward: -195.24\n",
            "[DDPG] Episode 520, Total Reward: -288.00\n",
            "[DDPG] Episode 540, Total Reward: -2198.73\n",
            "[DDPG] Episode 560, Total Reward: -85458.86\n",
            "[DDPG] Episode 580, Total Reward: -248646.30\n",
            "[DDPG] Episode 600, Total Reward: -33235.52\n",
            "[DDPG] Episode 620, Total Reward: -66204.60\n",
            "[DDPG] Episode 640, Total Reward: -33974.92\n",
            "[DDPG] Episode 660, Total Reward: -32641.69\n",
            "[DDPG] Episode 680, Total Reward: -17492.34\n",
            "[DDPG] Episode 700, Total Reward: -6906.51\n",
            "[DDPG] Episode 720, Total Reward: -16163.68\n",
            "[DDPG] Episode 740, Total Reward: -17828.27\n",
            "[DDPG] Episode 760, Total Reward: -104582.80\n",
            "[DDPG] Episode 780, Total Reward: -117257.44\n",
            "[DDPG] Episode 800, Total Reward: -25415.48\n",
            "[DDPG] Episode 820, Total Reward: -82194.61\n",
            "[DDPG] Episode 840, Total Reward: -14063.03\n",
            "[DDPG] Episode 860, Total Reward: -2877.31\n",
            "[DDPG] Episode 880, Total Reward: -2126.73\n",
            "[DDPG] Episode 900, Total Reward: -15162.42\n",
            "[DDPG] Episode 920, Total Reward: -79476.24\n",
            "[DDPG] Episode 940, Total Reward: -159669.29\n",
            "[DDPG] Episode 960, Total Reward: -277042.03\n",
            "[DDPG] Episode 980, Total Reward: -73943.30\n",
            "[DDPG] Episode 1000, Total Reward: -39015.76\n",
            "[DDPG] Episode 1020, Total Reward: -22603.29\n",
            "[DDPG] Episode 1040, Total Reward: -22206.40\n",
            "[DDPG] Episode 1060, Total Reward: -33888.92\n",
            "[DDPG] Episode 1080, Total Reward: -42160.10\n",
            "[DDPG] Episode 1100, Total Reward: -57816.25\n",
            "[DDPG] Episode 1120, Total Reward: -80167.67\n",
            "[DDPG] Episode 1140, Total Reward: -81982.42\n",
            "[DDPG] Episode 1160, Total Reward: -68066.27\n",
            "[DDPG] Episode 1180, Total Reward: -37696.80\n",
            "[DDPG] Episode 1200, Total Reward: -65341.57\n",
            "[DDPG] Episode 1220, Total Reward: -54184.37\n",
            "[DDPG] Episode 1240, Total Reward: -409889.12\n",
            "[DDPG] Episode 1260, Total Reward: -42673.74\n",
            "[DDPG] Episode 1280, Total Reward: -56196.82\n",
            "[DDPG] Episode 1300, Total Reward: -71562.77\n",
            "[DDPG] Episode 1320, Total Reward: -20534.45\n",
            "[DDPG] Episode 1340, Total Reward: -55999.61\n",
            "[DDPG] Episode 1360, Total Reward: -4295480.53\n",
            "[DDPG] Episode 1380, Total Reward: -18336.17\n",
            "[DDPG] Episode 1400, Total Reward: -13937.70\n",
            "[DDPG] Episode 1420, Total Reward: -4011.91\n",
            "[DDPG] Episode 1440, Total Reward: -11510.11\n",
            "[DDPG] Episode 1460, Total Reward: -12277.87\n",
            "[DDPG] Episode 1480, Total Reward: -6517.06\n",
            "[DDPG] Episode 1500, Total Reward: -13578.39\n",
            "[DDPG] Episode 1520, Total Reward: -1779.47\n",
            "[DDPG] Episode 1540, Total Reward: -18097.41\n",
            "[DDPG] Episode 1560, Total Reward: -51949.42\n",
            "[DDPG] Episode 1580, Total Reward: -8433403.70\n",
            "[DDPG] Episode 1600, Total Reward: -5225361.56\n",
            "[DDPG] Episode 1620, Total Reward: -61831.35\n",
            "[DDPG] Episode 1640, Total Reward: -80659.91\n",
            "[DDPG] Episode 1660, Total Reward: -7057.13\n",
            "[DDPG] Episode 1680, Total Reward: -9368.26\n",
            "[DDPG] Episode 1700, Total Reward: -712.34\n",
            "[DDPG] Episode 1720, Total Reward: -1609.21\n",
            "[DDPG] Episode 1740, Total Reward: -1440.71\n",
            "[DDPG] Episode 1760, Total Reward: -1092.29\n",
            "[DDPG] Episode 1780, Total Reward: -471.38\n",
            "[DDPG] Episode 1800, Total Reward: -1271.59\n",
            "[DDPG] Episode 1820, Total Reward: -1702.54\n",
            "[DDPG] Episode 1840, Total Reward: -463.08\n",
            "[DDPG] Episode 1860, Total Reward: -218.61\n",
            "[DDPG] Episode 1880, Total Reward: -1130.89\n",
            "[DDPG] Episode 1900, Total Reward: -611.37\n",
            "[DDPG] Episode 1920, Total Reward: -284.71\n",
            "[DDPG] Episode 1940, Total Reward: -662.55\n",
            "[DDPG] Episode 1960, Total Reward: -365.32\n",
            "[DDPG] Episode 1980, Total Reward: -1066.24\n",
            "[DDPG] Episode 2000, Total Reward: -102.67\n",
            "[DDPG] Episode 2020, Total Reward: -941.71\n",
            "[DDPG] Episode 2040, Total Reward: -232.66\n",
            "[DDPG] Episode 2060, Total Reward: -317.70\n",
            "[DDPG] Episode 2080, Total Reward: -824.90\n",
            "[DDPG] Episode 2100, Total Reward: -80.10\n",
            "[DDPG] Episode 2120, Total Reward: -197.33\n",
            "[DDPG] Episode 2140, Total Reward: -169.47\n",
            "[DDPG] Episode 2160, Total Reward: -590.77\n",
            "[DDPG] Episode 2180, Total Reward: -314.08\n",
            "[DDPG] Episode 2200, Total Reward: -263.25\n",
            "[DDPG] Episode 2220, Total Reward: -979.11\n",
            "[DDPG] Episode 2240, Total Reward: -264.99\n",
            "[DDPG] Episode 2260, Total Reward: -279.58\n",
            "[DDPG] Episode 2280, Total Reward: -328.27\n",
            "[DDPG] Episode 2300, Total Reward: -694.10\n",
            "[DDPG] Episode 2320, Total Reward: -895.40\n",
            "[DDPG] Episode 2340, Total Reward: -2582.90\n",
            "[DDPG] Episode 2360, Total Reward: -3548.33\n",
            "[DDPG] Episode 2380, Total Reward: -4668.36\n",
            "[DDPG] Episode 2400, Total Reward: -2926.27\n",
            "[DDPG] Episode 2420, Total Reward: -1308.78\n",
            "[DDPG] Episode 2440, Total Reward: -387.22\n",
            "[DDPG] Episode 2460, Total Reward: -681.02\n",
            "[DDPG] Episode 2480, Total Reward: -248.37\n",
            "[DDPG] Episode 2500, Total Reward: -240.55\n",
            "[DDPG] Episode 2520, Total Reward: -128.37\n",
            "[DDPG] Episode 2540, Total Reward: -349.71\n",
            "[DDPG] Episode 2560, Total Reward: -289.47\n",
            "[DDPG] Episode 2580, Total Reward: -165.17\n",
            "[DDPG] Episode 2600, Total Reward: -120.84\n",
            "[DDPG] Episode 2620, Total Reward: -383.94\n",
            "[DDPG] Episode 2640, Total Reward: -96.80\n",
            "[DDPG] Episode 2660, Total Reward: -390.54\n",
            "[DDPG] Episode 2680, Total Reward: -137.89\n",
            "[DDPG] Episode 2700, Total Reward: -420.25\n",
            "[DDPG] Episode 2720, Total Reward: -142.06\n",
            "[DDPG] Episode 2740, Total Reward: -616.61\n",
            "[DDPG] Episode 2760, Total Reward: -386.19\n",
            "[DDPG] Episode 2780, Total Reward: -6822.24\n",
            "[DDPG] Episode 2800, Total Reward: -5025.50\n",
            "[DDPG] Episode 2820, Total Reward: -8553.12\n",
            "[DDPG] Episode 2840, Total Reward: -21707.73\n",
            "[DDPG] Episode 2860, Total Reward: -27120.42\n",
            "[DDPG] Episode 2880, Total Reward: -11173.45\n",
            "[DDPG] Episode 2900, Total Reward: -53964.64\n",
            "[DDPG] Episode 2920, Total Reward: -24239.83\n",
            "[DDPG] Episode 2940, Total Reward: -48882.09\n",
            "[DDPG] Episode 2960, Total Reward: -119962.01\n",
            "[DDPG] Episode 2980, Total Reward: -135828.95\n",
            "[DDPG] Episode 3000, Total Reward: -96993.37\n",
            "[DDPG] Episode 3020, Total Reward: -420944.23\n",
            "[DDPG] Episode 3040, Total Reward: -14543.06\n",
            "[DDPG] Episode 3060, Total Reward: -45419.92\n",
            "[DDPG] Episode 3080, Total Reward: -52243.85\n",
            "[DDPG] Episode 3100, Total Reward: -35460.19\n",
            "[DDPG] Episode 3120, Total Reward: -63594.46\n",
            "[DDPG] Episode 3140, Total Reward: -6009.78\n",
            "[DDPG] Episode 3160, Total Reward: -16965.90\n",
            "[DDPG] Episode 3180, Total Reward: -8905.08\n",
            "[DDPG] Episode 3200, Total Reward: -4720.93\n",
            "[DDPG] Episode 3220, Total Reward: -25018.65\n",
            "[DDPG] Episode 3240, Total Reward: -35318.20\n",
            "[DDPG] Episode 3260, Total Reward: -6264.64\n",
            "[DDPG] Episode 3280, Total Reward: -20356.89\n",
            "[DDPG] Episode 3300, Total Reward: -19930.34\n",
            "[DDPG] Episode 3320, Total Reward: -1280.99\n",
            "[DDPG] Episode 3340, Total Reward: -9636.47\n",
            "[DDPG] Episode 3360, Total Reward: -7284.52\n",
            "[DDPG] Episode 3380, Total Reward: -1894.36\n",
            "[DDPG] Episode 3400, Total Reward: -2935.07\n",
            "[DDPG] Episode 3420, Total Reward: -710.45\n",
            "[DDPG] Episode 3440, Total Reward: -17283.37\n",
            "[DDPG] Episode 3460, Total Reward: -2907.65\n",
            "[DDPG] Episode 3480, Total Reward: -3722.30\n",
            "[DDPG] Episode 3500, Total Reward: -5510.79\n",
            "[DDPG] Episode 3520, Total Reward: -12151.72\n",
            "[DDPG] Episode 3540, Total Reward: -3102.40\n",
            "[DDPG] Episode 3560, Total Reward: -5051.15\n",
            "[DDPG] Episode 3580, Total Reward: -2815.67\n",
            "[DDPG] Episode 3600, Total Reward: -27373.35\n",
            "[DDPG] Episode 3620, Total Reward: -97830.07\n",
            "[DDPG] Episode 3640, Total Reward: -18465.65\n",
            "[DDPG] Episode 3660, Total Reward: -35171.25\n",
            "[DDPG] Episode 3680, Total Reward: -46854.58\n",
            "[DDPG] Episode 3700, Total Reward: -10374.59\n",
            "[DDPG] Episode 3720, Total Reward: -24429.24\n",
            "[DDPG] Episode 3740, Total Reward: -28953.46\n",
            "[DDPG] Episode 3760, Total Reward: -52182.81\n",
            "[DDPG] Episode 3780, Total Reward: -83490.66\n",
            "[DDPG] Episode 3800, Total Reward: -11285.71\n",
            "[DDPG] Episode 3820, Total Reward: -59866.15\n",
            "[DDPG] Episode 3840, Total Reward: -84812.04\n",
            "[DDPG] Episode 3860, Total Reward: -77596.71\n",
            "[DDPG] Episode 3880, Total Reward: -81423.56\n",
            "[DDPG] Episode 3900, Total Reward: -114381.43\n",
            "[DDPG] Episode 3920, Total Reward: -99221.50\n",
            "[DDPG] Episode 3940, Total Reward: -120174.63\n",
            "[DDPG] Episode 3960, Total Reward: -103633.63\n",
            "[DDPG] Episode 3980, Total Reward: -93701.46\n",
            "[DDPG] Episode 4000, Total Reward: -88764.14\n",
            "[DDPG] Episode 4020, Total Reward: -146746.69\n",
            "[DDPG] Episode 4040, Total Reward: -187620.15\n",
            "[DDPG] Episode 4060, Total Reward: -528171.04\n",
            "[DDPG] Episode 4080, Total Reward: -184498.23\n",
            "[DDPG] Episode 4100, Total Reward: -163043.92\n",
            "[DDPG] Episode 4120, Total Reward: -265133.34\n",
            "[DDPG] Episode 4140, Total Reward: -157428.30\n",
            "[DDPG] Episode 4160, Total Reward: -40675.26\n",
            "[DDPG] Episode 4180, Total Reward: -23078.69\n",
            "[DDPG] Episode 4200, Total Reward: -21571.72\n",
            "[DDPG] Episode 4220, Total Reward: -13310.04\n",
            "[DDPG] Episode 4240, Total Reward: -11922.54\n",
            "[DDPG] Episode 4260, Total Reward: -8349.24\n",
            "[DDPG] Episode 4280, Total Reward: -7048.63\n",
            "[DDPG] Episode 4300, Total Reward: -7923.36\n",
            "[DDPG] Episode 4320, Total Reward: -10063.03\n",
            "[DDPG] Episode 4340, Total Reward: -10837.22\n",
            "[DDPG] Episode 4360, Total Reward: -14242.87\n",
            "[DDPG] Episode 4380, Total Reward: -13725.89\n",
            "[DDPG] Episode 4400, Total Reward: -11526.83\n",
            "[DDPG] Episode 4420, Total Reward: -17742.27\n",
            "[DDPG] Episode 4440, Total Reward: -24431.70\n",
            "[DDPG] Episode 4460, Total Reward: -20928.38\n",
            "[DDPG] Episode 4480, Total Reward: -48941.77\n",
            "[DDPG] Episode 4500, Total Reward: -64251.41\n",
            "[DDPG] Episode 4520, Total Reward: -31657.33\n",
            "[DDPG] Episode 4540, Total Reward: -30081.90\n",
            "[DDPG] Episode 4560, Total Reward: -38298.09\n",
            "[DDPG] Episode 4580, Total Reward: -20704.25\n",
            "[DDPG] Episode 4600, Total Reward: -114973.30\n",
            "[DDPG] Episode 4620, Total Reward: -25851.42\n",
            "[DDPG] Episode 4640, Total Reward: -30517.39\n",
            "[DDPG] Episode 4660, Total Reward: -24009.74\n",
            "[DDPG] Episode 4680, Total Reward: -14301.17\n",
            "[DDPG] Episode 4700, Total Reward: -24957.64\n",
            "[DDPG] Episode 4720, Total Reward: -18732.79\n",
            "[DDPG] Episode 4740, Total Reward: -28292.16\n",
            "[DDPG] Episode 4760, Total Reward: -15027.44\n",
            "[DDPG] Episode 4780, Total Reward: -19221.07\n",
            "[DDPG] Episode 4800, Total Reward: -18578.00\n",
            "[DDPG] Episode 4820, Total Reward: -6196.22\n",
            "[DDPG] Episode 4840, Total Reward: -9448.32\n",
            "[DDPG] Episode 4860, Total Reward: -9546.60\n",
            "[DDPG] Episode 4880, Total Reward: -10527.26\n",
            "[DDPG] Episode 4900, Total Reward: -19069.29\n",
            "[DDPG] Episode 4920, Total Reward: -4942.94\n",
            "[DDPG] Episode 4940, Total Reward: -13139.43\n",
            "[DDPG] Episode 4960, Total Reward: -12800.77\n",
            "[DDPG] Episode 4980, Total Reward: -95.90\n",
            "[DDPG] Episode 5000, Total Reward: -115.38\n",
            "[DDPG] Episode 5020, Total Reward: -353.46\n",
            "[DDPG] Episode 5040, Total Reward: -548.17\n",
            "[DDPG] Episode 5060, Total Reward: -704.95\n",
            "[DDPG] Episode 5080, Total Reward: -642.66\n",
            "[DDPG] Episode 5100, Total Reward: -2569.81\n",
            "[DDPG] Episode 5120, Total Reward: -13787.21\n",
            "[DDPG] Episode 5140, Total Reward: -18246.17\n",
            "[DDPG] Episode 5160, Total Reward: -13651.75\n",
            "[DDPG] Episode 5180, Total Reward: -9426.23\n",
            "[DDPG] Episode 5200, Total Reward: -8332.86\n",
            "[DDPG] Episode 5220, Total Reward: -8441.81\n",
            "[DDPG] Episode 5240, Total Reward: -10094.16\n",
            "[DDPG] Episode 5260, Total Reward: -8033.77\n",
            "[DDPG] Episode 5280, Total Reward: -8279.10\n",
            "[DDPG] Episode 5300, Total Reward: -13662.25\n",
            "[DDPG] Episode 5320, Total Reward: -7928.83\n",
            "[DDPG] Episode 5340, Total Reward: -11550.14\n",
            "[DDPG] Episode 5360, Total Reward: -7726.29\n",
            "[DDPG] Episode 5380, Total Reward: -10268.68\n",
            "[DDPG] Episode 5400, Total Reward: -12619.52\n",
            "[DDPG] Episode 5420, Total Reward: -7259.67\n",
            "[DDPG] Episode 5440, Total Reward: -11438.05\n",
            "[DDPG] Episode 5460, Total Reward: -6737.46\n",
            "[DDPG] Episode 5480, Total Reward: -8592.95\n",
            "[DDPG] Episode 5500, Total Reward: -8462.39\n",
            "[DDPG] Episode 5520, Total Reward: -9324.19\n",
            "[DDPG] Episode 5540, Total Reward: -8491.41\n",
            "[DDPG] Episode 5560, Total Reward: -13165.58\n",
            "[DDPG] Episode 5580, Total Reward: -9216.49\n",
            "[DDPG] Episode 5600, Total Reward: -14463.96\n",
            "[DDPG] Episode 5620, Total Reward: -13465.13\n",
            "[DDPG] Episode 5640, Total Reward: -13236.14\n",
            "[DDPG] Episode 5660, Total Reward: -12038.35\n",
            "[DDPG] Episode 5680, Total Reward: -11134.01\n",
            "[DDPG] Episode 5700, Total Reward: -9765.18\n",
            "[DDPG] Episode 5720, Total Reward: -9532.17\n",
            "[DDPG] Episode 5740, Total Reward: -8939.98\n",
            "[DDPG] Episode 5760, Total Reward: -7115.82\n",
            "[DDPG] Episode 5780, Total Reward: -11740.10\n",
            "[DDPG] Episode 5800, Total Reward: -8006.27\n",
            "[DDPG] Episode 5820, Total Reward: -13429.23\n",
            "[DDPG] Episode 5840, Total Reward: -8010.20\n",
            "[DDPG] Episode 5860, Total Reward: -6896.71\n",
            "[DDPG] Episode 5880, Total Reward: -5636.60\n",
            "[DDPG] Episode 5900, Total Reward: -31248.05\n",
            "[DDPG] Episode 5920, Total Reward: -3069.17\n",
            "[DDPG] Episode 5940, Total Reward: -3680.56\n",
            "[DDPG] Episode 5960, Total Reward: -3128.99\n",
            "[DDPG] Episode 5980, Total Reward: -2249.35\n",
            "[DDPG] Episode 6000, Total Reward: -3285.12\n",
            "[DDPG] Episode 6020, Total Reward: -3919.90\n",
            "[DDPG] Episode 6040, Total Reward: -3940.74\n",
            "[DDPG] Episode 6060, Total Reward: -4361.41\n",
            "[DDPG] Episode 6080, Total Reward: -3298.39\n",
            "[DDPG] Episode 6100, Total Reward: -2479.69\n",
            "[DDPG] Episode 6120, Total Reward: -3661.47\n",
            "[DDPG] Episode 6140, Total Reward: -2389.38\n",
            "[DDPG] Episode 6160, Total Reward: -832.05\n",
            "[DDPG] Episode 6180, Total Reward: -1026.70\n",
            "[DDPG] Episode 6200, Total Reward: -66.31\n",
            "[DDPG] Episode 6220, Total Reward: -221853370156713.78\n",
            "[DDPG] Episode 6240, Total Reward: -1783.60\n",
            "[DDPG] Episode 6260, Total Reward: nan\n",
            "[DDPG] Episode 6280, Total Reward: nan\n",
            "[DDPG] Episode 6300, Total Reward: nan\n",
            "[DDPG] Episode 6320, Total Reward: nan\n",
            "[DDPG] Episode 6340, Total Reward: nan\n",
            "[DDPG] Episode 6360, Total Reward: nan\n",
            "[DDPG] Episode 6380, Total Reward: nan\n",
            "[DDPG] Episode 6400, Total Reward: nan\n",
            "[DDPG] Episode 6420, Total Reward: nan\n",
            "[DDPG] Episode 6440, Total Reward: nan\n",
            "[DDPG] Episode 6460, Total Reward: nan\n",
            "[DDPG] Episode 6480, Total Reward: nan\n",
            "[DDPG] Episode 6500, Total Reward: nan\n",
            "[DDPG] Episode 6520, Total Reward: nan\n",
            "[DDPG] Episode 6540, Total Reward: nan\n",
            "[DDPG] Episode 6560, Total Reward: nan\n",
            "[DDPG] Episode 6580, Total Reward: nan\n",
            "[DDPG] Episode 6600, Total Reward: nan\n",
            "[DDPG] Episode 6620, Total Reward: nan\n",
            "[DDPG] Episode 6640, Total Reward: nan\n",
            "[DDPG] Episode 6660, Total Reward: nan\n",
            "[DDPG] Episode 6680, Total Reward: nan\n",
            "[DDPG] Episode 6700, Total Reward: nan\n",
            "[DDPG] Episode 6720, Total Reward: nan\n",
            "[DDPG] Episode 6740, Total Reward: nan\n",
            "[DDPG] Episode 6760, Total Reward: nan\n",
            "[DDPG] Episode 6780, Total Reward: nan\n",
            "[DDPG] Episode 6800, Total Reward: nan\n",
            "[DDPG] Episode 6820, Total Reward: nan\n",
            "[DDPG] Episode 6840, Total Reward: nan\n",
            "[DDPG] Episode 6860, Total Reward: nan\n",
            "[DDPG] Episode 6880, Total Reward: nan\n",
            "[DDPG] Episode 6900, Total Reward: nan\n",
            "[DDPG] Episode 6920, Total Reward: nan\n",
            "[DDPG] Episode 6940, Total Reward: nan\n",
            "[DDPG] Episode 6960, Total Reward: nan\n",
            "[DDPG] Episode 6980, Total Reward: nan\n",
            "[DDPG] Episode 7000, Total Reward: nan\n",
            "[DDPG] Episode 7020, Total Reward: nan\n",
            "[DDPG] Episode 7040, Total Reward: nan\n",
            "[DDPG] Episode 7060, Total Reward: nan\n",
            "[DDPG] Episode 7080, Total Reward: nan\n",
            "[DDPG] Episode 7100, Total Reward: nan\n",
            "[DDPG] Episode 7120, Total Reward: nan\n",
            "[DDPG] Episode 7140, Total Reward: nan\n",
            "[DDPG] Episode 7160, Total Reward: nan\n",
            "[DDPG] Episode 7180, Total Reward: nan\n",
            "[DDPG] Episode 7200, Total Reward: nan\n",
            "[DDPG] Episode 7220, Total Reward: nan\n",
            "[DDPG] Episode 7240, Total Reward: nan\n",
            "[DDPG] Episode 7260, Total Reward: nan\n",
            "[DDPG] Episode 7280, Total Reward: nan\n",
            "[DDPG] Episode 7300, Total Reward: nan\n",
            "[DDPG] Episode 7320, Total Reward: nan\n",
            "[DDPG] Episode 7340, Total Reward: nan\n",
            "[DDPG] Episode 7360, Total Reward: nan\n",
            "[DDPG] Episode 7380, Total Reward: nan\n",
            "[DDPG] Episode 7400, Total Reward: nan\n",
            "[DDPG] Episode 7420, Total Reward: nan\n",
            "[DDPG] Episode 7440, Total Reward: nan\n",
            "[DDPG] Episode 7460, Total Reward: nan\n",
            "[DDPG] Episode 7480, Total Reward: nan\n",
            "[DDPG] Episode 7500, Total Reward: nan\n",
            "[DDPG] Episode 7520, Total Reward: nan\n",
            "[DDPG] Episode 7540, Total Reward: nan\n",
            "[DDPG] Episode 7560, Total Reward: nan\n",
            "[DDPG] Episode 7580, Total Reward: nan\n",
            "[DDPG] Episode 7600, Total Reward: nan\n",
            "[DDPG] Episode 7620, Total Reward: nan\n",
            "[DDPG] Episode 7640, Total Reward: nan\n",
            "[DDPG] Episode 7660, Total Reward: nan\n",
            "[DDPG] Episode 7680, Total Reward: nan\n",
            "[DDPG] Episode 7700, Total Reward: nan\n",
            "[DDPG] Episode 7720, Total Reward: nan\n",
            "[DDPG] Episode 7740, Total Reward: nan\n",
            "[DDPG] Episode 7760, Total Reward: nan\n",
            "[DDPG] Episode 7780, Total Reward: nan\n",
            "[DDPG] Episode 7800, Total Reward: nan\n",
            "[DDPG] Episode 7820, Total Reward: nan\n",
            "[DDPG] Episode 7840, Total Reward: nan\n",
            "[DDPG] Episode 7860, Total Reward: nan\n",
            "[DDPG] Episode 7880, Total Reward: nan\n",
            "[DDPG] Episode 7900, Total Reward: nan\n",
            "[DDPG] Episode 7920, Total Reward: nan\n",
            "[DDPG] Episode 7940, Total Reward: nan\n",
            "[DDPG] Episode 7960, Total Reward: nan\n",
            "[DDPG] Episode 7980, Total Reward: nan\n",
            "[DDPG] Episode 8000, Total Reward: nan\n",
            "[DDPG] Episode 8020, Total Reward: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n======== Running DDPG (Constant Volatility) ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0menv_const\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHedgingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstochastic_vol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0mrewards_ddpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_const\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n======== Running PPO (Constant Volatility) ========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, n_episodes, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Update if enough samples are available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0mstates_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mactions_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b66a46976fe0>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}