\documentclass{article}
\input{boilerplate}
\bibliographystyle{plain}  % or use: abbrv, unsrt, apalike, etc.
\begin{document}


\newcommand{\lectureTitle}{Project Proposal: Optimal Option Hedging and Pricing}
\newcommand{\lectureDate}{Due: March 24, 2024}

\textsc{COS435 / ECE433: Introduction to RL} \hfill \lectureDate
\vspace{1em}

\maketitle

\paragraph{Authors.} Arif Ansari (\texttt{aa4433@princeton.edu}), Christos Avgerinos Tegopoulos (\texttt{ct3125@princeton.edu}), Jeremy Jun-Ping Bird (\texttt{jb9895@princeton.edu}), 

\paragraph{Project type:}
2. 
Applying RL to a new problem

\paragraph{Introduction:}
We aim to apply RL to the well-established financial problem of option pricing. In perfect, frictionless markets, options are priced through a replicating portfolio of a risky asset and a risk-free bond. This renders the problems of optimal hedging and pricing synonymous.\\\\
The Black-Scholes-Merton's (BSM) model analytically solves the problem of optimal option hedging and pricing for European Options. Yet options trading remains a multi-billion dollar business where traders employ discretion is pricing and risk management. Clealy, this highlights the BSM model's shortcomings.
\paragraph{Pitfalls of BSM:}
\begin{itemize}
    \item \textbf{Non-Continuous Rehedging:} The assumption of continuous rebalancing is costly and impractical. Thus, the replicating portfolio will carry a certain degree of risk.
    \item \textbf{Transaction Costs:} Traders face transaction costs (e.g. spread, market impact) each time they rebalance. However, these costs are entirely neglected within the classical BSM model.
    \item \textbf{Stochastic Volatility:} Real markets have time-varying volatility, with strong autocorrelation. The BSM model however assumes log-normal price dynamics.
\end{itemize}

\paragraph{RL Approach:} The optimal hedging problem requires the agent to balance the tradeoff between transaction costs and perfectly hedging, which is an even tougher problem to solve for analytically under stochastic volatility assumptions. This suggests applying RL techniques to learn the optimal policy. Formally we have: 
\begin{itemize}
    \item \textbf{Actions $a_t$:} The amount of the underlying hedged at each timestep $t$\cite{halperin}\cite{stoiljkovic}
    \item \textbf{States $S_t$:} The price of the underlying (or some time transformed variable $X_t$ which normalizes for drift in the underlying dynamics)\cite{halperin}\cite{stoiljkovic}
    \item \textbf{Dynamics:} The underlying follows geometric Brownian motion with stochastic volatility (SABR model)\cite{cao}
    \item \textbf{Reward Function $R_t$:}
    Here our reward at each period $0<t<T$ is equal to our change in wealth $\Delta w_t = S_t(a_{t-1}-a_t)-\kappa|S_t(a_{t-1}-a_t)|$ minus the variance of $\Delta w_t$ scaled by some risk-aversion parameter $\lambda$\cite{kolm}
    \[R_t = \Delta w_t - \lambda\mathbb{V}[\Delta w_t] = \Delta w_t - \lambda(\Delta w_t-\mathbb{E}[\Delta w_t])^2\]
    Note that at $t=0$ the initial change in wealth $\Delta w_0=-S_0a_0-\kappa |S_0a_0|$ as we must buy the initial replicating portfolio and at matrity we have  $\Delta w_T=S_Ta_T-\kappa |S_Ta_T|-G(S_T)$ where $G(S_T)$ is the option payoff.
\end{itemize}
This gives final Bellman Equation \[V^{\pi}(S_t) = \mathbb{E}^{\pi}_t[R_t(S_t,a_t,S_{t+1})+\gamma V^{\pi}(S_{t+1})]\]

\paragraph{Why Not Bandit?:} Due to our transaction costs, how much we choose to hedge at period $t$ influences the transaction costs we pay when rebalancing at $t+1$. For example, if we accumulate a large position of stock in a single period, at some point in the future, we will have to unwind this position (given our terminality condition), resulting in larger transaction costs. Hence, treating this as a RL problem encourages small, incremental hedges and no wild swings in position size.

\paragraph{What We Hope to Achieve:} Our aim is for our RL agent to achieve the following:
\begin{itemize}
    \item \textbf{BSM Outperformance:} As established above, we expect delta hedging according to the BSM model to be suboptimal in a more realistic market setting. Subsequently, we hope for our RL-learned hedging strategy to outperform BSM hedging (i.e. achieve superior risk adjusted PnL).
    \item \textbf{Model Independence:} Our RL approach does not require an explicit knowledge of market dynamics, the "greeks" or a known model for transaction costs as it is model-free and aims to learn optimal hedging based solely on observed rewards and state transitions. This is a more appropriate reflection of reality where it is impossible to perfectly model these factors.
\end{itemize}

\paragraph{Final Methodology:} 
\begin{enumerate}
    \item Simulate N monte-carlo paths for the underlying according to our predefined model dynamics
    \item Using the RL problem setup described above, apply:
    \begin{itemize}
        \item \textbf{Tabular Q-Learning} \textit{(State-spaced discretized using lattice approximation of stock prices. Action-space discretized into 0.01 increments of quantity held.)}
        \item \textbf{DDPG} \textit{From the Lillicrap, 2019 paper}\cite{lillicrap}
        \item \textbf{PPO} \textit{From the Schulman, 2017 paper} \cite{schulman}
        \item \textbf{GRPO:} \textit{ From DeepSeek 2024 paper} \cite{deepseek}
    \end{itemize}
    Compare each algorithm's performance, data efficiency and rate of convergence.
    \item Repeat the above under stochastic volatility dynamics. Investigate the potency of the RL policy in the new environment.
\end{enumerate}
We're specifically interested in looking into \textbf{GRPO} as it doesn't need to learn the critic function which can be difficult due to stochasticity of transaction costs. By using only observed Monte Carlo methods it may be better at dealing with the constraints of the problem. Although it may be potentially unstable at points with high variance we are hoping to see if running a numerous amount of trials will help normalize these results.
\bibliography{references}
\end{document}
