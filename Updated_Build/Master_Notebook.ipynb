{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44084610-388f-4f87-89b8-04645197ef06",
   "metadata": {},
   "source": [
    "# RL Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569d20f-10f4-46ce-81fe-87e404152870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dab1c-9722-4295-92ec-6854affce411",
   "metadata": {},
   "source": [
    "#### Hedging Environment\n",
    "see *hedge_env.py*\n",
    "- Changed from \"cash flow PnL\" to **\"Accounting PnL\"** (see Cao 2019 for more description)\n",
    "    - general idea is now our daily reward now includes daily mark-to-market prices for options\n",
    "    - For *stationary vol*: mark-to-market is black scholes\n",
    "    - For *non-stationary vol*: mark-to-market is Hagan SABR implied vol (see Hagan 2002 paper for more)\n",
    "- Adjusted stationary vol stock dynamics to match analytical Black-Scholes analytical Euler discretization\n",
    "    - Stationary Vol:\n",
    "        - Before: $S_{t+1} = S_t (1 + \\sigma \\cdot \\sqrt{dt} \\cdot Z1)$\n",
    "        - New: Euler discretization of GBM analytical solution\n",
    "    - Stochastic Vol:\n",
    "        - Changed to SABR model with log-normal returns (e.g. beta = 1)\n",
    "- Changed the 4th state variable from \"time fraction\" to tau\n",
    "- Change it from hedging 1 option to hedging 100 options --> the bigger reward and action magnitude helps the model learn better\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74511587-9d5a-4b1e-8d90-ae88c2a5c2a8",
   "metadata": {},
   "source": [
    "#### Actor and Critic Models\n",
    "see *models.py*\n",
    "- I only have DDPG models. In my mind neither PPO nor GRPO make sense in this context given we have to re paramterize to policy to output some probability distribution over our action space. Given the high amounts of precision required to properly hedge the option, this isn't stable enough to converge (from what I've experienced)\n",
    "- Added batch normalization per layer on all networks\n",
    "- Added sigmoid activation to action output of actor\n",
    "- I use 2 seperate Q-functions (the \"cost critic\" and \"risk critic\"), each modelling $E[ \\ C_t]$ and $E[C_t^2]$ respectively. This is from Cao 2019. Can read for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d57c38-37e5-4635-9b7c-ad1b2d76efc4",
   "metadata": {},
   "source": [
    "#### Hot Start Actor\n",
    "see *hot_start_actor.py*\n",
    "- Supervised learning to teach actor either BS hedge (stationary vol) or Bartlett delta hedge (stochastic vol)\n",
    "- The trained model works well but it's hard to train a critic function on it and then proceed with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199cb94-89ed-47f0-bfa2-342bcf17025f",
   "metadata": {},
   "source": [
    "#### Hot Start Critic\n",
    "see *hot_start_critic.py*\n",
    "- Takes a pre-trained actor and uses similar DDPG method (with cost and risk critic functions) to train the 2 critic functions. Seems to have trouble learning long range dependencies. Can try mess around with it if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a5f88-fdf8-4b31-ae4d-e8c34e8c48b3",
   "metadata": {},
   "source": [
    "#### DDPG Trainer\n",
    "see *DDPG_train*\n",
    "- I rehaul the code to match Cao 2019 approach. Very gradual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc749c-04ff-426f-afc3-395a3f9c543f",
   "metadata": {},
   "source": [
    "#### Policy Evaluation Function\n",
    "see *evaluate.py*\n",
    "- Simulates paths and gets average total objective function\n",
    "- Can do simulation with BS policy, Bartlett policy, Do Nothing policy, and inputted policy\n",
    "- Will use this to compare actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6fa32-deb8-4c10-b5bd-cafca6e7a98b",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Will train 3 different actors and compare performances with Black-Scholes and Do Nothing Benchmarks: Note the costs are more than 100 times larger than our previous environment (i.e. a reward of -600 can be thought of as akin to a reward of -6 in our previous environments)\n",
    "1. Analytical Models\n",
    "2. Cold-start model\n",
    "3. Hot-started model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f296a2d3-299d-4764-9d04-90b1cbd38ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hedge_env import HedgingEnv\n",
    "from hot_start_actor import hot_start_gen_actor_samples, hot_start_actor\n",
    "from hot_start_critic import hot_start_critic_q_func, hot_start_critic_value_func\n",
    "from DDPG_train import DDPG_train\n",
    "from evaluate import evaluation\n",
    "from models import Actor, DDPG_Cost_Critic, DDPG_Risk_Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6b11b-e6d6-4eab-8c47-235e7cdf67bc",
   "metadata": {},
   "source": [
    "#### Analytical Models:\n",
    "Our analytical models are already coded into the evaluate function. Simply just need to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ea1cf9d-5da2-4872-bfba-138a41299c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = HedgingEnv(T = 1/12, sigma0 = 0.2, kappa = 0.01, risk_aversion = 1.5, stochastic_vol=True, n_steps = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b924ec-0137-4959-bb86-66dd29357f37",
   "metadata": {},
   "source": [
    "#### Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e8f7d8b-da8f-4048-8ed2-4889d6f34585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Do Nothing' policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -584.040 | PnL: -42.001, Risk: 542.040\n",
      "Episode: 200 | Mean Objective: -520.432 | PnL: -16.731, Risk: 503.701\n",
      "Episode: 300 | Mean Objective: -527.977 | PnL: -15.074, Risk: 512.902\n",
      "Episode: 400 | Mean Objective: -507.628 | PnL: 3.390, Risk: 511.018\n",
      "Episode: 500 | Mean Objective: -499.918 | PnL: 6.926, Risk: 506.843\n",
      "Episode: 600 | Mean Objective: -483.776 | PnL: 12.396, Risk: 496.172\n",
      "Episode: 700 | Mean Objective: -494.854 | PnL: 10.304, Risk: 505.158\n",
      "Episode: 800 | Mean Objective: -498.495 | PnL: 7.507, Risk: 506.002\n",
      "Episode: 900 | Mean Objective: -505.522 | PnL: 5.785, Risk: 511.307\n",
      "Episode: 1000 | Mean Objective: -507.858 | PnL: 4.158, Risk: 512.016\n",
      "Episode: 1100 | Mean Objective: -518.831 | PnL: -1.476, Risk: 517.355\n",
      "Episode: 1200 | Mean Objective: -516.209 | PnL: -1.167, Risk: 515.043\n",
      "Episode: 1300 | Mean Objective: -512.268 | PnL: -0.697, Risk: 511.570\n",
      "Episode: 1400 | Mean Objective: -517.110 | PnL: -1.506, Risk: 515.604\n",
      "Episode: 1500 | Mean Objective: -511.392 | PnL: 1.380, Risk: 512.772\n",
      "Episode: 1600 | Mean Objective: -514.506 | PnL: 1.446, Risk: 515.952\n",
      "Episode: 1700 | Mean Objective: -521.374 | PnL: -1.810, Risk: 519.564\n",
      "Episode: 1800 | Mean Objective: -518.606 | PnL: 0.016, Risk: 518.622\n",
      "Episode: 1900 | Mean Objective: -515.316 | PnL: -0.054, Risk: 515.262\n",
      "Episode: 2000 | Mean Objective: -518.449 | PnL: -1.809, Risk: 516.640\n",
      "Episode: 2100 | Mean Objective: -528.227 | PnL: -6.136, Risk: 522.092\n",
      "Episode: 2200 | Mean Objective: -523.819 | PnL: -5.043, Risk: 518.776\n",
      "Episode: 2300 | Mean Objective: -523.656 | PnL: -4.890, Risk: 518.766\n",
      "Episode: 2400 | Mean Objective: -518.526 | PnL: -3.254, Risk: 515.271\n",
      "Episode: 2500 | Mean Objective: -518.488 | PnL: -3.203, Risk: 515.284\n",
      "Episode: 2600 | Mean Objective: -516.933 | PnL: -2.139, Risk: 514.794\n",
      "Episode: 2700 | Mean Objective: -518.579 | PnL: -2.856, Risk: 515.723\n",
      "Episode: 2800 | Mean Objective: -518.216 | PnL: -3.298, Risk: 514.918\n",
      "Episode: 2900 | Mean Objective: -519.081 | PnL: -4.488, Risk: 514.593\n",
      "Episode: 3000 | Mean Objective: -519.368 | PnL: -3.969, Risk: 515.399\n",
      "Episode: 3100 | Mean Objective: -523.007 | PnL: -4.445, Risk: 518.562\n",
      "Episode: 3200 | Mean Objective: -520.440 | PnL: -3.298, Risk: 517.142\n",
      "Episode: 3300 | Mean Objective: -519.604 | PnL: -3.224, Risk: 516.380\n",
      "Episode: 3400 | Mean Objective: -521.064 | PnL: -3.584, Risk: 517.481\n",
      "Episode: 3500 | Mean Objective: -521.801 | PnL: -3.884, Risk: 517.917\n",
      "Episode: 3600 | Mean Objective: -519.954 | PnL: -3.642, Risk: 516.312\n",
      "Episode: 3700 | Mean Objective: -523.122 | PnL: -4.242, Risk: 518.880\n",
      "Episode: 3800 | Mean Objective: -523.419 | PnL: -3.980, Risk: 519.439\n",
      "Episode: 3900 | Mean Objective: -522.592 | PnL: -3.342, Risk: 519.250\n",
      "Episode: 4000 | Mean Objective: -522.628 | PnL: -3.166, Risk: 519.463\n",
      "Episode: 4100 | Mean Objective: -523.253 | PnL: -3.286, Risk: 519.967\n",
      "Episode: 4200 | Mean Objective: -520.171 | PnL: -2.321, Risk: 517.851\n",
      "Episode: 4300 | Mean Objective: -518.475 | PnL: -1.969, Risk: 516.506\n",
      "Episode: 4400 | Mean Objective: -517.237 | PnL: -1.307, Risk: 515.930\n",
      "Episode: 4500 | Mean Objective: -515.906 | PnL: -0.564, Risk: 515.342\n",
      "Episode: 4600 | Mean Objective: -514.720 | PnL: -0.073, Risk: 514.647\n",
      "Episode: 4700 | Mean Objective: -518.942 | PnL: -1.693, Risk: 517.249\n",
      "Episode: 4800 | Mean Objective: -518.746 | PnL: -1.600, Risk: 517.146\n",
      "Episode: 4900 | Mean Objective: -516.022 | PnL: -0.546, Risk: 515.476\n",
      "Episode: 5000 | Mean Objective: -516.153 | PnL: -0.398, Risk: 515.755\n",
      "Episode: 5100 | Mean Objective: -515.996 | PnL: -0.338, Risk: 515.658\n",
      "Episode: 5200 | Mean Objective: -517.602 | PnL: -0.766, Risk: 516.836\n",
      "Episode: 5300 | Mean Objective: -518.647 | PnL: -1.311, Risk: 517.336\n",
      "Episode: 5400 | Mean Objective: -520.663 | PnL: -1.855, Risk: 518.808\n",
      "Episode: 5500 | Mean Objective: -518.445 | PnL: -1.256, Risk: 517.190\n",
      "Episode: 5600 | Mean Objective: -515.676 | PnL: 0.115, Risk: 515.791\n",
      "Episode: 5700 | Mean Objective: -515.276 | PnL: 0.453, Risk: 515.728\n",
      "Episode: 5800 | Mean Objective: -514.759 | PnL: 0.773, Risk: 515.531\n",
      "Episode: 5900 | Mean Objective: -513.700 | PnL: 0.894, Risk: 514.594\n",
      "Episode: 6000 | Mean Objective: -513.481 | PnL: 1.108, Risk: 514.588\n",
      "Episode: 6100 | Mean Objective: -513.009 | PnL: 1.164, Risk: 514.173\n",
      "Episode: 6200 | Mean Objective: -512.348 | PnL: 1.620, Risk: 513.968\n",
      "Episode: 6300 | Mean Objective: -513.966 | PnL: 1.725, Risk: 515.691\n",
      "Episode: 6400 | Mean Objective: -512.557 | PnL: 2.578, Risk: 515.136\n",
      "Episode: 6500 | Mean Objective: -510.513 | PnL: 3.299, Risk: 513.813\n",
      "Episode: 6600 | Mean Objective: -509.756 | PnL: 3.837, Risk: 513.593\n",
      "Episode: 6700 | Mean Objective: -510.465 | PnL: 3.746, Risk: 514.211\n",
      "Episode: 6800 | Mean Objective: -510.993 | PnL: 3.259, Risk: 514.252\n",
      "Episode: 6900 | Mean Objective: -508.950 | PnL: 4.246, Risk: 513.196\n",
      "Episode: 7000 | Mean Objective: -510.969 | PnL: 3.537, Risk: 514.506\n",
      "Episode: 7100 | Mean Objective: -509.640 | PnL: 4.460, Risk: 514.100\n",
      "Episode: 7200 | Mean Objective: -509.356 | PnL: 4.406, Risk: 513.762\n",
      "Episode: 7300 | Mean Objective: -508.715 | PnL: 4.999, Risk: 513.714\n",
      "Episode: 7400 | Mean Objective: -508.890 | PnL: 5.018, Risk: 513.908\n",
      "Episode: 7500 | Mean Objective: -508.747 | PnL: 5.307, Risk: 514.054\n",
      "Episode: 7600 | Mean Objective: -508.871 | PnL: 5.287, Risk: 514.158\n",
      "Episode: 7700 | Mean Objective: -507.512 | PnL: 5.745, Risk: 513.257\n",
      "Episode: 7800 | Mean Objective: -509.023 | PnL: 5.162, Risk: 514.184\n",
      "Episode: 7900 | Mean Objective: -507.917 | PnL: 5.792, Risk: 513.709\n",
      "Episode: 8000 | Mean Objective: -508.012 | PnL: 5.425, Risk: 513.438\n",
      "Episode: 8100 | Mean Objective: -507.794 | PnL: 5.474, Risk: 513.268\n",
      "Episode: 8200 | Mean Objective: -506.944 | PnL: 5.836, Risk: 512.780\n",
      "Episode: 8300 | Mean Objective: -505.911 | PnL: 6.185, Risk: 512.096\n",
      "Episode: 8400 | Mean Objective: -506.479 | PnL: 5.776, Risk: 512.255\n",
      "Episode: 8500 | Mean Objective: -507.767 | PnL: 5.132, Risk: 512.899\n",
      "Episode: 8600 | Mean Objective: -507.373 | PnL: 5.253, Risk: 512.626\n",
      "Episode: 8700 | Mean Objective: -506.803 | PnL: 5.356, Risk: 512.158\n",
      "Episode: 8800 | Mean Objective: -507.415 | PnL: 5.123, Risk: 512.539\n",
      "Episode: 8900 | Mean Objective: -507.658 | PnL: 5.248, Risk: 512.906\n",
      "Episode: 9000 | Mean Objective: -506.870 | PnL: 5.601, Risk: 512.471\n",
      "Episode: 9100 | Mean Objective: -506.550 | PnL: 5.812, Risk: 512.362\n",
      "Episode: 9200 | Mean Objective: -505.737 | PnL: 6.051, Risk: 511.788\n",
      "Episode: 9300 | Mean Objective: -507.804 | PnL: 5.124, Risk: 512.928\n",
      "Episode: 9400 | Mean Objective: -508.660 | PnL: 4.675, Risk: 513.335\n",
      "Episode: 9500 | Mean Objective: -509.096 | PnL: 4.537, Risk: 513.632\n",
      "Episode: 9600 | Mean Objective: -508.720 | PnL: 4.560, Risk: 513.280\n",
      "Episode: 9700 | Mean Objective: -507.789 | PnL: 4.848, Risk: 512.638\n",
      "Episode: 9800 | Mean Objective: -507.613 | PnL: 4.904, Risk: 512.517\n",
      "Episode: 9900 | Mean Objective: -507.883 | PnL: 4.762, Risk: 512.644\n",
      "Episode: 10000 | Mean Objective: -506.161 | PnL: 5.524, Risk: 511.685\n",
      "\n",
      "BS policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -317.062 | PnL: -172.835, Risk: 144.227\n",
      "Episode: 200 | Mean Objective: -343.643 | PnL: -188.194, Risk: 155.450\n",
      "Episode: 300 | Mean Objective: -333.787 | PnL: -185.053, Risk: 148.735\n",
      "Episode: 400 | Mean Objective: -341.981 | PnL: -191.989, Risk: 149.992\n",
      "Episode: 500 | Mean Objective: -343.778 | PnL: -193.650, Risk: 150.128\n",
      "Episode: 600 | Mean Objective: -341.368 | PnL: -193.470, Risk: 147.898\n",
      "Episode: 700 | Mean Objective: -341.145 | PnL: -194.294, Risk: 146.851\n",
      "Episode: 800 | Mean Objective: -344.110 | PnL: -196.457, Risk: 147.654\n",
      "Episode: 900 | Mean Objective: -346.274 | PnL: -196.300, Risk: 149.975\n",
      "Episode: 1000 | Mean Objective: -346.020 | PnL: -194.331, Risk: 151.689\n",
      "Episode: 1100 | Mean Objective: -346.237 | PnL: -193.714, Risk: 152.523\n",
      "Episode: 1200 | Mean Objective: -349.394 | PnL: -194.983, Risk: 154.411\n",
      "Episode: 1300 | Mean Objective: -348.008 | PnL: -194.224, Risk: 153.784\n",
      "Episode: 1400 | Mean Objective: -348.277 | PnL: -194.070, Risk: 154.207\n",
      "Episode: 1500 | Mean Objective: -347.752 | PnL: -193.150, Risk: 154.602\n",
      "Episode: 1600 | Mean Objective: -346.757 | PnL: -192.449, Risk: 154.308\n",
      "Episode: 1700 | Mean Objective: -348.802 | PnL: -192.447, Risk: 156.354\n",
      "Episode: 1800 | Mean Objective: -348.293 | PnL: -192.213, Risk: 156.080\n",
      "Episode: 1900 | Mean Objective: -349.780 | PnL: -193.310, Risk: 156.470\n",
      "Episode: 2000 | Mean Objective: -348.530 | PnL: -192.908, Risk: 155.622\n",
      "Episode: 2100 | Mean Objective: -350.359 | PnL: -193.860, Risk: 156.499\n",
      "Episode: 2200 | Mean Objective: -349.340 | PnL: -193.028, Risk: 156.312\n",
      "Episode: 2300 | Mean Objective: -350.577 | PnL: -193.387, Risk: 157.190\n",
      "Episode: 2400 | Mean Objective: -350.536 | PnL: -193.015, Risk: 157.521\n",
      "Episode: 2500 | Mean Objective: -351.064 | PnL: -192.805, Risk: 158.259\n",
      "Episode: 2600 | Mean Objective: -349.646 | PnL: -192.216, Risk: 157.430\n",
      "Episode: 2700 | Mean Objective: -348.912 | PnL: -191.778, Risk: 157.134\n",
      "Episode: 2800 | Mean Objective: -348.049 | PnL: -191.345, Risk: 156.704\n",
      "Episode: 2900 | Mean Objective: -348.168 | PnL: -191.596, Risk: 156.572\n",
      "Episode: 3000 | Mean Objective: -347.523 | PnL: -191.353, Risk: 156.170\n",
      "Episode: 3100 | Mean Objective: -346.866 | PnL: -191.178, Risk: 155.688\n",
      "Episode: 3200 | Mean Objective: -347.283 | PnL: -191.531, Risk: 155.752\n",
      "Episode: 3300 | Mean Objective: -348.137 | PnL: -192.080, Risk: 156.057\n",
      "Episode: 3400 | Mean Objective: -348.243 | PnL: -192.604, Risk: 155.640\n",
      "Episode: 3500 | Mean Objective: -348.155 | PnL: -192.689, Risk: 155.465\n",
      "Episode: 3600 | Mean Objective: -348.799 | PnL: -193.037, Risk: 155.762\n",
      "Episode: 3700 | Mean Objective: -349.131 | PnL: -193.059, Risk: 156.072\n",
      "Episode: 3800 | Mean Objective: -348.903 | PnL: -193.129, Risk: 155.774\n",
      "Episode: 3900 | Mean Objective: -348.605 | PnL: -192.764, Risk: 155.840\n",
      "Episode: 4000 | Mean Objective: -348.534 | PnL: -192.658, Risk: 155.875\n",
      "Episode: 4100 | Mean Objective: -348.909 | PnL: -192.794, Risk: 156.115\n",
      "Episode: 4200 | Mean Objective: -349.830 | PnL: -192.967, Risk: 156.863\n",
      "Episode: 4300 | Mean Objective: -349.584 | PnL: -192.915, Risk: 156.669\n",
      "Episode: 4400 | Mean Objective: -349.280 | PnL: -192.531, Risk: 156.749\n",
      "Episode: 4500 | Mean Objective: -349.152 | PnL: -192.555, Risk: 156.597\n",
      "Episode: 4600 | Mean Objective: -348.956 | PnL: -192.588, Risk: 156.369\n",
      "Episode: 4700 | Mean Objective: -349.856 | PnL: -192.954, Risk: 156.902\n",
      "Episode: 4800 | Mean Objective: -349.879 | PnL: -193.095, Risk: 156.785\n",
      "Episode: 4900 | Mean Objective: -350.087 | PnL: -193.410, Risk: 156.677\n",
      "Episode: 5000 | Mean Objective: -350.091 | PnL: -193.123, Risk: 156.967\n",
      "Episode: 5100 | Mean Objective: -350.135 | PnL: -193.331, Risk: 156.804\n",
      "Episode: 5200 | Mean Objective: -350.058 | PnL: -193.467, Risk: 156.590\n",
      "Episode: 5300 | Mean Objective: -349.894 | PnL: -193.471, Risk: 156.423\n",
      "Episode: 5400 | Mean Objective: -349.831 | PnL: -193.448, Risk: 156.383\n",
      "Episode: 5500 | Mean Objective: -349.584 | PnL: -193.241, Risk: 156.343\n",
      "Episode: 5600 | Mean Objective: -349.335 | PnL: -193.036, Risk: 156.299\n",
      "Episode: 5700 | Mean Objective: -349.245 | PnL: -193.073, Risk: 156.172\n",
      "Episode: 5800 | Mean Objective: -349.533 | PnL: -193.201, Risk: 156.332\n",
      "Episode: 5900 | Mean Objective: -350.112 | PnL: -193.544, Risk: 156.568\n",
      "Episode: 6000 | Mean Objective: -350.204 | PnL: -193.588, Risk: 156.616\n",
      "Episode: 6100 | Mean Objective: -350.036 | PnL: -193.523, Risk: 156.513\n",
      "Episode: 6200 | Mean Objective: -349.947 | PnL: -193.409, Risk: 156.538\n",
      "Episode: 6300 | Mean Objective: -350.075 | PnL: -193.423, Risk: 156.652\n",
      "Episode: 6400 | Mean Objective: -349.792 | PnL: -193.460, Risk: 156.332\n",
      "Episode: 6500 | Mean Objective: -349.624 | PnL: -193.336, Risk: 156.288\n",
      "Episode: 6600 | Mean Objective: -349.599 | PnL: -193.449, Risk: 156.149\n",
      "Episode: 6700 | Mean Objective: -349.630 | PnL: -193.330, Risk: 156.299\n",
      "Episode: 6800 | Mean Objective: -349.725 | PnL: -193.459, Risk: 156.267\n",
      "Episode: 6900 | Mean Objective: -349.582 | PnL: -193.287, Risk: 156.294\n",
      "Episode: 7000 | Mean Objective: -349.420 | PnL: -193.192, Risk: 156.227\n",
      "Episode: 7100 | Mean Objective: -349.396 | PnL: -193.308, Risk: 156.088\n",
      "Episode: 7200 | Mean Objective: -349.290 | PnL: -193.267, Risk: 156.023\n",
      "Episode: 7300 | Mean Objective: -348.911 | PnL: -192.997, Risk: 155.915\n",
      "Episode: 7400 | Mean Objective: -348.861 | PnL: -192.915, Risk: 155.946\n",
      "Episode: 7500 | Mean Objective: -348.861 | PnL: -193.005, Risk: 155.856\n",
      "Episode: 7600 | Mean Objective: -348.668 | PnL: -192.988, Risk: 155.680\n",
      "Episode: 7700 | Mean Objective: -348.520 | PnL: -193.116, Risk: 155.404\n",
      "Episode: 7800 | Mean Objective: -348.090 | PnL: -192.863, Risk: 155.227\n",
      "Episode: 7900 | Mean Objective: -348.123 | PnL: -192.995, Risk: 155.128\n",
      "Episode: 8000 | Mean Objective: -348.127 | PnL: -193.009, Risk: 155.118\n",
      "Episode: 8100 | Mean Objective: -347.782 | PnL: -192.742, Risk: 155.039\n",
      "Episode: 8200 | Mean Objective: -347.721 | PnL: -192.787, Risk: 154.934\n",
      "Episode: 8300 | Mean Objective: -347.687 | PnL: -192.785, Risk: 154.902\n",
      "Episode: 8400 | Mean Objective: -347.667 | PnL: -192.781, Risk: 154.886\n",
      "Episode: 8500 | Mean Objective: -347.459 | PnL: -192.496, Risk: 154.963\n",
      "Episode: 8600 | Mean Objective: -347.291 | PnL: -192.518, Risk: 154.772\n",
      "Episode: 8700 | Mean Objective: -347.428 | PnL: -192.602, Risk: 154.827\n",
      "Episode: 8800 | Mean Objective: -347.489 | PnL: -192.622, Risk: 154.867\n",
      "Episode: 8900 | Mean Objective: -347.448 | PnL: -192.592, Risk: 154.856\n",
      "Episode: 9000 | Mean Objective: -347.603 | PnL: -192.591, Risk: 155.012\n",
      "Episode: 9100 | Mean Objective: -347.505 | PnL: -192.542, Risk: 154.963\n",
      "Episode: 9200 | Mean Objective: -347.444 | PnL: -192.518, Risk: 154.926\n",
      "Episode: 9300 | Mean Objective: -347.634 | PnL: -192.598, Risk: 155.036\n",
      "Episode: 9400 | Mean Objective: -347.727 | PnL: -192.595, Risk: 155.132\n",
      "Episode: 9500 | Mean Objective: -347.599 | PnL: -192.594, Risk: 155.005\n",
      "Episode: 9600 | Mean Objective: -347.578 | PnL: -192.685, Risk: 154.893\n",
      "Episode: 9700 | Mean Objective: -347.759 | PnL: -192.749, Risk: 155.009\n",
      "Episode: 9800 | Mean Objective: -347.634 | PnL: -192.623, Risk: 155.011\n",
      "Episode: 9900 | Mean Objective: -347.717 | PnL: -192.809, Risk: 154.908\n",
      "Episode: 10000 | Mean Objective: -347.723 | PnL: -192.798, Risk: 154.925\n",
      "\n",
      "Cold Start Actor:\n",
      "testing...\n",
      "Episode: 18/10000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262316/3957969931.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _,bs_history = evaluation().eval_policy(env, torch.load(f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 | Mean Objective: -332.862 | PnL: -66.761, Risk: 266.102\n",
      "Episode: 200 | Mean Objective: -350.220 | PnL: -72.445, Risk: 277.774\n",
      "Episode: 300 | Mean Objective: -334.398 | PnL: -65.345, Risk: 269.052\n",
      "Episode: 400 | Mean Objective: -333.536 | PnL: -62.035, Risk: 271.501\n",
      "Episode: 500 | Mean Objective: -330.682 | PnL: -58.613, Risk: 272.069\n",
      "Episode: 600 | Mean Objective: -326.437 | PnL: -58.143, Risk: 268.294\n",
      "Episode: 700 | Mean Objective: -332.027 | PnL: -58.196, Risk: 273.832\n",
      "Episode: 800 | Mean Objective: -336.711 | PnL: -59.399, Risk: 277.312\n",
      "Episode: 900 | Mean Objective: -338.009 | PnL: -57.576, Risk: 280.433\n",
      "Episode: 1000 | Mean Objective: -335.155 | PnL: -56.710, Risk: 278.445\n",
      "Episode: 1100 | Mean Objective: -331.297 | PnL: -54.247, Risk: 277.050\n",
      "Episode: 1200 | Mean Objective: -334.088 | PnL: -57.158, Risk: 276.930\n",
      "Episode: 1300 | Mean Objective: -338.002 | PnL: -58.623, Risk: 279.380\n",
      "Episode: 1400 | Mean Objective: -339.929 | PnL: -59.462, Risk: 280.467\n",
      "Episode: 1500 | Mean Objective: -337.178 | PnL: -58.278, Risk: 278.900\n",
      "Episode: 1600 | Mean Objective: -332.859 | PnL: -56.529, Risk: 276.330\n",
      "Episode: 1700 | Mean Objective: -333.469 | PnL: -57.120, Risk: 276.349\n",
      "Episode: 1800 | Mean Objective: -338.505 | PnL: -58.696, Risk: 279.809\n",
      "Episode: 1900 | Mean Objective: -339.162 | PnL: -58.928, Risk: 280.233\n",
      "Episode: 2000 | Mean Objective: -336.627 | PnL: -57.921, Risk: 278.706\n",
      "Episode: 2100 | Mean Objective: -336.144 | PnL: -58.210, Risk: 277.934\n",
      "Episode: 2200 | Mean Objective: -334.039 | PnL: -57.708, Risk: 276.331\n",
      "Episode: 2300 | Mean Objective: -332.917 | PnL: -57.189, Risk: 275.727\n",
      "Episode: 2400 | Mean Objective: -332.320 | PnL: -56.556, Risk: 275.765\n",
      "Episode: 2500 | Mean Objective: -331.129 | PnL: -56.342, Risk: 274.787\n",
      "Episode: 2600 | Mean Objective: -331.296 | PnL: -56.188, Risk: 275.108\n",
      "Episode: 2700 | Mean Objective: -330.557 | PnL: -56.683, Risk: 273.874\n",
      "Episode: 2800 | Mean Objective: -331.103 | PnL: -57.019, Risk: 274.084\n",
      "Episode: 2900 | Mean Objective: -329.359 | PnL: -55.860, Risk: 273.500\n",
      "Episode: 3000 | Mean Objective: -329.945 | PnL: -56.504, Risk: 273.441\n",
      "Episode: 3100 | Mean Objective: -330.242 | PnL: -57.117, Risk: 273.125\n",
      "Episode: 3200 | Mean Objective: -329.416 | PnL: -57.133, Risk: 272.283\n",
      "Episode: 3300 | Mean Objective: -326.800 | PnL: -55.671, Risk: 271.128\n",
      "Episode: 3400 | Mean Objective: -326.575 | PnL: -56.131, Risk: 270.444\n",
      "Episode: 3500 | Mean Objective: -326.623 | PnL: -56.106, Risk: 270.517\n",
      "Episode: 3600 | Mean Objective: -326.103 | PnL: -56.034, Risk: 270.070\n",
      "Episode: 3700 | Mean Objective: -326.010 | PnL: -56.111, Risk: 269.900\n",
      "Episode: 3800 | Mean Objective: -325.385 | PnL: -56.197, Risk: 269.188\n",
      "Episode: 3900 | Mean Objective: -324.311 | PnL: -55.823, Risk: 268.489\n",
      "Episode: 4000 | Mean Objective: -324.476 | PnL: -55.868, Risk: 268.608\n",
      "Episode: 4100 | Mean Objective: -324.778 | PnL: -56.074, Risk: 268.704\n",
      "Episode: 4200 | Mean Objective: -324.791 | PnL: -56.140, Risk: 268.650\n",
      "Episode: 4300 | Mean Objective: -324.453 | PnL: -55.924, Risk: 268.529\n",
      "Episode: 4400 | Mean Objective: -324.897 | PnL: -56.039, Risk: 268.858\n",
      "Episode: 4500 | Mean Objective: -324.396 | PnL: -55.776, Risk: 268.620\n",
      "Episode: 4600 | Mean Objective: -324.599 | PnL: -55.618, Risk: 268.981\n",
      "Episode: 4700 | Mean Objective: -325.480 | PnL: -56.113, Risk: 269.368\n",
      "Episode: 4800 | Mean Objective: -325.781 | PnL: -56.438, Risk: 269.343\n",
      "Episode: 4900 | Mean Objective: -325.814 | PnL: -56.488, Risk: 269.326\n",
      "Episode: 5000 | Mean Objective: -325.883 | PnL: -56.491, Risk: 269.392\n",
      "Episode: 5100 | Mean Objective: -325.544 | PnL: -56.412, Risk: 269.131\n",
      "Episode: 5200 | Mean Objective: -326.043 | PnL: -56.998, Risk: 269.045\n",
      "Episode: 5300 | Mean Objective: -326.256 | PnL: -56.911, Risk: 269.345\n",
      "Episode: 5400 | Mean Objective: -326.280 | PnL: -57.027, Risk: 269.252\n",
      "Episode: 5500 | Mean Objective: -325.985 | PnL: -56.870, Risk: 269.114\n",
      "Episode: 5600 | Mean Objective: -325.820 | PnL: -56.842, Risk: 268.977\n",
      "Episode: 5700 | Mean Objective: -326.086 | PnL: -57.158, Risk: 268.928\n",
      "Episode: 5800 | Mean Objective: -325.353 | PnL: -56.619, Risk: 268.734\n",
      "Episode: 5900 | Mean Objective: -325.617 | PnL: -56.943, Risk: 268.673\n",
      "Episode: 6000 | Mean Objective: -325.631 | PnL: -56.915, Risk: 268.716\n",
      "Episode: 6100 | Mean Objective: -325.752 | PnL: -56.966, Risk: 268.786\n",
      "Episode: 6200 | Mean Objective: -325.770 | PnL: -56.997, Risk: 268.773\n",
      "Episode: 6300 | Mean Objective: -325.516 | PnL: -56.951, Risk: 268.565\n",
      "Episode: 6400 | Mean Objective: -324.858 | PnL: -56.884, Risk: 267.974\n",
      "Episode: 6500 | Mean Objective: -324.481 | PnL: -56.469, Risk: 268.012\n",
      "Episode: 6600 | Mean Objective: -323.482 | PnL: -55.997, Risk: 267.486\n",
      "Episode: 6700 | Mean Objective: -322.950 | PnL: -55.850, Risk: 267.099\n",
      "Episode: 6800 | Mean Objective: -321.937 | PnL: -55.193, Risk: 266.744\n",
      "Episode: 6900 | Mean Objective: -323.138 | PnL: -55.914, Risk: 267.223\n",
      "Episode: 7000 | Mean Objective: -322.534 | PnL: -55.722, Risk: 266.812\n",
      "Episode: 7100 | Mean Objective: -322.118 | PnL: -55.454, Risk: 266.664\n",
      "Episode: 7200 | Mean Objective: -322.489 | PnL: -55.869, Risk: 266.620\n",
      "Episode: 7300 | Mean Objective: -322.313 | PnL: -55.853, Risk: 266.459\n",
      "Episode: 7400 | Mean Objective: -322.289 | PnL: -55.788, Risk: 266.501\n",
      "Episode: 7500 | Mean Objective: -322.766 | PnL: -55.969, Risk: 266.797\n",
      "Episode: 7600 | Mean Objective: -323.147 | PnL: -56.144, Risk: 267.002\n",
      "Episode: 7700 | Mean Objective: -323.604 | PnL: -56.263, Risk: 267.342\n",
      "Episode: 7800 | Mean Objective: -323.442 | PnL: -55.993, Risk: 267.449\n",
      "Episode: 7900 | Mean Objective: -323.263 | PnL: -55.953, Risk: 267.310\n",
      "Episode: 8000 | Mean Objective: -322.900 | PnL: -55.576, Risk: 267.325\n",
      "Episode: 8100 | Mean Objective: -322.734 | PnL: -55.502, Risk: 267.232\n",
      "Episode: 8200 | Mean Objective: -322.750 | PnL: -55.261, Risk: 267.489\n",
      "Episode: 8300 | Mean Objective: -323.291 | PnL: -55.656, Risk: 267.636\n",
      "Episode: 8400 | Mean Objective: -323.078 | PnL: -55.481, Risk: 267.596\n",
      "Episode: 8500 | Mean Objective: -323.119 | PnL: -55.397, Risk: 267.722\n",
      "Episode: 8600 | Mean Objective: -323.781 | PnL: -55.499, Risk: 268.282\n",
      "Episode: 8700 | Mean Objective: -323.192 | PnL: -55.250, Risk: 267.942\n",
      "Episode: 8800 | Mean Objective: -323.163 | PnL: -55.399, Risk: 267.764\n",
      "Episode: 8900 | Mean Objective: -323.029 | PnL: -55.401, Risk: 267.628\n",
      "Episode: 9000 | Mean Objective: -322.919 | PnL: -55.369, Risk: 267.551\n",
      "Episode: 9100 | Mean Objective: -323.238 | PnL: -55.720, Risk: 267.518\n",
      "Episode: 9200 | Mean Objective: -322.707 | PnL: -55.382, Risk: 267.325\n",
      "Episode: 9300 | Mean Objective: -323.393 | PnL: -55.574, Risk: 267.819\n",
      "Episode: 9400 | Mean Objective: -323.810 | PnL: -55.820, Risk: 267.990\n",
      "Episode: 9500 | Mean Objective: -323.721 | PnL: -55.676, Risk: 268.045\n",
      "Episode: 9600 | Mean Objective: -324.287 | PnL: -55.840, Risk: 268.446\n",
      "Episode: 9700 | Mean Objective: -324.629 | PnL: -55.955, Risk: 268.674\n",
      "Episode: 9800 | Mean Objective: -324.292 | PnL: -55.867, Risk: 268.425\n",
      "Episode: 9900 | Mean Objective: -324.464 | PnL: -55.997, Risk: 268.467\n",
      "Episode: 10000 | Mean Objective: -324.851 | PnL: -56.217, Risk: 268.634\n",
      "\n",
      "Hot Start Actor:\n",
      "testing...\n",
      "Episode: 24/10000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262316/3957969931.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _,bs_history = evaluation().eval_policy(env, torch.load(f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 | Mean Objective: -357.046 | PnL: -75.348, Risk: 281.698\n",
      "Episode: 200 | Mean Objective: -359.273 | PnL: -81.387, Risk: 277.886\n",
      "Episode: 300 | Mean Objective: -387.206 | PnL: -82.838, Risk: 304.369\n",
      "Episode: 400 | Mean Objective: -380.385 | PnL: -82.358, Risk: 298.028\n",
      "Episode: 500 | Mean Objective: -373.584 | PnL: -79.411, Risk: 294.172\n",
      "Episode: 600 | Mean Objective: -383.572 | PnL: -83.420, Risk: 300.152\n",
      "Episode: 700 | Mean Objective: -371.411 | PnL: -75.958, Risk: 295.453\n",
      "Episode: 800 | Mean Objective: -370.710 | PnL: -76.995, Risk: 293.715\n",
      "Episode: 900 | Mean Objective: -372.090 | PnL: -77.351, Risk: 294.739\n",
      "Episode: 1000 | Mean Objective: -365.562 | PnL: -73.456, Risk: 292.106\n",
      "Episode: 1100 | Mean Objective: -366.908 | PnL: -74.425, Risk: 292.483\n",
      "Episode: 1200 | Mean Objective: -364.893 | PnL: -73.840, Risk: 291.053\n",
      "Episode: 1300 | Mean Objective: -365.173 | PnL: -73.844, Risk: 291.329\n",
      "Episode: 1400 | Mean Objective: -364.837 | PnL: -73.883, Risk: 290.954\n",
      "Episode: 1500 | Mean Objective: -364.802 | PnL: -73.514, Risk: 291.288\n",
      "Episode: 1600 | Mean Objective: -363.003 | PnL: -73.179, Risk: 289.823\n",
      "Episode: 1700 | Mean Objective: -362.834 | PnL: -73.901, Risk: 288.933\n",
      "Episode: 1800 | Mean Objective: -362.567 | PnL: -74.200, Risk: 288.367\n",
      "Episode: 1900 | Mean Objective: -361.728 | PnL: -73.874, Risk: 287.854\n",
      "Episode: 2000 | Mean Objective: -357.536 | PnL: -71.386, Risk: 286.150\n",
      "Episode: 2100 | Mean Objective: -358.985 | PnL: -71.336, Risk: 287.649\n",
      "Episode: 2200 | Mean Objective: -359.329 | PnL: -71.046, Risk: 288.284\n",
      "Episode: 2300 | Mean Objective: -358.038 | PnL: -70.100, Risk: 287.938\n",
      "Episode: 2400 | Mean Objective: -359.816 | PnL: -70.162, Risk: 289.654\n",
      "Episode: 2500 | Mean Objective: -359.909 | PnL: -70.554, Risk: 289.355\n",
      "Episode: 2600 | Mean Objective: -358.559 | PnL: -70.481, Risk: 288.078\n",
      "Episode: 2700 | Mean Objective: -359.079 | PnL: -70.107, Risk: 288.971\n",
      "Episode: 2800 | Mean Objective: -360.223 | PnL: -69.819, Risk: 290.404\n",
      "Episode: 2900 | Mean Objective: -361.171 | PnL: -70.218, Risk: 290.953\n",
      "Episode: 3000 | Mean Objective: -359.324 | PnL: -69.274, Risk: 290.050\n",
      "Episode: 3100 | Mean Objective: -359.199 | PnL: -69.781, Risk: 289.417\n",
      "Episode: 3200 | Mean Objective: -358.911 | PnL: -69.075, Risk: 289.836\n",
      "Episode: 3300 | Mean Objective: -358.325 | PnL: -69.160, Risk: 289.165\n",
      "Episode: 3400 | Mean Objective: -356.696 | PnL: -68.497, Risk: 288.199\n",
      "Episode: 3500 | Mean Objective: -357.704 | PnL: -68.942, Risk: 288.762\n",
      "Episode: 3600 | Mean Objective: -356.640 | PnL: -68.354, Risk: 288.286\n",
      "Episode: 3700 | Mean Objective: -355.741 | PnL: -67.455, Risk: 288.286\n",
      "Episode: 3800 | Mean Objective: -355.426 | PnL: -67.462, Risk: 287.964\n",
      "Episode: 3900 | Mean Objective: -354.439 | PnL: -67.378, Risk: 287.061\n",
      "Episode: 4000 | Mean Objective: -354.815 | PnL: -68.053, Risk: 286.763\n",
      "Episode: 4100 | Mean Objective: -353.925 | PnL: -67.957, Risk: 285.967\n",
      "Episode: 4200 | Mean Objective: -352.696 | PnL: -67.508, Risk: 285.188\n",
      "Episode: 4300 | Mean Objective: -353.287 | PnL: -67.759, Risk: 285.528\n",
      "Episode: 4400 | Mean Objective: -353.102 | PnL: -67.648, Risk: 285.454\n",
      "Episode: 4500 | Mean Objective: -352.614 | PnL: -67.634, Risk: 284.980\n",
      "Episode: 4600 | Mean Objective: -351.978 | PnL: -67.489, Risk: 284.489\n",
      "Episode: 4700 | Mean Objective: -352.382 | PnL: -67.696, Risk: 284.686\n",
      "Episode: 4800 | Mean Objective: -350.326 | PnL: -66.825, Risk: 283.501\n",
      "Episode: 4900 | Mean Objective: -350.248 | PnL: -66.613, Risk: 283.634\n",
      "Episode: 5000 | Mean Objective: -349.554 | PnL: -66.439, Risk: 283.115\n",
      "Episode: 5100 | Mean Objective: -350.453 | PnL: -66.779, Risk: 283.674\n",
      "Episode: 5200 | Mean Objective: -350.088 | PnL: -66.553, Risk: 283.535\n",
      "Episode: 5300 | Mean Objective: -348.816 | PnL: -65.875, Risk: 282.941\n",
      "Episode: 5400 | Mean Objective: -348.777 | PnL: -66.099, Risk: 282.678\n",
      "Episode: 5500 | Mean Objective: -347.876 | PnL: -65.460, Risk: 282.416\n",
      "Episode: 5600 | Mean Objective: -347.202 | PnL: -65.390, Risk: 281.813\n",
      "Episode: 5700 | Mean Objective: -347.008 | PnL: -65.602, Risk: 281.407\n",
      "Episode: 5800 | Mean Objective: -346.893 | PnL: -65.605, Risk: 281.288\n",
      "Episode: 5900 | Mean Objective: -347.843 | PnL: -66.135, Risk: 281.708\n",
      "Episode: 6000 | Mean Objective: -347.799 | PnL: -66.057, Risk: 281.742\n",
      "Episode: 6100 | Mean Objective: -347.722 | PnL: -66.064, Risk: 281.658\n",
      "Episode: 6200 | Mean Objective: -347.509 | PnL: -65.773, Risk: 281.736\n",
      "Episode: 6300 | Mean Objective: -346.699 | PnL: -65.373, Risk: 281.327\n",
      "Episode: 6400 | Mean Objective: -346.354 | PnL: -65.007, Risk: 281.347\n",
      "Episode: 6500 | Mean Objective: -345.766 | PnL: -64.790, Risk: 280.976\n",
      "Episode: 6600 | Mean Objective: -345.210 | PnL: -64.733, Risk: 280.477\n",
      "Episode: 6700 | Mean Objective: -345.033 | PnL: -64.789, Risk: 280.244\n",
      "Episode: 6800 | Mean Objective: -345.179 | PnL: -64.769, Risk: 280.409\n",
      "Episode: 6900 | Mean Objective: -344.816 | PnL: -64.414, Risk: 280.401\n",
      "Episode: 7000 | Mean Objective: -344.541 | PnL: -64.393, Risk: 280.148\n",
      "Episode: 7100 | Mean Objective: -344.652 | PnL: -64.649, Risk: 280.003\n",
      "Episode: 7200 | Mean Objective: -344.635 | PnL: -64.774, Risk: 279.861\n",
      "Episode: 7300 | Mean Objective: -343.859 | PnL: -64.713, Risk: 279.145\n",
      "Episode: 7400 | Mean Objective: -344.459 | PnL: -64.982, Risk: 279.477\n",
      "Episode: 7500 | Mean Objective: -343.739 | PnL: -64.706, Risk: 279.033\n",
      "Episode: 7600 | Mean Objective: -343.378 | PnL: -64.723, Risk: 278.655\n",
      "Episode: 7700 | Mean Objective: -343.567 | PnL: -65.152, Risk: 278.415\n",
      "Episode: 7800 | Mean Objective: -343.662 | PnL: -65.088, Risk: 278.574\n",
      "Episode: 7900 | Mean Objective: -343.798 | PnL: -64.705, Risk: 279.093\n",
      "Episode: 8000 | Mean Objective: -343.738 | PnL: -64.615, Risk: 279.123\n",
      "Episode: 8100 | Mean Objective: -343.735 | PnL: -64.759, Risk: 278.976\n",
      "Episode: 8200 | Mean Objective: -343.651 | PnL: -64.480, Risk: 279.171\n",
      "Episode: 8300 | Mean Objective: -344.096 | PnL: -64.898, Risk: 279.198\n",
      "Episode: 8400 | Mean Objective: -343.632 | PnL: -64.812, Risk: 278.821\n",
      "Episode: 8500 | Mean Objective: -343.312 | PnL: -64.492, Risk: 278.820\n",
      "Episode: 8600 | Mean Objective: -342.927 | PnL: -64.262, Risk: 278.665\n",
      "Episode: 8700 | Mean Objective: -342.494 | PnL: -64.165, Risk: 278.329\n",
      "Episode: 8800 | Mean Objective: -342.835 | PnL: -64.415, Risk: 278.420\n",
      "Episode: 8900 | Mean Objective: -342.743 | PnL: -64.474, Risk: 278.269\n",
      "Episode: 9000 | Mean Objective: -343.531 | PnL: -64.735, Risk: 278.797\n",
      "Episode: 9100 | Mean Objective: -343.339 | PnL: -64.739, Risk: 278.600\n",
      "Episode: 9200 | Mean Objective: -343.082 | PnL: -64.583, Risk: 278.499\n",
      "Episode: 9300 | Mean Objective: -343.591 | PnL: -64.832, Risk: 278.760\n",
      "Episode: 9400 | Mean Objective: -342.887 | PnL: -64.545, Risk: 278.343\n",
      "Episode: 9500 | Mean Objective: -342.638 | PnL: -64.446, Risk: 278.192\n",
      "Episode: 9600 | Mean Objective: -342.784 | PnL: -64.572, Risk: 278.213\n",
      "Episode: 9700 | Mean Objective: -341.999 | PnL: -64.027, Risk: 277.972\n",
      "Episode: 9800 | Mean Objective: -341.646 | PnL: -63.824, Risk: 277.822\n",
      "Episode: 9900 | Mean Objective: -341.739 | PnL: -63.806, Risk: 277.933\n",
      "Episode: 10000 | Mean Objective: -341.561 | PnL: -63.650, Risk: 277.911\n"
     ]
    }
   ],
   "source": [
    "# Test performance of policy that does nothing every step\n",
    "n = 10000\n",
    "\n",
    "print(\"'Do Nothing' policy:\")\n",
    "_,nothing_history = evaluation().eval_policy(env, \"Nothing\", episodes = n, verbose = True)\n",
    "\n",
    "# Test analytical BS hedge\n",
    "print()\n",
    "print(\"BS policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"BS\", episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Cold Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Hot Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c1dc5-7791-42da-8bdc-9ed3c53d1cae",
   "metadata": {},
   "source": [
    "#### Cold Start Model:\n",
    "1. Initialize actor and critic functions\n",
    "2. Call DDPG train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d751833e-3a39-4963-81ce-bec6dda7d40a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -1182.663 | PnL: -719.668, Risk: 462.995 | e: 0.990\n",
      "Episode: 200 | Mean Objective: -1107.030 | PnL: -669.455, Risk: 437.574 | e: 0.980\n",
      "Episode: 300 | Mean Objective: -1106.163 | PnL: -671.607, Risk: 434.556 | e: 0.970\n",
      "Episode: 400 | Mean Objective: -1171.221 | PnL: -758.832, Risk: 412.389 | e: 0.961\n",
      "Episode: 500 | Mean Objective: -1121.366 | PnL: -694.669, Risk: 426.697 | e: 0.951\n",
      "Episode: 600 | Mean Objective: -1158.925 | PnL: -726.193, Risk: 432.733 | e: 0.942\n",
      "Episode: 700 | Mean Objective: -1071.113 | PnL: -678.975, Risk: 392.138 | e: 0.932\n",
      "Episode: 800 | Mean Objective: -1145.819 | PnL: -736.247, Risk: 409.572 | e: 0.923\n",
      "Episode: 900 | Mean Objective: -1071.420 | PnL: -672.657, Risk: 398.763 | e: 0.914\n",
      "Episode: 1000 | Mean Objective: -1070.463 | PnL: -682.054, Risk: 388.409 | e: 0.905\n",
      "Episode: 1100 | Mean Objective: -1127.588 | PnL: -710.365, Risk: 417.223 | e: 0.896\n",
      "Episode: 1200 | Mean Objective: -1237.975 | PnL: -812.630, Risk: 425.345 | e: 0.887\n",
      "Episode: 1300 | Mean Objective: -1146.679 | PnL: -744.846, Risk: 401.833 | e: 0.878\n",
      "Episode: 1400 | Mean Objective: -1160.961 | PnL: -673.263, Risk: 487.699 | e: 0.869\n",
      "Episode: 1500 | Mean Objective: -1116.346 | PnL: -676.522, Risk: 439.825 | e: 0.861\n",
      "Episode: 1600 | Mean Objective: -1022.115 | PnL: -610.500, Risk: 411.616 | e: 0.852\n",
      "Episode: 1700 | Mean Objective: -1060.847 | PnL: -621.699, Risk: 439.148 | e: 0.844\n",
      "Episode: 1800 | Mean Objective: -963.267 | PnL: -578.800, Risk: 384.468 | e: 0.835\n",
      "Episode: 1900 | Mean Objective: -994.578 | PnL: -581.913, Risk: 412.665 | e: 0.827\n",
      "Episode: 2000 | Mean Objective: -1001.397 | PnL: -539.371, Risk: 462.026 | e: 0.819\n",
      "Episode: 2100 | Mean Objective: -994.395 | PnL: -563.470, Risk: 430.925 | e: 0.811\n",
      "Episode: 2200 | Mean Objective: -897.581 | PnL: -507.007, Risk: 390.574 | e: 0.803\n",
      "Episode: 2300 | Mean Objective: -932.060 | PnL: -566.159, Risk: 365.900 | e: 0.795\n",
      "Episode: 2400 | Mean Objective: -1162.510 | PnL: -639.553, Risk: 522.957 | e: 0.787\n",
      "Episode: 2500 | Mean Objective: -959.076 | PnL: -583.271, Risk: 375.804 | e: 0.779\n",
      "Episode: 2600 | Mean Objective: -989.203 | PnL: -562.816, Risk: 426.387 | e: 0.771\n",
      "Episode: 2700 | Mean Objective: -970.340 | PnL: -570.484, Risk: 399.856 | e: 0.763\n",
      "Episode: 2800 | Mean Objective: -1042.568 | PnL: -594.435, Risk: 448.133 | e: 0.756\n",
      "Episode: 2900 | Mean Objective: -979.753 | PnL: -558.773, Risk: 420.979 | e: 0.748\n",
      "Episode: 3000 | Mean Objective: -936.072 | PnL: -555.467, Risk: 380.605 | e: 0.741\n",
      "Episode: 3100 | Mean Objective: -1071.889 | PnL: -578.127, Risk: 493.762 | e: 0.733\n",
      "Episode: 3200 | Mean Objective: -913.283 | PnL: -557.360, Risk: 355.923 | e: 0.726\n",
      "Episode: 3300 | Mean Objective: -960.512 | PnL: -580.902, Risk: 379.610 | e: 0.719\n",
      "Episode: 3400 | Mean Objective: -962.874 | PnL: -550.643, Risk: 412.232 | e: 0.712\n",
      "Episode: 3500 | Mean Objective: -1020.169 | PnL: -555.555, Risk: 464.614 | e: 0.705\n",
      "Episode: 3600 | Mean Objective: -894.376 | PnL: -531.524, Risk: 362.852 | e: 0.698\n",
      "Episode: 3700 | Mean Objective: -965.214 | PnL: -548.224, Risk: 416.989 | e: 0.691\n",
      "Episode: 3800 | Mean Objective: -965.916 | PnL: -540.252, Risk: 425.664 | e: 0.684\n",
      "Episode: 3900 | Mean Objective: -940.786 | PnL: -564.565, Risk: 376.222 | e: 0.677\n",
      "Episode: 4000 | Mean Objective: -952.769 | PnL: -523.778, Risk: 428.991 | e: 0.670\n",
      "Episode: 4100 | Mean Objective: -954.145 | PnL: -503.327, Risk: 450.818 | e: 0.664\n",
      "Episode: 4200 | Mean Objective: -900.001 | PnL: -545.852, Risk: 354.148 | e: 0.657\n",
      "Episode: 4300 | Mean Objective: -946.812 | PnL: -553.064, Risk: 393.748 | e: 0.650\n",
      "Episode: 4400 | Mean Objective: -953.680 | PnL: -568.708, Risk: 384.972 | e: 0.644\n",
      "Episode: 4500 | Mean Objective: -934.683 | PnL: -564.949, Risk: 369.734 | e: 0.638\n",
      "Episode: 4600 | Mean Objective: -976.020 | PnL: -551.901, Risk: 424.119 | e: 0.631\n",
      "Episode: 4700 | Mean Objective: -830.719 | PnL: -460.751, Risk: 369.968 | e: 0.625\n",
      "Episode: 4800 | Mean Objective: -927.329 | PnL: -547.701, Risk: 379.629 | e: 0.619\n",
      "Episode: 4900 | Mean Objective: -922.418 | PnL: -550.796, Risk: 371.622 | e: 0.613\n",
      "Episode: 5000 | Mean Objective: -847.067 | PnL: -471.738, Risk: 375.329 | e: 0.607\n",
      "Episode: 5100 | Mean Objective: -899.028 | PnL: -522.389, Risk: 376.639 | e: 0.600\n",
      "Episode: 5200 | Mean Objective: -897.584 | PnL: -511.207, Risk: 386.377 | e: 0.595\n",
      "Episode: 5300 | Mean Objective: -899.586 | PnL: -531.521, Risk: 368.066 | e: 0.589\n",
      "Episode: 5400 | Mean Objective: -864.098 | PnL: -492.710, Risk: 371.387 | e: 0.583\n",
      "Episode: 5500 | Mean Objective: -868.213 | PnL: -459.908, Risk: 408.305 | e: 0.577\n",
      "Episode: 5600 | Mean Objective: -818.308 | PnL: -455.755, Risk: 362.553 | e: 0.571\n",
      "Episode: 5700 | Mean Objective: -899.389 | PnL: -501.033, Risk: 398.356 | e: 0.566\n",
      "Episode: 5800 | Mean Objective: -893.549 | PnL: -490.633, Risk: 402.915 | e: 0.560\n",
      "Episode: 5900 | Mean Objective: -911.635 | PnL: -513.296, Risk: 398.338 | e: 0.554\n",
      "Episode: 6000 | Mean Objective: -850.578 | PnL: -452.485, Risk: 398.093 | e: 0.549\n",
      "Episode: 6100 | Mean Objective: -882.376 | PnL: -479.222, Risk: 403.154 | e: 0.543\n",
      "Episode: 6200 | Mean Objective: -809.568 | PnL: -452.621, Risk: 356.947 | e: 0.538\n",
      "Episode: 6300 | Mean Objective: -744.726 | PnL: -414.781, Risk: 329.945 | e: 0.533\n",
      "Episode: 6400 | Mean Objective: -844.311 | PnL: -474.963, Risk: 369.348 | e: 0.527\n",
      "Episode: 6500 | Mean Objective: -746.050 | PnL: -421.908, Risk: 324.142 | e: 0.522\n",
      "Episode: 6600 | Mean Objective: -717.212 | PnL: -414.377, Risk: 302.835 | e: 0.517\n",
      "Episode: 6700 | Mean Objective: -796.680 | PnL: -461.840, Risk: 334.840 | e: 0.512\n",
      "Episode: 6800 | Mean Objective: -842.907 | PnL: -460.278, Risk: 382.630 | e: 0.507\n",
      "Episode: 6900 | Mean Objective: -801.767 | PnL: -425.886, Risk: 375.882 | e: 0.502\n",
      "Episode: 7000 | Mean Objective: -837.938 | PnL: -478.588, Risk: 359.350 | e: 0.497\n",
      "Episode: 7100 | Mean Objective: -816.444 | PnL: -445.945, Risk: 370.499 | e: 0.492\n",
      "Episode: 7200 | Mean Objective: -843.869 | PnL: -454.576, Risk: 389.293 | e: 0.487\n",
      "Episode: 7300 | Mean Objective: -830.210 | PnL: -433.484, Risk: 396.725 | e: 0.482\n",
      "Episode: 7400 | Mean Objective: -879.902 | PnL: -443.422, Risk: 436.480 | e: 0.477\n",
      "Episode: 7500 | Mean Objective: -761.911 | PnL: -420.105, Risk: 341.805 | e: 0.472\n",
      "Episode: 7600 | Mean Objective: -758.549 | PnL: -386.166, Risk: 372.383 | e: 0.468\n",
      "Episode: 7700 | Mean Objective: -851.028 | PnL: -432.832, Risk: 418.196 | e: 0.463\n",
      "Episode: 7800 | Mean Objective: -672.525 | PnL: -361.040, Risk: 311.486 | e: 0.458\n",
      "Episode: 7900 | Mean Objective: -787.580 | PnL: -422.390, Risk: 365.190 | e: 0.454\n",
      "Episode: 8000 | Mean Objective: -838.537 | PnL: -408.849, Risk: 429.688 | e: 0.449\n",
      "Episode: 8100 | Mean Objective: -788.769 | PnL: -409.581, Risk: 379.189 | e: 0.445\n",
      "Episode: 8200 | Mean Objective: -734.231 | PnL: -420.404, Risk: 313.827 | e: 0.440\n",
      "Episode: 8300 | Mean Objective: -765.840 | PnL: -382.474, Risk: 383.367 | e: 0.436\n",
      "Episode: 8400 | Mean Objective: -752.772 | PnL: -388.001, Risk: 364.771 | e: 0.432\n",
      "Episode: 8500 | Mean Objective: -737.879 | PnL: -384.701, Risk: 353.178 | e: 0.427\n",
      "Episode: 8600 | Mean Objective: -760.453 | PnL: -381.687, Risk: 378.767 | e: 0.423\n",
      "Episode: 8700 | Mean Objective: -797.815 | PnL: -402.008, Risk: 395.806 | e: 0.419\n",
      "Episode: 8800 | Mean Objective: -759.467 | PnL: -370.878, Risk: 388.589 | e: 0.415\n",
      "Episode: 8900 | Mean Objective: -613.183 | PnL: -338.731, Risk: 274.452 | e: 0.411\n",
      "Episode: 9000 | Mean Objective: -795.648 | PnL: -405.433, Risk: 390.215 | e: 0.407\n",
      "Episode: 9100 | Mean Objective: -726.546 | PnL: -360.613, Risk: 365.932 | e: 0.403\n",
      "Episode: 9200 | Mean Objective: -743.068 | PnL: -368.180, Risk: 374.888 | e: 0.399\n",
      "Episode: 9300 | Mean Objective: -714.708 | PnL: -367.555, Risk: 347.153 | e: 0.395\n",
      "Episode: 9400 | Mean Objective: -775.633 | PnL: -380.538, Risk: 395.096 | e: 0.391\n",
      "Episode: 9500 | Mean Objective: -791.409 | PnL: -400.971, Risk: 390.438 | e: 0.387\n",
      "Episode: 9600 | Mean Objective: -713.900 | PnL: -349.274, Risk: 364.627 | e: 0.383\n",
      "Episode: 9700 | Mean Objective: -672.603 | PnL: -334.540, Risk: 338.063 | e: 0.379\n",
      "Episode: 9800 | Mean Objective: -693.926 | PnL: -360.724, Risk: 333.202 | e: 0.375\n",
      "Episode: 9900 | Mean Objective: -668.634 | PnL: -335.074, Risk: 333.560 | e: 0.372\n",
      "Episode: 10000 | Mean Objective: -724.650 | PnL: -363.863, Risk: 360.787 | e: 0.368\n",
      "Episode: 10100 | Mean Objective: -781.359 | PnL: -390.138, Risk: 391.221 | e: 0.364\n",
      "Episode: 10200 | Mean Objective: -736.400 | PnL: -366.421, Risk: 369.979 | e: 0.361\n",
      "Episode: 10300 | Mean Objective: -701.791 | PnL: -343.064, Risk: 358.727 | e: 0.357\n",
      "Episode: 10400 | Mean Objective: -663.537 | PnL: -336.999, Risk: 326.538 | e: 0.353\n",
      "Episode: 10500 | Mean Objective: -773.813 | PnL: -376.667, Risk: 397.146 | e: 0.350\n",
      "Episode: 10600 | Mean Objective: -698.784 | PnL: -334.389, Risk: 364.395 | e: 0.346\n",
      "Episode: 10700 | Mean Objective: -646.452 | PnL: -321.492, Risk: 324.960 | e: 0.343\n",
      "Episode: 10800 | Mean Objective: -699.479 | PnL: -338.225, Risk: 361.253 | e: 0.340\n",
      "Episode: 10900 | Mean Objective: -669.154 | PnL: -347.593, Risk: 321.562 | e: 0.336\n",
      "Episode: 11000 | Mean Objective: -785.208 | PnL: -397.685, Risk: 387.523 | e: 0.333\n",
      "Episode: 11100 | Mean Objective: -665.160 | PnL: -343.087, Risk: 322.073 | e: 0.330\n",
      "Episode: 11200 | Mean Objective: -711.865 | PnL: -368.069, Risk: 343.795 | e: 0.326\n",
      "Episode: 11300 | Mean Objective: -602.875 | PnL: -308.258, Risk: 294.617 | e: 0.323\n",
      "Episode: 11400 | Mean Objective: -667.128 | PnL: -315.996, Risk: 351.131 | e: 0.320\n",
      "Episode: 11500 | Mean Objective: -686.842 | PnL: -339.510, Risk: 347.332 | e: 0.317\n",
      "Episode: 11600 | Mean Objective: -685.592 | PnL: -311.327, Risk: 374.265 | e: 0.313\n",
      "Episode: 11700 | Mean Objective: -619.931 | PnL: -309.495, Risk: 310.435 | e: 0.310\n",
      "Episode: 11800 | Mean Objective: -712.906 | PnL: -358.341, Risk: 354.565 | e: 0.307\n",
      "Episode: 11900 | Mean Objective: -648.903 | PnL: -314.448, Risk: 334.455 | e: 0.304\n",
      "Episode: 12000 | Mean Objective: -652.734 | PnL: -307.890, Risk: 344.844 | e: 0.301\n",
      "Episode: 12100 | Mean Objective: -606.068 | PnL: -293.090, Risk: 312.978 | e: 0.298\n",
      "Episode: 12200 | Mean Objective: -643.968 | PnL: -307.308, Risk: 336.660 | e: 0.295\n",
      "Episode: 12300 | Mean Objective: -640.200 | PnL: -288.250, Risk: 351.950 | e: 0.292\n",
      "Episode: 12400 | Mean Objective: -649.809 | PnL: -298.338, Risk: 351.471 | e: 0.289\n",
      "Episode: 12500 | Mean Objective: -521.428 | PnL: -238.126, Risk: 283.302 | e: 0.286\n",
      "Episode: 12600 | Mean Objective: -601.962 | PnL: -281.099, Risk: 320.863 | e: 0.284\n",
      "Episode: 12700 | Mean Objective: -674.477 | PnL: -308.272, Risk: 366.205 | e: 0.281\n",
      "Episode: 12800 | Mean Objective: -581.994 | PnL: -293.367, Risk: 288.626 | e: 0.278\n",
      "Episode: 12900 | Mean Objective: -679.444 | PnL: -289.942, Risk: 389.502 | e: 0.275\n",
      "Episode: 13000 | Mean Objective: -683.446 | PnL: -309.814, Risk: 373.632 | e: 0.273\n",
      "Episode: 13100 | Mean Objective: -636.276 | PnL: -283.323, Risk: 352.953 | e: 0.270\n",
      "Episode: 13200 | Mean Objective: -669.855 | PnL: -298.119, Risk: 371.736 | e: 0.267\n",
      "Episode: 13300 | Mean Objective: -610.268 | PnL: -263.113, Risk: 347.155 | e: 0.264\n",
      "Episode: 13400 | Mean Objective: -614.895 | PnL: -278.007, Risk: 336.888 | e: 0.262\n",
      "Episode: 13500 | Mean Objective: -556.139 | PnL: -254.603, Risk: 301.536 | e: 0.259\n",
      "Episode: 13600 | Mean Objective: -631.299 | PnL: -292.529, Risk: 338.770 | e: 0.257\n",
      "Episode: 13700 | Mean Objective: -595.510 | PnL: -281.317, Risk: 314.194 | e: 0.254\n",
      "Episode: 13800 | Mean Objective: -648.604 | PnL: -281.795, Risk: 366.809 | e: 0.252\n",
      "Episode: 13900 | Mean Objective: -621.590 | PnL: -255.128, Risk: 366.462 | e: 0.249\n",
      "Episode: 14000 | Mean Objective: -623.306 | PnL: -284.961, Risk: 338.345 | e: 0.247\n",
      "Episode: 14100 | Mean Objective: -557.985 | PnL: -271.430, Risk: 286.555 | e: 0.244\n",
      "Episode: 14200 | Mean Objective: -563.739 | PnL: -251.579, Risk: 312.160 | e: 0.242\n",
      "Episode: 14300 | Mean Objective: -645.181 | PnL: -293.413, Risk: 351.768 | e: 0.239\n",
      "Episode: 14400 | Mean Objective: -599.721 | PnL: -255.111, Risk: 344.610 | e: 0.237\n",
      "Episode: 14500 | Mean Objective: -598.340 | PnL: -263.771, Risk: 334.569 | e: 0.235\n",
      "Episode: 14600 | Mean Objective: -517.863 | PnL: -228.439, Risk: 289.423 | e: 0.232\n",
      "Episode: 14700 | Mean Objective: -603.972 | PnL: -264.283, Risk: 339.689 | e: 0.230\n",
      "Episode: 14800 | Mean Objective: -549.199 | PnL: -246.556, Risk: 302.642 | e: 0.228\n",
      "Episode: 14900 | Mean Objective: -587.818 | PnL: -261.684, Risk: 326.134 | e: 0.225\n",
      "Episode: 15000 | Mean Objective: -541.945 | PnL: -197.046, Risk: 344.898 | e: 0.223\n",
      "Episode: 15100 | Mean Objective: -579.330 | PnL: -245.582, Risk: 333.747 | e: 0.221\n",
      "Episode: 15200 | Mean Objective: -622.028 | PnL: -257.030, Risk: 364.998 | e: 0.219\n",
      "Episode: 15300 | Mean Objective: -565.126 | PnL: -254.005, Risk: 311.121 | e: 0.217\n",
      "Episode: 15400 | Mean Objective: -586.650 | PnL: -255.190, Risk: 331.460 | e: 0.214\n",
      "Episode: 15500 | Mean Objective: -510.588 | PnL: -211.432, Risk: 299.156 | e: 0.212\n",
      "Episode: 15600 | Mean Objective: -603.420 | PnL: -253.239, Risk: 350.180 | e: 0.210\n",
      "Episode: 15700 | Mean Objective: -526.384 | PnL: -229.666, Risk: 296.719 | e: 0.208\n",
      "Episode: 15800 | Mean Objective: -539.066 | PnL: -250.088, Risk: 288.979 | e: 0.206\n",
      "Episode: 15900 | Mean Objective: -577.631 | PnL: -251.837, Risk: 325.794 | e: 0.204\n",
      "Episode: 16000 | Mean Objective: -586.728 | PnL: -254.996, Risk: 331.732 | e: 0.202\n",
      "Episode: 16100 | Mean Objective: -527.578 | PnL: -194.380, Risk: 333.198 | e: 0.200\n",
      "Episode: 16200 | Mean Objective: -582.652 | PnL: -252.565, Risk: 330.087 | e: 0.198\n",
      "Episode: 16300 | Mean Objective: -490.295 | PnL: -192.904, Risk: 297.391 | e: 0.196\n",
      "Episode: 16400 | Mean Objective: -561.331 | PnL: -231.903, Risk: 329.428 | e: 0.194\n",
      "Episode: 16500 | Mean Objective: -542.270 | PnL: -247.101, Risk: 295.168 | e: 0.192\n",
      "Episode: 16600 | Mean Objective: -509.986 | PnL: -206.270, Risk: 303.715 | e: 0.190\n",
      "Episode: 16700 | Mean Objective: -479.648 | PnL: -182.049, Risk: 297.598 | e: 0.188\n",
      "Episode: 16800 | Mean Objective: -561.074 | PnL: -247.534, Risk: 313.540 | e: 0.186\n",
      "Episode: 16900 | Mean Objective: -548.864 | PnL: -219.420, Risk: 329.444 | e: 0.185\n",
      "Episode: 17000 | Mean Objective: -535.086 | PnL: -210.355, Risk: 324.732 | e: 0.183\n",
      "Episode: 17100 | Mean Objective: -520.116 | PnL: -208.540, Risk: 311.576 | e: 0.181\n",
      "Episode: 17200 | Mean Objective: -505.549 | PnL: -196.975, Risk: 308.574 | e: 0.179\n",
      "Episode: 17300 | Mean Objective: -453.643 | PnL: -184.003, Risk: 269.640 | e: 0.177\n",
      "Episode: 17400 | Mean Objective: -525.362 | PnL: -211.033, Risk: 314.329 | e: 0.176\n",
      "Episode: 17500 | Mean Objective: -509.697 | PnL: -203.421, Risk: 306.276 | e: 0.174\n",
      "Episode: 17600 | Mean Objective: -512.238 | PnL: -193.133, Risk: 319.105 | e: 0.172\n",
      "Episode: 17700 | Mean Objective: -415.078 | PnL: -160.490, Risk: 254.588 | e: 0.170\n",
      "Episode: 17800 | Mean Objective: -512.540 | PnL: -191.141, Risk: 321.400 | e: 0.169\n",
      "Episode: 17900 | Mean Objective: -540.400 | PnL: -198.539, Risk: 341.861 | e: 0.167\n",
      "Episode: 18000 | Mean Objective: -491.034 | PnL: -199.406, Risk: 291.628 | e: 0.165\n",
      "Episode: 18100 | Mean Objective: -388.469 | PnL: -154.713, Risk: 233.755 | e: 0.164\n",
      "Episode: 18200 | Mean Objective: -519.845 | PnL: -198.294, Risk: 321.551 | e: 0.162\n",
      "Episode: 18300 | Mean Objective: -514.405 | PnL: -217.434, Risk: 296.972 | e: 0.160\n",
      "Episode: 18400 | Mean Objective: -463.540 | PnL: -181.582, Risk: 281.958 | e: 0.159\n",
      "Episode: 18500 | Mean Objective: -561.064 | PnL: -228.665, Risk: 332.399 | e: 0.157\n",
      "Episode: 18600 | Mean Objective: -458.042 | PnL: -157.365, Risk: 300.678 | e: 0.156\n",
      "Episode: 18700 | Mean Objective: -482.158 | PnL: -177.117, Risk: 305.041 | e: 0.154\n",
      "Episode: 18800 | Mean Objective: -431.090 | PnL: -141.263, Risk: 289.827 | e: 0.153\n",
      "Episode: 18900 | Mean Objective: -507.710 | PnL: -165.611, Risk: 342.098 | e: 0.151\n",
      "Episode: 19000 | Mean Objective: -525.825 | PnL: -180.361, Risk: 345.464 | e: 0.150\n",
      "Episode: 19100 | Mean Objective: -479.097 | PnL: -178.016, Risk: 301.081 | e: 0.148\n",
      "Episode: 19200 | Mean Objective: -498.591 | PnL: -193.795, Risk: 304.795 | e: 0.147\n",
      "Episode: 19300 | Mean Objective: -505.783 | PnL: -176.770, Risk: 329.013 | e: 0.145\n",
      "Episode: 19400 | Mean Objective: -488.450 | PnL: -170.536, Risk: 317.914 | e: 0.144\n",
      "Episode: 19500 | Mean Objective: -510.778 | PnL: -165.156, Risk: 345.623 | e: 0.142\n",
      "Episode: 19600 | Mean Objective: -402.800 | PnL: -163.705, Risk: 239.095 | e: 0.141\n",
      "Episode: 19700 | Mean Objective: -473.100 | PnL: -158.734, Risk: 314.366 | e: 0.139\n",
      "Episode: 19800 | Mean Objective: -534.464 | PnL: -182.472, Risk: 351.992 | e: 0.138\n",
      "Episode: 19900 | Mean Objective: -524.962 | PnL: -208.711, Risk: 316.251 | e: 0.137\n",
      "Episode: 20000 | Mean Objective: -437.077 | PnL: -138.366, Risk: 298.712 | e: 0.135\n",
      "Episode: 20100 | Mean Objective: -460.936 | PnL: -147.897, Risk: 313.039 | e: 0.134\n",
      "Episode: 20200 | Mean Objective: -503.570 | PnL: -175.552, Risk: 328.018 | e: 0.133\n",
      "Episode: 20300 | Mean Objective: -473.253 | PnL: -155.211, Risk: 318.042 | e: 0.131\n",
      "Episode: 20400 | Mean Objective: -470.850 | PnL: -160.661, Risk: 310.189 | e: 0.130\n",
      "Episode: 20500 | Mean Objective: -454.229 | PnL: -160.783, Risk: 293.445 | e: 0.129\n",
      "Episode: 20600 | Mean Objective: -430.180 | PnL: -162.054, Risk: 268.126 | e: 0.127\n",
      "Episode: 20700 | Mean Objective: -415.483 | PnL: -146.512, Risk: 268.971 | e: 0.126\n",
      "Episode: 20800 | Mean Objective: -488.802 | PnL: -159.405, Risk: 329.397 | e: 0.125\n",
      "Episode: 20900 | Mean Objective: -445.659 | PnL: -158.315, Risk: 287.344 | e: 0.124\n",
      "Episode: 21000 | Mean Objective: -527.726 | PnL: -172.192, Risk: 355.533 | e: 0.122\n",
      "Episode: 21100 | Mean Objective: -531.318 | PnL: -193.895, Risk: 337.424 | e: 0.121\n",
      "Episode: 21200 | Mean Objective: -466.185 | PnL: -164.616, Risk: 301.569 | e: 0.120\n",
      "Episode: 21300 | Mean Objective: -439.023 | PnL: -151.866, Risk: 287.156 | e: 0.119\n",
      "Episode: 21400 | Mean Objective: -464.070 | PnL: -171.935, Risk: 292.135 | e: 0.118\n",
      "Episode: 21500 | Mean Objective: -447.475 | PnL: -163.433, Risk: 284.043 | e: 0.116\n",
      "Episode: 21600 | Mean Objective: -406.357 | PnL: -106.634, Risk: 299.723 | e: 0.115\n",
      "Episode: 21700 | Mean Objective: -472.401 | PnL: -168.556, Risk: 303.845 | e: 0.114\n",
      "Episode: 21800 | Mean Objective: -447.621 | PnL: -143.617, Risk: 304.004 | e: 0.113\n",
      "Episode: 21900 | Mean Objective: -483.797 | PnL: -186.959, Risk: 296.838 | e: 0.112\n",
      "Episode: 22000 | Mean Objective: -434.496 | PnL: -172.188, Risk: 262.309 | e: 0.111\n",
      "Episode: 22100 | Mean Objective: -509.442 | PnL: -198.458, Risk: 310.984 | e: 0.110\n",
      "Episode: 22200 | Mean Objective: -485.801 | PnL: -162.135, Risk: 323.666 | e: 0.109\n",
      "Episode: 22300 | Mean Objective: -439.967 | PnL: -154.202, Risk: 285.766 | e: 0.108\n",
      "Episode: 22400 | Mean Objective: -549.960 | PnL: -153.505, Risk: 396.455 | e: 0.106\n",
      "Episode: 22500 | Mean Objective: -455.931 | PnL: -161.214, Risk: 294.717 | e: 0.105\n",
      "Episode: 22600 | Mean Objective: -473.506 | PnL: -178.758, Risk: 294.748 | e: 0.104\n",
      "Episode: 22700 | Mean Objective: -483.482 | PnL: -172.786, Risk: 310.696 | e: 0.103\n",
      "Episode: 22800 | Mean Objective: -446.220 | PnL: -150.063, Risk: 296.156 | e: 0.102\n",
      "Episode: 22900 | Mean Objective: -509.431 | PnL: -177.883, Risk: 331.548 | e: 0.101\n",
      "Episode: 23000 | Mean Objective: -430.915 | PnL: -136.249, Risk: 294.666 | e: 0.100\n",
      "Episode: 23100 | Mean Objective: -460.916 | PnL: -170.059, Risk: 290.857 | e: 0.099\n",
      "Episode: 23200 | Mean Objective: -457.946 | PnL: -154.918, Risk: 303.028 | e: 0.098\n",
      "Episode: 23300 | Mean Objective: -423.414 | PnL: -123.068, Risk: 300.346 | e: 0.097\n",
      "Episode: 23400 | Mean Objective: -434.183 | PnL: -138.867, Risk: 295.316 | e: 0.096\n",
      "Episode: 23500 | Mean Objective: -420.073 | PnL: -142.195, Risk: 277.878 | e: 0.095\n",
      "Episode: 23600 | Mean Objective: -482.396 | PnL: -177.939, Risk: 304.457 | e: 0.094\n",
      "Episode: 23700 | Mean Objective: -448.986 | PnL: -143.088, Risk: 305.898 | e: 0.093\n",
      "Episode: 23800 | Mean Objective: -407.593 | PnL: -133.868, Risk: 273.725 | e: 0.093\n",
      "Episode: 23900 | Mean Objective: -465.547 | PnL: -144.322, Risk: 321.225 | e: 0.092\n",
      "Episode: 24000 | Mean Objective: -453.029 | PnL: -166.489, Risk: 286.540 | e: 0.091\n",
      "Episode: 24100 | Mean Objective: -421.747 | PnL: -136.478, Risk: 285.270 | e: 0.090\n",
      "Episode: 24200 | Mean Objective: -421.838 | PnL: -127.754, Risk: 294.083 | e: 0.089\n",
      "Episode: 24300 | Mean Objective: -455.073 | PnL: -139.047, Risk: 316.026 | e: 0.088\n",
      "Episode: 24400 | Mean Objective: -386.152 | PnL: -119.468, Risk: 266.684 | e: 0.087\n",
      "Episode: 24500 | Mean Objective: -485.389 | PnL: -156.195, Risk: 329.194 | e: 0.086\n",
      "Episode: 24600 | Mean Objective: -434.082 | PnL: -149.586, Risk: 284.496 | e: 0.085\n",
      "Episode: 24700 | Mean Objective: -394.192 | PnL: -117.259, Risk: 276.933 | e: 0.085\n",
      "Episode: 24800 | Mean Objective: -394.976 | PnL: -122.460, Risk: 272.516 | e: 0.084\n",
      "Episode: 24900 | Mean Objective: -482.227 | PnL: -141.458, Risk: 340.769 | e: 0.083\n",
      "Episode: 25000 | Mean Objective: -474.108 | PnL: -147.955, Risk: 326.153 | e: 0.082\n",
      "Episode: 25100 | Mean Objective: -474.991 | PnL: -184.842, Risk: 290.149 | e: 0.081\n",
      "Episode: 25200 | Mean Objective: -402.419 | PnL: -136.023, Risk: 266.397 | e: 0.080\n",
      "Episode: 25300 | Mean Objective: -435.366 | PnL: -146.736, Risk: 288.630 | e: 0.080\n",
      "Episode: 25400 | Mean Objective: -370.360 | PnL: -104.516, Risk: 265.844 | e: 0.079\n",
      "Episode: 25500 | Mean Objective: -402.863 | PnL: -146.038, Risk: 256.825 | e: 0.078\n",
      "Episode: 25600 | Mean Objective: -382.494 | PnL: -120.978, Risk: 261.515 | e: 0.077\n",
      "Episode: 25700 | Mean Objective: -349.492 | PnL: -80.239, Risk: 269.253 | e: 0.077\n",
      "Episode: 25800 | Mean Objective: -383.243 | PnL: -125.073, Risk: 258.170 | e: 0.076\n",
      "Episode: 25900 | Mean Objective: -394.133 | PnL: -121.604, Risk: 272.528 | e: 0.075\n",
      "Episode: 26000 | Mean Objective: -486.658 | PnL: -166.732, Risk: 319.926 | e: 0.074\n",
      "Episode: 26100 | Mean Objective: -370.609 | PnL: -89.764, Risk: 280.846 | e: 0.074\n",
      "Episode: 26200 | Mean Objective: -394.527 | PnL: -109.124, Risk: 285.402 | e: 0.073\n",
      "Episode: 26300 | Mean Objective: -465.843 | PnL: -141.983, Risk: 323.859 | e: 0.072\n",
      "Episode: 26400 | Mean Objective: -390.560 | PnL: -110.085, Risk: 280.475 | e: 0.071\n",
      "Episode: 26500 | Mean Objective: -395.295 | PnL: -91.636, Risk: 303.659 | e: 0.071\n",
      "Episode: 26600 | Mean Objective: -385.681 | PnL: -111.876, Risk: 273.805 | e: 0.070\n",
      "Episode: 26700 | Mean Objective: -437.633 | PnL: -145.335, Risk: 292.297 | e: 0.069\n",
      "Episode: 26800 | Mean Objective: -453.606 | PnL: -135.402, Risk: 318.204 | e: 0.069\n",
      "Episode: 26900 | Mean Objective: -407.301 | PnL: -129.707, Risk: 277.594 | e: 0.068\n",
      "Episode: 27000 | Mean Objective: -396.846 | PnL: -132.516, Risk: 264.329 | e: 0.067\n",
      "Episode: 27100 | Mean Objective: -396.887 | PnL: -111.809, Risk: 285.077 | e: 0.067\n",
      "Episode: 27200 | Mean Objective: -406.953 | PnL: -115.199, Risk: 291.754 | e: 0.066\n",
      "Episode: 27300 | Mean Objective: -429.502 | PnL: -138.075, Risk: 291.427 | e: 0.065\n",
      "Episode: 27400 | Mean Objective: -428.364 | PnL: -141.879, Risk: 286.485 | e: 0.065\n",
      "Episode: 27500 | Mean Objective: -430.034 | PnL: -112.058, Risk: 317.976 | e: 0.064\n",
      "Episode: 27600 | Mean Objective: -380.611 | PnL: -109.454, Risk: 271.158 | e: 0.063\n",
      "Episode: 27700 | Mean Objective: -414.871 | PnL: -117.745, Risk: 297.126 | e: 0.063\n",
      "Episode: 27800 | Mean Objective: -436.092 | PnL: -126.907, Risk: 309.185 | e: 0.062\n",
      "Episode: 27900 | Mean Objective: -424.945 | PnL: -132.590, Risk: 292.355 | e: 0.061\n",
      "Episode: 28000 | Mean Objective: -359.360 | PnL: -96.429, Risk: 262.931 | e: 0.061\n",
      "Episode: 28100 | Mean Objective: -347.133 | PnL: -83.187, Risk: 263.946 | e: 0.060\n",
      "Episode: 28200 | Mean Objective: -407.225 | PnL: -130.377, Risk: 276.848 | e: 0.060\n",
      "Episode: 28300 | Mean Objective: -423.882 | PnL: -126.281, Risk: 297.601 | e: 0.059\n",
      "Episode: 28400 | Mean Objective: -359.696 | PnL: -112.557, Risk: 247.139 | e: 0.058\n",
      "Episode: 28500 | Mean Objective: -376.257 | PnL: -98.523, Risk: 277.735 | e: 0.058\n",
      "Episode: 28600 | Mean Objective: -387.075 | PnL: -124.110, Risk: 262.965 | e: 0.057\n",
      "Episode: 28700 | Mean Objective: -410.063 | PnL: -127.993, Risk: 282.070 | e: 0.057\n",
      "Episode: 28800 | Mean Objective: -413.505 | PnL: -127.664, Risk: 285.840 | e: 0.056\n",
      "Episode: 28900 | Mean Objective: -419.553 | PnL: -147.508, Risk: 272.046 | e: 0.056\n",
      "Episode: 29000 | Mean Objective: -412.518 | PnL: -112.074, Risk: 300.444 | e: 0.055\n",
      "Episode: 29100 | Mean Objective: -358.128 | PnL: -106.296, Risk: 251.831 | e: 0.054\n",
      "Episode: 29200 | Mean Objective: -363.434 | PnL: -89.798, Risk: 273.636 | e: 0.054\n",
      "Episode: 29300 | Mean Objective: -418.408 | PnL: -110.864, Risk: 307.545 | e: 0.053\n",
      "Episode: 29400 | Mean Objective: -432.762 | PnL: -140.032, Risk: 292.730 | e: 0.053\n",
      "Episode: 29500 | Mean Objective: -463.082 | PnL: -134.922, Risk: 328.160 | e: 0.052\n",
      "Episode: 29600 | Mean Objective: -443.089 | PnL: -108.799, Risk: 334.291 | e: 0.052\n",
      "Episode: 29700 | Mean Objective: -429.472 | PnL: -127.022, Risk: 302.450 | e: 0.051\n",
      "Episode: 29800 | Mean Objective: -363.257 | PnL: -76.500, Risk: 286.756 | e: 0.051\n",
      "Episode: 29900 | Mean Objective: -327.781 | PnL: -73.813, Risk: 253.968 | e: 0.050\n",
      "Episode: 30000 | Mean Objective: -347.828 | PnL: -81.156, Risk: 266.673 | e: 0.050\n",
      "Episode: 30100 | Mean Objective: -436.814 | PnL: -107.911, Risk: 328.903 | e: 0.049\n",
      "Episode: 30200 | Mean Objective: -393.518 | PnL: -95.641, Risk: 297.877 | e: 0.049\n",
      "Episode: 30300 | Mean Objective: -395.718 | PnL: -133.383, Risk: 262.335 | e: 0.048\n",
      "Episode: 30400 | Mean Objective: -407.607 | PnL: -100.675, Risk: 306.931 | e: 0.048\n",
      "Episode: 30500 | Mean Objective: -441.025 | PnL: -138.574, Risk: 302.451 | e: 0.047\n",
      "Episode: 30600 | Mean Objective: -407.560 | PnL: -87.465, Risk: 320.095 | e: 0.047\n",
      "Episode: 30700 | Mean Objective: -444.980 | PnL: -121.722, Risk: 323.259 | e: 0.046\n",
      "Episode: 30800 | Mean Objective: -400.194 | PnL: -100.645, Risk: 299.549 | e: 0.046\n",
      "Episode: 30900 | Mean Objective: -382.785 | PnL: -78.788, Risk: 303.997 | e: 0.045\n",
      "Episode: 31000 | Mean Objective: -470.817 | PnL: -106.215, Risk: 364.602 | e: 0.045\n",
      "Episode: 31100 | Mean Objective: -287.148 | PnL: -51.913, Risk: 235.236 | e: 0.045\n",
      "Episode: 31200 | Mean Objective: -435.235 | PnL: -120.723, Risk: 314.512 | e: 0.044\n",
      "Episode: 31300 | Mean Objective: -445.381 | PnL: -117.345, Risk: 328.036 | e: 0.044\n",
      "Episode: 31400 | Mean Objective: -341.285 | PnL: -75.520, Risk: 265.765 | e: 0.043\n",
      "Episode: 31500 | Mean Objective: -355.763 | PnL: -88.676, Risk: 267.087 | e: 0.043\n",
      "Episode: 31600 | Mean Objective: -369.609 | PnL: -98.099, Risk: 271.510 | e: 0.042\n",
      "Episode: 31700 | Mean Objective: -340.708 | PnL: -78.597, Risk: 262.111 | e: 0.042\n",
      "Episode: 31800 | Mean Objective: -390.476 | PnL: -103.574, Risk: 286.902 | e: 0.042\n",
      "Episode: 31900 | Mean Objective: -462.097 | PnL: -124.021, Risk: 338.076 | e: 0.041\n",
      "Episode: 32000 | Mean Objective: -356.620 | PnL: -86.074, Risk: 270.545 | e: 0.041\n",
      "Episode: 32100 | Mean Objective: -405.553 | PnL: -105.728, Risk: 299.825 | e: 0.040\n",
      "Episode: 32200 | Mean Objective: -425.731 | PnL: -126.070, Risk: 299.661 | e: 0.040\n",
      "Episode: 32300 | Mean Objective: -368.166 | PnL: -92.117, Risk: 276.049 | e: 0.040\n",
      "Episode: 32400 | Mean Objective: -392.665 | PnL: -97.330, Risk: 295.335 | e: 0.039\n",
      "Episode: 32500 | Mean Objective: -388.207 | PnL: -105.155, Risk: 283.053 | e: 0.039\n",
      "Episode: 32600 | Mean Objective: -406.781 | PnL: -106.222, Risk: 300.558 | e: 0.038\n",
      "Episode: 32700 | Mean Objective: -382.394 | PnL: -112.151, Risk: 270.243 | e: 0.038\n",
      "Episode: 32800 | Mean Objective: -355.659 | PnL: -88.981, Risk: 266.678 | e: 0.038\n",
      "Episode: 32900 | Mean Objective: -344.961 | PnL: -78.756, Risk: 266.205 | e: 0.037\n",
      "Episode: 33000 | Mean Objective: -361.392 | PnL: -71.968, Risk: 289.424 | e: 0.037\n",
      "Episode: 33100 | Mean Objective: -413.115 | PnL: -119.314, Risk: 293.801 | e: 0.037\n",
      "Episode: 33200 | Mean Objective: -360.938 | PnL: -85.248, Risk: 275.690 | e: 0.036\n",
      "Episode: 33300 | Mean Objective: -381.971 | PnL: -99.736, Risk: 282.235 | e: 0.036\n",
      "Episode: 33400 | Mean Objective: -379.796 | PnL: -85.803, Risk: 293.993 | e: 0.035\n",
      "Episode: 33500 | Mean Objective: -395.600 | PnL: -110.259, Risk: 285.341 | e: 0.035\n",
      "Episode: 33600 | Mean Objective: -382.484 | PnL: -89.206, Risk: 293.278 | e: 0.035\n",
      "Episode: 33700 | Mean Objective: -397.630 | PnL: -98.739, Risk: 298.891 | e: 0.034\n",
      "Episode: 33800 | Mean Objective: -428.624 | PnL: -103.262, Risk: 325.363 | e: 0.034\n",
      "Episode: 33900 | Mean Objective: -425.247 | PnL: -112.833, Risk: 312.415 | e: 0.034\n",
      "Episode: 34000 | Mean Objective: -317.238 | PnL: -89.744, Risk: 227.494 | e: 0.033\n",
      "Episode: 34100 | Mean Objective: -363.897 | PnL: -79.802, Risk: 284.094 | e: 0.033\n",
      "Episode: 34200 | Mean Objective: -409.362 | PnL: -116.485, Risk: 292.877 | e: 0.033\n",
      "Episode: 34300 | Mean Objective: -420.566 | PnL: -98.524, Risk: 322.042 | e: 0.032\n",
      "Episode: 34400 | Mean Objective: -409.191 | PnL: -113.276, Risk: 295.915 | e: 0.032\n",
      "Episode: 34500 | Mean Objective: -440.623 | PnL: -108.775, Risk: 331.849 | e: 0.032\n",
      "Episode: 34600 | Mean Objective: -337.702 | PnL: -71.444, Risk: 266.258 | e: 0.031\n",
      "Episode: 34700 | Mean Objective: -424.927 | PnL: -107.418, Risk: 317.509 | e: 0.031\n",
      "Episode: 34800 | Mean Objective: -363.051 | PnL: -81.907, Risk: 281.144 | e: 0.031\n",
      "Episode: 34900 | Mean Objective: -389.418 | PnL: -89.801, Risk: 299.617 | e: 0.030\n",
      "Episode: 35000 | Mean Objective: -443.601 | PnL: -115.186, Risk: 328.415 | e: 0.030\n",
      "Episode: 35100 | Mean Objective: -444.977 | PnL: -98.470, Risk: 346.507 | e: 0.030\n",
      "Episode: 35200 | Mean Objective: -399.221 | PnL: -84.853, Risk: 314.368 | e: 0.030\n",
      "Episode: 35300 | Mean Objective: -382.895 | PnL: -82.895, Risk: 300.000 | e: 0.029\n",
      "Episode: 35400 | Mean Objective: -395.107 | PnL: -105.508, Risk: 289.599 | e: 0.029\n",
      "Episode: 35500 | Mean Objective: -379.327 | PnL: -82.102, Risk: 297.225 | e: 0.029\n",
      "Episode: 35600 | Mean Objective: -321.894 | PnL: -56.605, Risk: 265.289 | e: 0.028\n",
      "Episode: 35700 | Mean Objective: -417.201 | PnL: -103.919, Risk: 313.282 | e: 0.028\n",
      "Episode: 35800 | Mean Objective: -332.777 | PnL: -63.088, Risk: 269.689 | e: 0.028\n",
      "Episode: 35900 | Mean Objective: -339.363 | PnL: -64.773, Risk: 274.590 | e: 0.028\n",
      "Episode: 36000 | Mean Objective: -349.893 | PnL: -63.060, Risk: 286.834 | e: 0.027\n",
      "Episode: 36100 | Mean Objective: -325.828 | PnL: -79.755, Risk: 246.073 | e: 0.027\n",
      "Episode: 36200 | Mean Objective: -375.636 | PnL: -83.369, Risk: 292.267 | e: 0.027\n",
      "Episode: 36300 | Mean Objective: -447.112 | PnL: -114.465, Risk: 332.646 | e: 0.027\n",
      "Episode: 36400 | Mean Objective: -403.116 | PnL: -90.034, Risk: 313.083 | e: 0.026\n",
      "Episode: 36500 | Mean Objective: -401.372 | PnL: -105.095, Risk: 296.277 | e: 0.026\n",
      "Episode: 36600 | Mean Objective: -309.622 | PnL: -59.763, Risk: 249.859 | e: 0.026\n",
      "Episode: 36700 | Mean Objective: -310.497 | PnL: -60.097, Risk: 250.400 | e: 0.025\n",
      "Episode: 36800 | Mean Objective: -349.142 | PnL: -75.994, Risk: 273.147 | e: 0.025\n",
      "Episode: 36900 | Mean Objective: -416.009 | PnL: -115.432, Risk: 300.578 | e: 0.025\n",
      "Episode: 37000 | Mean Objective: -346.207 | PnL: -79.884, Risk: 266.322 | e: 0.025\n",
      "Episode: 37100 | Mean Objective: -376.699 | PnL: -99.909, Risk: 276.790 | e: 0.024\n",
      "Episode: 37200 | Mean Objective: -324.705 | PnL: -50.446, Risk: 274.259 | e: 0.024\n",
      "Episode: 37300 | Mean Objective: -422.826 | PnL: -95.029, Risk: 327.797 | e: 0.024\n",
      "Episode: 37400 | Mean Objective: -362.665 | PnL: -86.087, Risk: 276.578 | e: 0.024\n",
      "Episode: 37500 | Mean Objective: -373.936 | PnL: -80.512, Risk: 293.424 | e: 0.024\n",
      "Episode: 37600 | Mean Objective: -365.605 | PnL: -84.665, Risk: 280.940 | e: 0.023\n",
      "Episode: 37700 | Mean Objective: -385.400 | PnL: -95.570, Risk: 289.831 | e: 0.023\n",
      "Episode: 37800 | Mean Objective: -394.842 | PnL: -122.563, Risk: 272.279 | e: 0.023\n",
      "Episode: 37900 | Mean Objective: -424.014 | PnL: -112.938, Risk: 311.076 | e: 0.023\n",
      "Episode: 38000 | Mean Objective: -349.985 | PnL: -69.185, Risk: 280.800 | e: 0.022\n",
      "Episode: 38100 | Mean Objective: -339.341 | PnL: -86.167, Risk: 253.175 | e: 0.022\n",
      "Episode: 38200 | Mean Objective: -343.145 | PnL: -72.624, Risk: 270.520 | e: 0.022\n",
      "Episode: 38300 | Mean Objective: -313.405 | PnL: -58.156, Risk: 255.249 | e: 0.022\n",
      "Episode: 38400 | Mean Objective: -374.532 | PnL: -71.424, Risk: 303.107 | e: 0.021\n",
      "Episode: 38500 | Mean Objective: -343.132 | PnL: -51.065, Risk: 292.067 | e: 0.021\n",
      "Episode: 38600 | Mean Objective: -379.692 | PnL: -75.725, Risk: 303.966 | e: 0.021\n",
      "Episode: 38700 | Mean Objective: -352.961 | PnL: -67.288, Risk: 285.673 | e: 0.021\n",
      "Episode: 38800 | Mean Objective: -374.377 | PnL: -77.811, Risk: 296.567 | e: 0.021\n",
      "Episode: 38900 | Mean Objective: -370.016 | PnL: -69.141, Risk: 300.875 | e: 0.020\n",
      "Episode: 39000 | Mean Objective: -363.258 | PnL: -69.110, Risk: 294.148 | e: 0.020\n",
      "Episode: 39100 | Mean Objective: -341.051 | PnL: -61.409, Risk: 279.642 | e: 0.020\n",
      "Episode: 39200 | Mean Objective: -452.191 | PnL: -101.335, Risk: 350.856 | e: 0.020\n",
      "Episode: 39300 | Mean Objective: -364.553 | PnL: -96.531, Risk: 268.023 | e: 0.020\n",
      "Episode: 39400 | Mean Objective: -304.021 | PnL: -58.161, Risk: 245.860 | e: 0.019\n",
      "Episode: 39500 | Mean Objective: -361.571 | PnL: -87.567, Risk: 274.004 | e: 0.019\n",
      "Episode: 39600 | Mean Objective: -393.477 | PnL: -81.530, Risk: 311.947 | e: 0.019\n",
      "Episode: 39700 | Mean Objective: -366.768 | PnL: -86.373, Risk: 280.395 | e: 0.019\n",
      "Episode: 39800 | Mean Objective: -377.972 | PnL: -94.497, Risk: 283.475 | e: 0.019\n",
      "Episode: 39900 | Mean Objective: -397.791 | PnL: -99.298, Risk: 298.493 | e: 0.018\n",
      "Episode: 40000 | Mean Objective: -383.734 | PnL: -70.566, Risk: 313.169 | e: 0.018\n",
      "Episode: 40100 | Mean Objective: -349.279 | PnL: -90.377, Risk: 258.902 | e: 0.018\n",
      "Episode: 40200 | Mean Objective: -393.197 | PnL: -80.091, Risk: 313.106 | e: 0.018\n",
      "Episode: 40300 | Mean Objective: -399.830 | PnL: -101.417, Risk: 298.413 | e: 0.018\n",
      "Episode: 40400 | Mean Objective: -332.147 | PnL: -86.865, Risk: 245.282 | e: 0.018\n",
      "Episode: 40500 | Mean Objective: -367.164 | PnL: -86.921, Risk: 280.243 | e: 0.017\n",
      "Episode: 40600 | Mean Objective: -400.184 | PnL: -102.424, Risk: 297.759 | e: 0.017\n",
      "Episode: 40700 | Mean Objective: -340.473 | PnL: -73.437, Risk: 267.036 | e: 0.017\n",
      "Episode: 40800 | Mean Objective: -297.320 | PnL: -54.005, Risk: 243.315 | e: 0.017\n",
      "Episode: 40900 | Mean Objective: -417.151 | PnL: -89.948, Risk: 327.203 | e: 0.017\n",
      "Episode: 41000 | Mean Objective: -369.843 | PnL: -75.666, Risk: 294.177 | e: 0.017\n",
      "Episode: 41100 | Mean Objective: -345.329 | PnL: -90.520, Risk: 254.809 | e: 0.016\n",
      "Episode: 41200 | Mean Objective: -375.620 | PnL: -89.342, Risk: 286.278 | e: 0.016\n",
      "Episode: 41300 | Mean Objective: -414.382 | PnL: -106.774, Risk: 307.608 | e: 0.016\n",
      "Episode: 41400 | Mean Objective: -306.626 | PnL: -49.505, Risk: 257.120 | e: 0.016\n",
      "Episode: 41500 | Mean Objective: -364.301 | PnL: -78.620, Risk: 285.681 | e: 0.016\n",
      "Episode: 41600 | Mean Objective: -313.973 | PnL: -57.957, Risk: 256.016 | e: 0.016\n",
      "Episode: 41700 | Mean Objective: -350.637 | PnL: -97.705, Risk: 252.932 | e: 0.015\n",
      "Episode: 41800 | Mean Objective: -387.379 | PnL: -89.476, Risk: 297.903 | e: 0.015\n",
      "Episode: 41900 | Mean Objective: -348.918 | PnL: -75.922, Risk: 272.995 | e: 0.015\n",
      "Episode: 42000 | Mean Objective: -261.985 | PnL: -31.067, Risk: 230.919 | e: 0.015\n",
      "Episode: 42100 | Mean Objective: -368.709 | PnL: -68.548, Risk: 300.161 | e: 0.015\n",
      "Episode: 42200 | Mean Objective: -326.414 | PnL: -75.649, Risk: 250.766 | e: 0.015\n",
      "Episode: 42300 | Mean Objective: -364.009 | PnL: -81.574, Risk: 282.435 | e: 0.015\n",
      "Episode: 42400 | Mean Objective: -339.728 | PnL: -98.293, Risk: 241.435 | e: 0.014\n",
      "Episode: 42500 | Mean Objective: -381.319 | PnL: -80.112, Risk: 301.207 | e: 0.014\n",
      "Episode: 42600 | Mean Objective: -308.237 | PnL: -59.136, Risk: 249.101 | e: 0.014\n",
      "Episode: 42700 | Mean Objective: -362.140 | PnL: -80.174, Risk: 281.967 | e: 0.014\n",
      "Episode: 42800 | Mean Objective: -327.739 | PnL: -65.711, Risk: 262.028 | e: 0.014\n",
      "Episode: 42900 | Mean Objective: -348.234 | PnL: -87.735, Risk: 260.500 | e: 0.014\n",
      "Episode: 43000 | Mean Objective: -324.833 | PnL: -58.824, Risk: 266.009 | e: 0.014\n",
      "Episode: 43100 | Mean Objective: -295.652 | PnL: -50.859, Risk: 244.793 | e: 0.013\n",
      "Episode: 43200 | Mean Objective: -344.629 | PnL: -72.895, Risk: 271.734 | e: 0.013\n",
      "Episode: 43300 | Mean Objective: -318.726 | PnL: -68.216, Risk: 250.510 | e: 0.013\n",
      "Episode: 43400 | Mean Objective: -356.487 | PnL: -79.210, Risk: 277.277 | e: 0.013\n",
      "Episode: 43500 | Mean Objective: -409.783 | PnL: -100.371, Risk: 309.412 | e: 0.013\n",
      "Episode: 43600 | Mean Objective: -307.880 | PnL: -56.153, Risk: 251.727 | e: 0.013\n",
      "Episode: 43700 | Mean Objective: -363.167 | PnL: -73.927, Risk: 289.240 | e: 0.013\n",
      "Episode: 43800 | Mean Objective: -328.359 | PnL: -54.528, Risk: 273.831 | e: 0.013\n",
      "Episode: 43900 | Mean Objective: -351.184 | PnL: -60.037, Risk: 291.147 | e: 0.012\n",
      "Episode: 44000 | Mean Objective: -301.912 | PnL: -75.499, Risk: 226.413 | e: 0.012\n",
      "Episode: 44100 | Mean Objective: -405.175 | PnL: -97.056, Risk: 308.119 | e: 0.012\n",
      "Episode: 44200 | Mean Objective: -328.777 | PnL: -71.536, Risk: 257.241 | e: 0.012\n",
      "Episode: 44300 | Mean Objective: -323.258 | PnL: -51.967, Risk: 271.291 | e: 0.012\n",
      "Episode: 44400 | Mean Objective: -293.477 | PnL: -45.763, Risk: 247.714 | e: 0.012\n",
      "Episode: 44500 | Mean Objective: -295.161 | PnL: -49.482, Risk: 245.679 | e: 0.012\n",
      "Episode: 44600 | Mean Objective: -383.028 | PnL: -95.783, Risk: 287.244 | e: 0.012\n",
      "Episode: 44700 | Mean Objective: -315.395 | PnL: -56.192, Risk: 259.203 | e: 0.011\n",
      "Episode: 44800 | Mean Objective: -366.321 | PnL: -100.407, Risk: 265.913 | e: 0.011\n",
      "Episode: 44900 | Mean Objective: -319.790 | PnL: -62.097, Risk: 257.693 | e: 0.011\n",
      "Episode: 45000 | Mean Objective: -311.066 | PnL: -38.567, Risk: 272.499 | e: 0.011\n",
      "Episode: 45100 | Mean Objective: -350.684 | PnL: -61.769, Risk: 288.915 | e: 0.011\n",
      "Episode: 45200 | Mean Objective: -306.035 | PnL: -56.841, Risk: 249.194 | e: 0.011\n",
      "Episode: 45300 | Mean Objective: -328.155 | PnL: -74.039, Risk: 254.116 | e: 0.011\n",
      "Episode: 45400 | Mean Objective: -355.245 | PnL: -81.155, Risk: 274.090 | e: 0.011\n",
      "Episode: 45500 | Mean Objective: -296.009 | PnL: -52.330, Risk: 243.679 | e: 0.011\n",
      "Episode: 45600 | Mean Objective: -326.264 | PnL: -55.630, Risk: 270.634 | e: 0.010\n",
      "Episode: 45700 | Mean Objective: -362.679 | PnL: -90.178, Risk: 272.501 | e: 0.010\n",
      "Episode: 45800 | Mean Objective: -337.725 | PnL: -44.795, Risk: 292.930 | e: 0.010\n",
      "Episode: 45900 | Mean Objective: -307.442 | PnL: -59.238, Risk: 248.204 | e: 0.010\n",
      "Episode: 46000 | Mean Objective: -350.707 | PnL: -64.257, Risk: 286.450 | e: 0.010\n",
      "Episode: 46100 | Mean Objective: -339.042 | PnL: -80.858, Risk: 258.184 | e: 0.010\n",
      "Episode: 46200 | Mean Objective: -333.611 | PnL: -71.521, Risk: 262.090 | e: 0.010\n",
      "Episode: 46300 | Mean Objective: -355.985 | PnL: -73.837, Risk: 282.149 | e: 0.010\n",
      "Episode: 46400 | Mean Objective: -298.755 | PnL: -37.183, Risk: 261.572 | e: 0.010\n",
      "Episode: 46500 | Mean Objective: -333.090 | PnL: -55.667, Risk: 277.423 | e: 0.010\n",
      "Episode: 46600 | Mean Objective: -378.565 | PnL: -79.499, Risk: 299.066 | e: 0.009\n",
      "Episode: 46700 | Mean Objective: -281.991 | PnL: -47.614, Risk: 234.377 | e: 0.009\n",
      "Episode: 46800 | Mean Objective: -267.633 | PnL: -53.654, Risk: 213.979 | e: 0.009\n",
      "Episode: 46900 | Mean Objective: -364.199 | PnL: -86.440, Risk: 277.760 | e: 0.009\n",
      "Episode: 47000 | Mean Objective: -304.390 | PnL: -46.722, Risk: 257.668 | e: 0.009\n",
      "Episode: 47100 | Mean Objective: -362.289 | PnL: -86.004, Risk: 276.285 | e: 0.009\n",
      "Episode: 47200 | Mean Objective: -323.153 | PnL: -64.349, Risk: 258.805 | e: 0.009\n",
      "Episode: 47300 | Mean Objective: -349.423 | PnL: -53.841, Risk: 295.583 | e: 0.009\n",
      "Episode: 47400 | Mean Objective: -341.820 | PnL: -77.549, Risk: 264.272 | e: 0.009\n",
      "Episode: 47500 | Mean Objective: -319.087 | PnL: -49.034, Risk: 270.053 | e: 0.009\n",
      "Episode: 47600 | Mean Objective: -309.116 | PnL: -78.612, Risk: 230.504 | e: 0.009\n",
      "Episode: 47700 | Mean Objective: -341.987 | PnL: -62.268, Risk: 279.719 | e: 0.008\n",
      "Episode: 47800 | Mean Objective: -326.832 | PnL: -58.740, Risk: 268.093 | e: 0.008\n",
      "Episode: 47900 | Mean Objective: -270.347 | PnL: -14.810, Risk: 255.538 | e: 0.008\n",
      "Episode: 48000 | Mean Objective: -336.534 | PnL: -71.827, Risk: 264.707 | e: 0.008\n",
      "Episode: 48100 | Mean Objective: -310.715 | PnL: -44.288, Risk: 266.428 | e: 0.008\n",
      "Episode: 48200 | Mean Objective: -329.734 | PnL: -63.231, Risk: 266.502 | e: 0.008\n",
      "Episode: 48300 | Mean Objective: -308.910 | PnL: -44.194, Risk: 264.716 | e: 0.008\n",
      "Episode: 48400 | Mean Objective: -296.642 | PnL: -51.236, Risk: 245.406 | e: 0.008\n",
      "Episode: 48500 | Mean Objective: -303.286 | PnL: -53.813, Risk: 249.473 | e: 0.008\n",
      "Episode: 48600 | Mean Objective: -377.808 | PnL: -75.538, Risk: 302.270 | e: 0.008\n",
      "Episode: 48700 | Mean Objective: -287.619 | PnL: -32.636, Risk: 254.983 | e: 0.008\n",
      "Episode: 48800 | Mean Objective: -320.559 | PnL: -54.746, Risk: 265.814 | e: 0.008\n",
      "Episode: 48900 | Mean Objective: -316.738 | PnL: -66.605, Risk: 250.134 | e: 0.008\n",
      "Episode: 49000 | Mean Objective: -312.524 | PnL: -50.034, Risk: 262.490 | e: 0.007\n",
      "Episode: 49100 | Mean Objective: -327.809 | PnL: -56.875, Risk: 270.934 | e: 0.007\n",
      "Episode: 49200 | Mean Objective: -302.507 | PnL: -55.851, Risk: 246.656 | e: 0.007\n",
      "Episode: 49300 | Mean Objective: -378.512 | PnL: -83.540, Risk: 294.972 | e: 0.007\n",
      "Episode: 49400 | Mean Objective: -290.818 | PnL: -37.358, Risk: 253.459 | e: 0.007\n",
      "Episode: 49500 | Mean Objective: -382.424 | PnL: -94.489, Risk: 287.935 | e: 0.007\n",
      "Episode: 49600 | Mean Objective: -381.086 | PnL: -75.847, Risk: 305.239 | e: 0.007\n",
      "Episode: 49700 | Mean Objective: -343.217 | PnL: -57.650, Risk: 285.566 | e: 0.007\n",
      "Episode: 49800 | Mean Objective: -307.305 | PnL: -37.638, Risk: 269.667 | e: 0.007\n",
      "Episode: 49900 | Mean Objective: -336.358 | PnL: -67.328, Risk: 269.030 | e: 0.007\n",
      "Episode: 50000 | Mean Objective: -310.875 | PnL: -57.147, Risk: 253.728 | e: 0.007\n",
      "Episode: 50001 | Mean Objective: -323.790 | e: 0.007\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "cold_actor = Actor(4,1)\n",
    "cold_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "cold_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# train\n",
    "cold_obj_hist, _ = DDPG_train(cold_actor,\n",
    "                            cold_cost_critic,\n",
    "                            cold_risk_critic,\n",
    "                            env,\n",
    "                            episodes= 50000,\n",
    "                            batch_size = 128,\n",
    "                            lr = 0.0001,\n",
    "                            tau=0.00005,\n",
    "                            epsilon = 1,\n",
    "                            epsilon_decay = 0.9999,\n",
    "                            discount = 1,\n",
    "                            eval_freq = 100,\n",
    "                            min_epsilon = 0.0,\n",
    "                            noisy_exploration = False,\n",
    "                            inital_buffer = 1000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "712be770-feb3-435a-b4f8-348d91abd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cold_actor, f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_cost_critic, f'cold_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_risk_critic, f'cold_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'cold_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', cold_obj_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b36c-7bdb-4a69-9159-41eab4f7f0a9",
   "metadata": {},
   "source": [
    "#### Hot-Start Model:\n",
    "1. Generate Training Data\n",
    "2. Hot start actor\n",
    "3. Hot start critic (still not very good)\n",
    "4. Do DDPG on pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04fc6a52-0727-4db0-af81-2a240768279e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samples for hot start...\n",
      "Training Actor...\n",
      "Epoch: 1, Batch: 164/164 | Loss: 585.1235\n",
      "Epoch: 11, Batch: 164/164 | Loss: 168.440\n",
      "Epoch: 21, Batch: 164/164 | Loss: 166.432\n",
      "Epoch: 31, Batch: 164/164 | Loss: 144.622\n",
      "Epoch: 41, Batch: 164/164 | Loss: 85.6487\n",
      "Epoch: 51, Batch: 164/164 | Loss: 80.9184\n",
      "Epoch: 61, Batch: 164/164 | Loss: 83.5688\n",
      "Epoch: 71, Batch: 164/164 | Loss: 85.5526\n",
      "Epoch: 81, Batch: 164/164 | Loss: 96.5416\n",
      "Epoch: 91, Batch: 164/164 | Loss: 102.780\n",
      "Epoch: 101, Batch: 164/164 | Loss: 67.0517\n",
      "Epoch: 111, Batch: 164/164 | Loss: 37.3928\n",
      "Epoch: 121, Batch: 164/164 | Loss: 29.3563\n",
      "Epoch: 131, Batch: 164/164 | Loss: 29.2694\n",
      "Epoch: 141, Batch: 164/164 | Loss: 29.0160\n",
      "Epoch: 151, Batch: 164/164 | Loss: 23.2487\n",
      "Epoch: 161, Batch: 164/164 | Loss: 23.9586\n",
      "Epoch: 171, Batch: 164/164 | Loss: 29.6952\n",
      "Epoch: 181, Batch: 164/164 | Loss: 32.010\n",
      "Epoch: 191, Batch: 164/164 | Loss: 17.5691\n",
      "Epoch: 201, Batch: 164/164 | Loss: 17.338\n",
      "Epoch: 211, Batch: 164/164 | Loss: 26.388\n",
      "Epoch: 221, Batch: 164/164 | Loss: 40.726\n",
      "Epoch: 231, Batch: 164/164 | Loss: 44.740\n",
      "Epoch: 241, Batch: 164/164 | Loss: 26.674\n",
      "Epoch: 251, Batch: 164/164 | Loss: 18.9498\n",
      "Epoch: 261, Batch: 164/164 | Loss: 37.0395\n",
      "Epoch: 271, Batch: 164/164 | Loss: 28.412\n",
      "Epoch: 281, Batch: 164/164 | Loss: 15.8756\n",
      "Epoch: 291, Batch: 164/164 | Loss: 12.4372\n",
      "Epoch: 300, Batch: 164/164 | Loss: 15.2185\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "hot_actor = Actor(4,1)\n",
    "hot_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "hot_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# Generate data\n",
    "X, y = hot_start_gen_actor_samples(env,n_paths = 1000)\n",
    "\n",
    "# BC Train actor\n",
    "hot_actor, loss_hist = hot_start_actor(hot_actor, X, y, lr=0.001, batch_size=128, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44b1fac9-03d5-4b7e-8042-9d10cfe5e472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -2052.429 | Guess: -177.497, Diff: 1874.933 | e: 55.942\n",
      "Episode: 200 | Mean Objective: -2053.041 | Guess: -246.102, Diff: 1806.940 | e: 52.159\n",
      "Episode: 300 | Mean Objective: -1890.845 | Guess: -301.025, Diff: 1589.820 | e: 48.631\n",
      "Episode: 400 | Mean Objective: -1821.772 | Guess: -351.681, Diff: 1470.091 | e: 45.343\n",
      "Episode: 500 | Mean Objective: -1773.828 | Guess: -409.134, Diff: 1364.694 | e: 42.276\n",
      "Episode: 600 | Mean Objective: -1671.061 | Guess: -460.460, Diff: 1210.601 | e: 39.417\n",
      "Episode: 700 | Mean Objective: -1550.176 | Guess: -504.756, Diff: 1045.420 | e: 36.751\n",
      "Episode: 800 | Mean Objective: -1455.615 | Guess: -545.348, Diff: 910.267 | e: 34.2667\n",
      "Episode: 900 | Mean Objective: -1335.399 | Guess: -585.998, Diff: 749.401 | e: 31.948\n",
      "Episode: 1000 | Mean Objective: -1243.743 | Guess: -627.285, Diff: 616.458 | e: 29.788\n",
      "Episode: 1100 | Mean Objective: -1098.540 | Guess: -662.202, Diff: 436.338 | e: 27.773\n",
      "Episode: 1200 | Mean Objective: -1166.970 | Guess: -692.317, Diff: 474.653 | e: 25.895\n",
      "Episode: 1300 | Mean Objective: -1112.559 | Guess: -720.989, Diff: 391.569 | e: 24.144\n",
      "Episode: 1400 | Mean Objective: -1068.942 | Guess: -750.592, Diff: 318.350 | e: 22.511\n",
      "Episode: 1500 | Mean Objective: -994.274 | Guess: -775.385, Diff: 218.889 | e: 20.9892\n",
      "Episode: 1600 | Mean Objective: -937.039 | Guess: -791.943, Diff: 145.096 | e: 19.5693\n",
      "Episode: 1700 | Mean Objective: -1002.553 | Guess: -789.926, Diff: 212.627 | e: 18.246\n",
      "Episode: 1800 | Mean Objective: -895.145 | Guess: -769.198, Diff: 125.947 | e: 17.0125\n",
      "Episode: 1900 | Mean Objective: -922.853 | Guess: -747.798, Diff: 175.055 | e: 15.861\n",
      "Episode: 2000 | Mean Objective: -799.140 | Guess: -730.422, Diff: 68.719 | e: 14.7893\n",
      "Episode: 2100 | Mean Objective: -795.994 | Guess: -728.582, Diff: 67.411 | e: 13.7882\n",
      "Episode: 2200 | Mean Objective: -695.532 | Guess: -729.763, Diff: -34.230 | e: 12.856\n",
      "Episode: 2300 | Mean Objective: -710.883 | Guess: -767.736, Diff: -56.853 | e: 11.986\n",
      "Episode: 2400 | Mean Objective: -697.361 | Guess: -808.614, Diff: -111.253 | e: 11.176\n",
      "Episode: 2500 | Mean Objective: -732.534 | Guess: -837.558, Diff: -105.023 | e: 10.420\n",
      "Episode: 2600 | Mean Objective: -636.446 | Guess: -865.071, Diff: -228.626 | e: 10.000\n",
      "Episode: 2700 | Mean Objective: -685.403 | Guess: -886.982, Diff: -201.579 | e: 10.000\n",
      "Episode: 2800 | Mean Objective: -597.400 | Guess: -907.751, Diff: -310.352 | e: 10.000\n",
      "Episode: 2900 | Mean Objective: -628.482 | Guess: -928.079, Diff: -299.597 | e: 10.000\n",
      "Episode: 3000 | Mean Objective: -593.159 | Guess: -943.740, Diff: -350.582 | e: 10.000\n",
      "Episode: 3100 | Mean Objective: -617.753 | Guess: -952.151, Diff: -334.398 | e: 10.000\n",
      "Episode: 3200 | Mean Objective: -669.025 | Guess: -948.714, Diff: -279.689 | e: 10.000\n",
      "Episode: 3300 | Mean Objective: -595.144 | Guess: -931.902, Diff: -336.757 | e: 10.000\n",
      "Episode: 3400 | Mean Objective: -646.248 | Guess: -903.315, Diff: -257.067 | e: 10.000\n",
      "Episode: 3500 | Mean Objective: -642.847 | Guess: -868.752, Diff: -225.906 | e: 10.000\n",
      "Episode: 3600 | Mean Objective: -599.185 | Guess: -830.702, Diff: -231.517 | e: 10.000\n",
      "Episode: 3700 | Mean Objective: -584.667 | Guess: -786.799, Diff: -202.132 | e: 10.000\n",
      "Episode: 3800 | Mean Objective: -631.646 | Guess: -738.293, Diff: -106.648 | e: 10.000\n",
      "Episode: 3900 | Mean Objective: -598.285 | Guess: -685.395, Diff: -87.110 | e: 10.0000\n",
      "Episode: 4000 | Mean Objective: -760.823 | Guess: -629.082, Diff: 131.741 | e: 10.0000\n",
      "Episode: 4100 | Mean Objective: -665.422 | Guess: -562.649, Diff: 102.773 | e: 10.000\n",
      "Episode: 4200 | Mean Objective: -688.161 | Guess: -495.270, Diff: 192.891 | e: 10.000\n",
      "Episode: 4300 | Mean Objective: -671.741 | Guess: -435.216, Diff: 236.525 | e: 10.000\n",
      "Episode: 4400 | Mean Objective: -580.057 | Guess: -407.402, Diff: 172.655 | e: 10.000\n",
      "Episode: 4500 | Mean Objective: -660.251 | Guess: -393.262, Diff: 266.989 | e: 10.000\n",
      "Episode: 4600 | Mean Objective: -561.948 | Guess: -382.988, Diff: 178.960 | e: 10.000\n",
      "Episode: 4700 | Mean Objective: -631.215 | Guess: -371.476, Diff: 259.739 | e: 10.000\n",
      "Episode: 4800 | Mean Objective: -690.023 | Guess: -369.538, Diff: 320.484 | e: 10.000\n",
      "Episode: 4900 | Mean Objective: -584.149 | Guess: -363.629, Diff: 220.519 | e: 10.000\n",
      "Episode: 5000 | Mean Objective: -667.116 | Guess: -363.901, Diff: 303.215 | e: 10.000\n",
      "Episode: 5001 | Mean Objective: -666.380 | Guess: -364.426, Diff: 301.955 | e: 10.000\r"
     ]
    }
   ],
   "source": [
    "# Hot start critic\n",
    "hot_obj, hot_q_guess = hot_start_critic_q_func(hot_actor,\n",
    "                                            hot_cost_critic,\n",
    "                                            hot_risk_critic,\n",
    "                                            env,\n",
    "                                            episodes= 5000,\n",
    "                                            batch_size = 128,\n",
    "                                            lr = 0.0005,\n",
    "                                            tau=0.001,\n",
    "                                            epsilon = 60.0,\n",
    "                                            epsilon_decay = 0.9993,\n",
    "                                            discount = 1.0,\n",
    "                                            eval_freq = 100,\n",
    "                                            min_epsilon = 10.0,\n",
    "                                            noisy_exploration = True\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f88e31c6-0a46-4135-a0d2-1f15e7a07c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -562.257 | PnL: -312.328, Risk: 249.928 | e: 9.802\n",
      "Episode: 200 | Mean Objective: -584.218 | PnL: -299.826, Risk: 284.392 | e: 9.608\n",
      "Episode: 300 | Mean Objective: -641.892 | PnL: -335.275, Risk: 306.617 | e: 9.418\n",
      "Episode: 400 | Mean Objective: -607.761 | PnL: -322.130, Risk: 285.631 | e: 9.231\n",
      "Episode: 500 | Mean Objective: -574.870 | PnL: -325.943, Risk: 248.926 | e: 9.048\n",
      "Episode: 600 | Mean Objective: -595.143 | PnL: -288.715, Risk: 306.429 | e: 8.869\n",
      "Episode: 700 | Mean Objective: -514.892 | PnL: -295.794, Risk: 219.098 | e: 8.693\n",
      "Episode: 800 | Mean Objective: -570.746 | PnL: -282.732, Risk: 288.015 | e: 8.521\n",
      "Episode: 900 | Mean Objective: -643.795 | PnL: -326.596, Risk: 317.199 | e: 8.353\n",
      "Episode: 1000 | Mean Objective: -612.937 | PnL: -322.420, Risk: 290.517 | e: 8.187\n",
      "Episode: 1100 | Mean Objective: -713.236 | PnL: -320.611, Risk: 392.625 | e: 8.025\n",
      "Episode: 1200 | Mean Objective: -771.052 | PnL: -315.875, Risk: 455.177 | e: 7.866\n",
      "Episode: 1300 | Mean Objective: -678.064 | PnL: -277.079, Risk: 400.985 | e: 7.710\n",
      "Episode: 1400 | Mean Objective: -661.639 | PnL: -246.441, Risk: 415.197 | e: 7.558\n",
      "Episode: 1500 | Mean Objective: -646.182 | PnL: -267.818, Risk: 378.365 | e: 7.408\n",
      "Episode: 1600 | Mean Objective: -731.679 | PnL: -288.577, Risk: 443.102 | e: 7.261\n",
      "Episode: 1700 | Mean Objective: -693.498 | PnL: -258.755, Risk: 434.743 | e: 7.117\n",
      "Episode: 1800 | Mean Objective: -592.148 | PnL: -229.806, Risk: 362.342 | e: 6.977\n",
      "Episode: 1900 | Mean Objective: -666.794 | PnL: -248.998, Risk: 417.796 | e: 6.838\n",
      "Episode: 2000 | Mean Objective: -724.878 | PnL: -286.413, Risk: 438.465 | e: 6.703\n",
      "Episode: 2100 | Mean Objective: -648.047 | PnL: -240.847, Risk: 407.200 | e: 6.570\n",
      "Episode: 2200 | Mean Objective: -655.071 | PnL: -220.623, Risk: 434.448 | e: 6.440\n",
      "Episode: 2300 | Mean Objective: -610.499 | PnL: -229.248, Risk: 381.250 | e: 6.313\n",
      "Episode: 2400 | Mean Objective: -591.821 | PnL: -231.976, Risk: 359.845 | e: 6.188\n",
      "Episode: 2500 | Mean Objective: -632.203 | PnL: -214.850, Risk: 417.353 | e: 6.065\n",
      "Episode: 2600 | Mean Objective: -637.837 | PnL: -201.555, Risk: 436.282 | e: 5.945\n",
      "Episode: 2700 | Mean Objective: -711.378 | PnL: -265.489, Risk: 445.889 | e: 5.827\n",
      "Episode: 2800 | Mean Objective: -706.774 | PnL: -235.394, Risk: 471.379 | e: 5.712\n",
      "Episode: 2900 | Mean Objective: -604.206 | PnL: -205.397, Risk: 398.810 | e: 5.599\n",
      "Episode: 3000 | Mean Objective: -563.609 | PnL: -182.000, Risk: 381.609 | e: 5.488\n",
      "Episode: 3100 | Mean Objective: -639.272 | PnL: -198.361, Risk: 440.912 | e: 5.379\n",
      "Episode: 3200 | Mean Objective: -550.433 | PnL: -164.869, Risk: 385.564 | e: 5.273\n",
      "Episode: 3300 | Mean Objective: -577.375 | PnL: -167.163, Risk: 410.212 | e: 5.168\n",
      "Episode: 3400 | Mean Objective: -686.989 | PnL: -173.582, Risk: 513.408 | e: 5.066\n",
      "Episode: 3500 | Mean Objective: -559.275 | PnL: -133.162, Risk: 426.114 | e: 4.966\n",
      "Episode: 3600 | Mean Objective: -535.226 | PnL: -119.189, Risk: 416.037 | e: 4.867\n",
      "Episode: 3700 | Mean Objective: -551.138 | PnL: -142.479, Risk: 408.660 | e: 4.771\n",
      "Episode: 3800 | Mean Objective: -530.499 | PnL: -141.911, Risk: 388.588 | e: 4.676\n",
      "Episode: 3900 | Mean Objective: -510.028 | PnL: -97.860, Risk: 412.168 | e: 4.584\n",
      "Episode: 4000 | Mean Objective: -382.343 | PnL: -84.563, Risk: 297.780 | e: 4.493\n",
      "Episode: 4100 | Mean Objective: -460.841 | PnL: -179.642, Risk: 281.200 | e: 4.404\n",
      "Episode: 4200 | Mean Objective: -531.470 | PnL: -189.091, Risk: 342.379 | e: 4.317\n",
      "Episode: 4300 | Mean Objective: -474.207 | PnL: -144.932, Risk: 329.276 | e: 4.231\n",
      "Episode: 4400 | Mean Objective: -575.989 | PnL: -127.595, Risk: 448.394 | e: 4.147\n",
      "Episode: 4500 | Mean Objective: -547.918 | PnL: -117.059, Risk: 430.859 | e: 4.065\n",
      "Episode: 4600 | Mean Objective: -682.000 | PnL: -176.064, Risk: 505.936 | e: 3.985\n",
      "Episode: 4700 | Mean Objective: -424.319 | PnL: -63.842, Risk: 360.478 | e: 3.906\n",
      "Episode: 4800 | Mean Objective: -498.782 | PnL: -70.252, Risk: 428.530 | e: 3.829\n",
      "Episode: 4900 | Mean Objective: -396.880 | PnL: -67.431, Risk: 329.449 | e: 3.753\n",
      "Episode: 5000 | Mean Objective: -525.317 | PnL: -108.510, Risk: 416.808 | e: 3.678\n",
      "Episode: 5100 | Mean Objective: -429.831 | PnL: -81.810, Risk: 348.020 | e: 3.606\n",
      "Episode: 5200 | Mean Objective: -442.700 | PnL: -101.055, Risk: 341.645 | e: 3.534\n",
      "Episode: 5300 | Mean Objective: -390.776 | PnL: -68.911, Risk: 321.865 | e: 3.464\n",
      "Episode: 5400 | Mean Objective: -420.507 | PnL: -86.518, Risk: 333.989 | e: 3.396\n",
      "Episode: 5500 | Mean Objective: -359.932 | PnL: -54.823, Risk: 305.109 | e: 3.328\n",
      "Episode: 5600 | Mean Objective: -530.040 | PnL: -114.846, Risk: 415.194 | e: 3.262\n",
      "Episode: 5700 | Mean Objective: -398.050 | PnL: -73.375, Risk: 324.675 | e: 3.198\n",
      "Episode: 5800 | Mean Objective: -447.749 | PnL: -92.486, Risk: 355.263 | e: 3.134\n",
      "Episode: 5900 | Mean Objective: -607.981 | PnL: -138.085, Risk: 469.896 | e: 3.072\n",
      "Episode: 6000 | Mean Objective: -370.279 | PnL: -50.479, Risk: 319.800 | e: 3.012\n",
      "Episode: 6100 | Mean Objective: -460.324 | PnL: -100.427, Risk: 359.896 | e: 2.952\n",
      "Episode: 6200 | Mean Objective: -406.023 | PnL: -53.824, Risk: 352.199 | e: 2.893\n",
      "Episode: 6300 | Mean Objective: -383.478 | PnL: -86.873, Risk: 296.606 | e: 2.836\n",
      "Episode: 6400 | Mean Objective: -445.599 | PnL: -97.838, Risk: 347.761 | e: 2.780\n",
      "Episode: 6500 | Mean Objective: -389.591 | PnL: -57.223, Risk: 332.368 | e: 2.725\n",
      "Episode: 6600 | Mean Objective: -402.013 | PnL: -79.698, Risk: 322.316 | e: 2.671\n",
      "Episode: 6700 | Mean Objective: -324.936 | PnL: -51.274, Risk: 273.662 | e: 2.618\n",
      "Episode: 6800 | Mean Objective: -365.303 | PnL: -58.848, Risk: 306.455 | e: 2.566\n",
      "Episode: 6900 | Mean Objective: -407.639 | PnL: -90.239, Risk: 317.400 | e: 2.515\n",
      "Episode: 7000 | Mean Objective: -388.882 | PnL: -52.807, Risk: 336.076 | e: 2.466\n",
      "Episode: 7100 | Mean Objective: -292.730 | PnL: -48.361, Risk: 244.369 | e: 2.417\n",
      "Episode: 7200 | Mean Objective: -457.846 | PnL: -88.349, Risk: 369.497 | e: 2.369\n",
      "Episode: 7300 | Mean Objective: -448.553 | PnL: -107.299, Risk: 341.255 | e: 2.322\n",
      "Episode: 7400 | Mean Objective: -320.738 | PnL: -53.139, Risk: 267.599 | e: 2.276\n",
      "Episode: 7500 | Mean Objective: -386.983 | PnL: -67.121, Risk: 319.862 | e: 2.231\n",
      "Episode: 7600 | Mean Objective: -363.648 | PnL: -61.246, Risk: 302.402 | e: 2.187\n",
      "Episode: 7700 | Mean Objective: -425.352 | PnL: -82.005, Risk: 343.348 | e: 2.143\n",
      "Episode: 7800 | Mean Objective: -356.606 | PnL: -64.974, Risk: 291.631 | e: 2.101\n",
      "Episode: 7900 | Mean Objective: -368.040 | PnL: -54.058, Risk: 313.982 | e: 2.059\n",
      "Episode: 8000 | Mean Objective: -414.065 | PnL: -90.131, Risk: 323.934 | e: 2.019\n",
      "Episode: 8100 | Mean Objective: -349.415 | PnL: -50.846, Risk: 298.569 | e: 1.979\n",
      "Episode: 8200 | Mean Objective: -362.329 | PnL: -58.585, Risk: 303.744 | e: 1.939\n",
      "Episode: 8300 | Mean Objective: -411.745 | PnL: -63.323, Risk: 348.422 | e: 1.901\n",
      "Episode: 8400 | Mean Objective: -359.661 | PnL: -54.752, Risk: 304.910 | e: 1.863\n",
      "Episode: 8500 | Mean Objective: -366.420 | PnL: -70.626, Risk: 295.794 | e: 1.827\n",
      "Episode: 8600 | Mean Objective: -387.975 | PnL: -73.661, Risk: 314.315 | e: 1.790\n",
      "Episode: 8700 | Mean Objective: -317.446 | PnL: -43.211, Risk: 274.235 | e: 1.755\n",
      "Episode: 8800 | Mean Objective: -307.103 | PnL: -55.309, Risk: 251.794 | e: 1.720\n",
      "Episode: 8900 | Mean Objective: -403.659 | PnL: -68.129, Risk: 335.530 | e: 1.686\n",
      "Episode: 9000 | Mean Objective: -311.153 | PnL: -44.262, Risk: 266.892 | e: 1.653\n",
      "Episode: 9100 | Mean Objective: -310.168 | PnL: -47.894, Risk: 262.274 | e: 1.620\n",
      "Episode: 9200 | Mean Objective: -368.462 | PnL: -81.869, Risk: 286.593 | e: 1.588\n",
      "Episode: 9300 | Mean Objective: -400.817 | PnL: -87.321, Risk: 313.496 | e: 1.556\n",
      "Episode: 9400 | Mean Objective: -373.988 | PnL: -87.007, Risk: 286.982 | e: 1.526\n",
      "Episode: 9500 | Mean Objective: -346.042 | PnL: -68.014, Risk: 278.028 | e: 1.495\n",
      "Episode: 9600 | Mean Objective: -320.141 | PnL: -48.545, Risk: 271.597 | e: 1.466\n",
      "Episode: 9700 | Mean Objective: -390.818 | PnL: -79.548, Risk: 311.269 | e: 1.437\n",
      "Episode: 9800 | Mean Objective: -352.569 | PnL: -48.770, Risk: 303.799 | e: 1.408\n",
      "Episode: 9900 | Mean Objective: -320.216 | PnL: -54.024, Risk: 266.192 | e: 1.380\n",
      "Episode: 10000 | Mean Objective: -406.710 | PnL: -82.361, Risk: 324.349 | e: 1.353\n",
      "Episode: 10100 | Mean Objective: -301.394 | PnL: -43.609, Risk: 257.785 | e: 1.326\n",
      "Episode: 10200 | Mean Objective: -355.689 | PnL: -68.302, Risk: 287.388 | e: 1.300\n",
      "Episode: 10300 | Mean Objective: -354.908 | PnL: -65.781, Risk: 289.126 | e: 1.274\n",
      "Episode: 10400 | Mean Objective: -357.374 | PnL: -58.078, Risk: 299.296 | e: 1.249\n",
      "Episode: 10500 | Mean Objective: -359.722 | PnL: -62.431, Risk: 297.292 | e: 1.224\n",
      "Episode: 10600 | Mean Objective: -369.633 | PnL: -52.111, Risk: 317.522 | e: 1.200\n",
      "Episode: 10700 | Mean Objective: -370.196 | PnL: -58.117, Risk: 312.079 | e: 1.176\n",
      "Episode: 10800 | Mean Objective: -391.808 | PnL: -73.921, Risk: 317.887 | e: 1.153\n",
      "Episode: 10900 | Mean Objective: -396.873 | PnL: -68.657, Risk: 328.216 | e: 1.130\n",
      "Episode: 11000 | Mean Objective: -405.604 | PnL: -91.023, Risk: 314.581 | e: 1.108\n",
      "Episode: 11100 | Mean Objective: -336.856 | PnL: -67.396, Risk: 269.461 | e: 1.086\n",
      "Episode: 11200 | Mean Objective: -372.888 | PnL: -67.704, Risk: 305.184 | e: 1.064\n",
      "Episode: 11300 | Mean Objective: -338.609 | PnL: -48.015, Risk: 290.595 | e: 1.043\n",
      "Episode: 11400 | Mean Objective: -371.545 | PnL: -64.958, Risk: 306.587 | e: 1.023\n",
      "Episode: 11500 | Mean Objective: -413.730 | PnL: -91.085, Risk: 322.646 | e: 1.002\n",
      "Episode: 11600 | Mean Objective: -372.300 | PnL: -41.214, Risk: 331.086 | e: 0.983\n",
      "Episode: 11700 | Mean Objective: -366.055 | PnL: -61.587, Risk: 304.468 | e: 0.963\n",
      "Episode: 11800 | Mean Objective: -361.436 | PnL: -47.220, Risk: 314.216 | e: 0.944\n",
      "Episode: 11900 | Mean Objective: -426.794 | PnL: -105.600, Risk: 321.194 | e: 0.925\n",
      "Episode: 12000 | Mean Objective: -345.412 | PnL: -66.654, Risk: 278.758 | e: 0.907\n",
      "Episode: 12100 | Mean Objective: -335.584 | PnL: -45.671, Risk: 289.913 | e: 0.889\n",
      "Episode: 12200 | Mean Objective: -354.702 | PnL: -50.179, Risk: 304.523 | e: 0.871\n",
      "Episode: 12300 | Mean Objective: -413.490 | PnL: -88.840, Risk: 324.650 | e: 0.854\n",
      "Episode: 12400 | Mean Objective: -359.643 | PnL: -78.490, Risk: 281.153 | e: 0.837\n",
      "Episode: 12500 | Mean Objective: -293.260 | PnL: -23.724, Risk: 269.536 | e: 0.821\n",
      "Episode: 12600 | Mean Objective: -307.923 | PnL: -36.891, Risk: 271.032 | e: 0.804\n",
      "Episode: 12700 | Mean Objective: -333.287 | PnL: -57.043, Risk: 276.244 | e: 0.788\n",
      "Episode: 12800 | Mean Objective: -363.486 | PnL: -79.920, Risk: 283.565 | e: 0.773\n",
      "Episode: 12900 | Mean Objective: -288.603 | PnL: -42.216, Risk: 246.387 | e: 0.758\n",
      "Episode: 13000 | Mean Objective: -292.550 | PnL: -63.132, Risk: 229.418 | e: 0.743\n",
      "Episode: 13100 | Mean Objective: -413.728 | PnL: -79.184, Risk: 334.544 | e: 0.728\n",
      "Episode: 13200 | Mean Objective: -349.333 | PnL: -73.989, Risk: 275.343 | e: 0.713\n",
      "Episode: 13300 | Mean Objective: -346.367 | PnL: -59.932, Risk: 286.435 | e: 0.699\n",
      "Episode: 13400 | Mean Objective: -327.521 | PnL: -51.621, Risk: 275.901 | e: 0.685\n",
      "Episode: 13500 | Mean Objective: -316.157 | PnL: -46.286, Risk: 269.871 | e: 0.672\n",
      "Episode: 13600 | Mean Objective: -327.639 | PnL: -60.732, Risk: 266.907 | e: 0.659\n",
      "Episode: 13700 | Mean Objective: -296.267 | PnL: -36.253, Risk: 260.014 | e: 0.646\n",
      "Episode: 13800 | Mean Objective: -393.718 | PnL: -82.888, Risk: 310.830 | e: 0.633\n",
      "Episode: 13900 | Mean Objective: -371.964 | PnL: -71.447, Risk: 300.517 | e: 0.620\n",
      "Episode: 14000 | Mean Objective: -282.579 | PnL: -44.044, Risk: 238.535 | e: 0.608\n",
      "Episode: 14100 | Mean Objective: -323.737 | PnL: -50.740, Risk: 272.997 | e: 0.596\n",
      "Episode: 14200 | Mean Objective: -377.277 | PnL: -86.167, Risk: 291.110 | e: 0.584\n",
      "Episode: 14300 | Mean Objective: -318.536 | PnL: -48.592, Risk: 269.944 | e: 0.573\n",
      "Episode: 14400 | Mean Objective: -346.516 | PnL: -74.918, Risk: 271.597 | e: 0.561\n",
      "Episode: 14500 | Mean Objective: -251.742 | PnL: -31.664, Risk: 220.078 | e: 0.550\n",
      "Episode: 14600 | Mean Objective: -304.560 | PnL: -45.014, Risk: 259.547 | e: 0.539\n",
      "Episode: 14700 | Mean Objective: -296.157 | PnL: -39.336, Risk: 256.821 | e: 0.529\n",
      "Episode: 14800 | Mean Objective: -301.355 | PnL: -33.995, Risk: 267.360 | e: 0.518\n",
      "Episode: 14900 | Mean Objective: -331.014 | PnL: -64.024, Risk: 266.990 | e: 0.508\n",
      "Episode: 15000 | Mean Objective: -423.198 | PnL: -91.687, Risk: 331.511 | e: 0.498\n",
      "Episode: 15100 | Mean Objective: -291.345 | PnL: -38.830, Risk: 252.515 | e: 0.488\n",
      "Episode: 15200 | Mean Objective: -339.472 | PnL: -68.082, Risk: 271.390 | e: 0.478\n",
      "Episode: 15300 | Mean Objective: -335.904 | PnL: -49.137, Risk: 286.767 | e: 0.469\n",
      "Episode: 15400 | Mean Objective: -346.520 | PnL: -65.952, Risk: 280.567 | e: 0.459\n",
      "Episode: 15500 | Mean Objective: -332.547 | PnL: -57.701, Risk: 274.846 | e: 0.450\n",
      "Episode: 15600 | Mean Objective: -313.443 | PnL: -41.397, Risk: 272.046 | e: 0.441\n",
      "Episode: 15700 | Mean Objective: -293.243 | PnL: -33.011, Risk: 260.231 | e: 0.433\n",
      "Episode: 15800 | Mean Objective: -407.844 | PnL: -81.796, Risk: 326.048 | e: 0.424\n",
      "Episode: 15900 | Mean Objective: -302.340 | PnL: -47.681, Risk: 254.659 | e: 0.416\n",
      "Episode: 16000 | Mean Objective: -391.364 | PnL: -75.648, Risk: 315.716 | e: 0.407\n",
      "Episode: 16100 | Mean Objective: -328.616 | PnL: -55.485, Risk: 273.131 | e: 0.399\n",
      "Episode: 16200 | Mean Objective: -353.972 | PnL: -64.503, Risk: 289.469 | e: 0.392\n",
      "Episode: 16300 | Mean Objective: -298.026 | PnL: -34.196, Risk: 263.830 | e: 0.384\n",
      "Episode: 16400 | Mean Objective: -313.590 | PnL: -56.924, Risk: 256.665 | e: 0.376\n",
      "Episode: 16500 | Mean Objective: -301.913 | PnL: -45.107, Risk: 256.807 | e: 0.369\n",
      "Episode: 16600 | Mean Objective: -341.058 | PnL: -56.752, Risk: 284.306 | e: 0.361\n",
      "Episode: 16700 | Mean Objective: -320.490 | PnL: -64.537, Risk: 255.952 | e: 0.354\n",
      "Episode: 16800 | Mean Objective: -320.083 | PnL: -62.883, Risk: 257.200 | e: 0.347\n",
      "Episode: 16900 | Mean Objective: -352.440 | PnL: -62.679, Risk: 289.760 | e: 0.340\n",
      "Episode: 17000 | Mean Objective: -339.742 | PnL: -69.446, Risk: 270.296 | e: 0.334\n",
      "Episode: 17100 | Mean Objective: -386.946 | PnL: -97.507, Risk: 289.439 | e: 0.327\n",
      "Episode: 17200 | Mean Objective: -304.967 | PnL: -46.182, Risk: 258.784 | e: 0.321\n",
      "Episode: 17300 | Mean Objective: -304.542 | PnL: -56.038, Risk: 248.504 | e: 0.314\n",
      "Episode: 17400 | Mean Objective: -338.434 | PnL: -50.494, Risk: 287.939 | e: 0.308\n",
      "Episode: 17500 | Mean Objective: -347.787 | PnL: -82.535, Risk: 265.252 | e: 0.302\n",
      "Episode: 17600 | Mean Objective: -340.962 | PnL: -61.339, Risk: 279.623 | e: 0.296\n",
      "Episode: 17700 | Mean Objective: -314.052 | PnL: -43.588, Risk: 270.464 | e: 0.290\n",
      "Episode: 17800 | Mean Objective: -272.587 | PnL: -35.423, Risk: 237.163 | e: 0.284\n",
      "Episode: 17900 | Mean Objective: -367.187 | PnL: -67.097, Risk: 300.090 | e: 0.279\n",
      "Episode: 18000 | Mean Objective: -349.865 | PnL: -61.261, Risk: 288.604 | e: 0.273\n",
      "Episode: 18100 | Mean Objective: -354.210 | PnL: -71.541, Risk: 282.669 | e: 0.268\n",
      "Episode: 18200 | Mean Objective: -360.086 | PnL: -62.990, Risk: 297.096 | e: 0.262\n",
      "Episode: 18300 | Mean Objective: -294.652 | PnL: -49.233, Risk: 245.419 | e: 0.257\n",
      "Episode: 18400 | Mean Objective: -339.279 | PnL: -58.136, Risk: 281.143 | e: 0.252\n",
      "Episode: 18500 | Mean Objective: -284.660 | PnL: -40.253, Risk: 244.407 | e: 0.247\n",
      "Episode: 18600 | Mean Objective: -274.503 | PnL: -48.428, Risk: 226.075 | e: 0.242\n",
      "Episode: 18700 | Mean Objective: -295.227 | PnL: -62.237, Risk: 232.990 | e: 0.237\n",
      "Episode: 18800 | Mean Objective: -379.291 | PnL: -86.953, Risk: 292.338 | e: 0.233\n",
      "Episode: 18900 | Mean Objective: -352.920 | PnL: -87.958, Risk: 264.962 | e: 0.228\n",
      "Episode: 19000 | Mean Objective: -370.060 | PnL: -70.372, Risk: 299.688 | e: 0.224\n",
      "Episode: 19100 | Mean Objective: -341.115 | PnL: -62.181, Risk: 278.934 | e: 0.219\n",
      "Episode: 19200 | Mean Objective: -317.066 | PnL: -59.521, Risk: 257.546 | e: 0.215\n",
      "Episode: 19300 | Mean Objective: -353.874 | PnL: -72.931, Risk: 280.943 | e: 0.211\n",
      "Episode: 19400 | Mean Objective: -280.817 | PnL: -45.060, Risk: 235.757 | e: 0.206\n",
      "Episode: 19500 | Mean Objective: -383.731 | PnL: -94.206, Risk: 289.525 | e: 0.202\n",
      "Episode: 19600 | Mean Objective: -369.285 | PnL: -82.035, Risk: 287.249 | e: 0.198\n",
      "Episode: 19700 | Mean Objective: -318.807 | PnL: -52.141, Risk: 266.666 | e: 0.194\n",
      "Episode: 19800 | Mean Objective: -390.716 | PnL: -86.506, Risk: 304.209 | e: 0.191\n",
      "Episode: 19900 | Mean Objective: -355.704 | PnL: -42.510, Risk: 313.193 | e: 0.187\n",
      "Episode: 20000 | Mean Objective: -309.485 | PnL: -55.557, Risk: 253.928 | e: 0.183\n",
      "Episode: 20001 | Mean Objective: -310.104 | e: 0.183\r"
     ]
    }
   ],
   "source": [
    "# Continue training\n",
    "hot_obj_train_hist, _ = DDPG_train(hot_actor,\n",
    "                                    hot_cost_critic,\n",
    "                                    hot_risk_critic,\n",
    "                                    env,\n",
    "                                    episodes= 20000,\n",
    "                                    batch_size = 128,\n",
    "                                    lr = 0.0001,\n",
    "                                    tau=0.00005,\n",
    "                                    epsilon = 10.00,\n",
    "                                    epsilon_decay = 0.9998,\n",
    "                                    discount = 1,\n",
    "                                    eval_freq = 100,\n",
    "                                    min_epsilon = 0.0,\n",
    "                                    noisy_exploration = True,\n",
    "                                    inital_buffer = 1000\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "443cf18c-94e3-4f93-897c-7cb8d5a823a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hot_actor, f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_cost_critic, f'hot_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_risk_critic, f'hot_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'hot_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', hot_obj+hot_obj_train_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155a64b-7d0d-4105-8531-de8410309e40",
   "metadata": {},
   "source": [
    "#### 3M env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cdaa6eb-d6e1-4ccc-88f5-4388510662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = HedgingEnv(T = 1/4, sigma0 = 0.2, kappa = 0.01, risk_aversion = 1.5, stochastic_vol=False, n_steps = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec00a49-10b1-423c-b054-d68dd1fb0913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -2905.213 | PnL: -2150.071, Risk: 755.142 | e: 0.990\n",
      "Episode: 200 | Mean Objective: -3143.255 | PnL: -2304.955, Risk: 838.300 | e: 0.980\n",
      "Episode: 300 | Mean Objective: -2866.280 | PnL: -2168.423, Risk: 697.857 | e: 0.970\n",
      "Episode: 400 | Mean Objective: -2935.323 | PnL: -2174.903, Risk: 760.420 | e: 0.961\n",
      "Episode: 500 | Mean Objective: -3011.270 | PnL: -2209.826, Risk: 801.444 | e: 0.951\n",
      "Episode: 600 | Mean Objective: -2925.807 | PnL: -2189.229, Risk: 736.579 | e: 0.942\n",
      "Episode: 700 | Mean Objective: -3103.830 | PnL: -2248.733, Risk: 855.097 | e: 0.932\n",
      "Episode: 800 | Mean Objective: -2957.910 | PnL: -2262.814, Risk: 695.096 | e: 0.923\n",
      "Episode: 900 | Mean Objective: -3079.852 | PnL: -2278.127, Risk: 801.725 | e: 0.914\n",
      "Episode: 1000 | Mean Objective: -2826.445 | PnL: -2162.091, Risk: 664.354 | e: 0.905\n",
      "Episode: 1100 | Mean Objective: -3250.146 | PnL: -2383.462, Risk: 866.684 | e: 0.896\n",
      "Episode: 1200 | Mean Objective: -3048.331 | PnL: -2360.008, Risk: 688.324 | e: 0.887\n",
      "Episode: 1300 | Mean Objective: -3220.188 | PnL: -2354.907, Risk: 865.281 | e: 0.878\n",
      "Episode: 1400 | Mean Objective: -3055.726 | PnL: -2187.914, Risk: 867.811 | e: 0.869\n",
      "Episode: 1500 | Mean Objective: -2637.207 | PnL: -1908.997, Risk: 728.210 | e: 0.861\n",
      "Episode: 1512 | Mean Objective: -2713.719 | e: 0.860\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "cold_actor = Actor(4,1)\n",
    "cold_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "cold_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# train\n",
    "cold_obj_hist, _ = DDPG_train(cold_actor,\n",
    "                            cold_cost_critic,\n",
    "                            cold_risk_critic,\n",
    "                            env,\n",
    "                            episodes= 50000,\n",
    "                            batch_size = 128,\n",
    "                            lr = 0.0001,\n",
    "                            tau=0.00005,\n",
    "                            epsilon = 1,\n",
    "                            epsilon_decay = 0.9999,\n",
    "                            discount = 1,\n",
    "                            eval_freq = 100,\n",
    "                            min_epsilon = 0.0,\n",
    "                            noisy_exploration = False,\n",
    "                            inital_buffer = 1000\n",
    "                            )\n",
    "\n",
    "torch.save(cold_actor, f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_cost_critic, f'cold_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_risk_critic, f'cold_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'cold_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', cold_obj_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddb03827-fdb7-48b5-a3bc-ef6988c11b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samples for hot start...\n",
      "Progress: 100.00%\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "hot_actor = Actor(4,1)\n",
    "hot_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "hot_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# Generate data\n",
    "X, y = hot_start_gen_actor_samples(env,n_paths = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce2cf3bc-19c5-49b1-bbd0-355c91e37f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Actor...\n",
      "Epoch: 1, Batch: 508/508 | Loss: 48.1297\n",
      "Epoch: 11, Batch: 508/508 | Loss: 67.2236\n",
      "Epoch: 21, Batch: 508/508 | Loss: 45.4249\n",
      "Epoch: 31, Batch: 508/508 | Loss: 39.1863\n",
      "Epoch: 41, Batch: 508/508 | Loss: 12.688\n",
      "Epoch: 51, Batch: 508/508 | Loss: 11.357\n",
      "Epoch: 61, Batch: 508/508 | Loss: 17.420\n",
      "Epoch: 71, Batch: 508/508 | Loss: 12.213\n",
      "Epoch: 81, Batch: 508/508 | Loss: 8.5863\n",
      "Epoch: 91, Batch: 508/508 | Loss: 5.7333\n",
      "Epoch: 101, Batch: 508/508 | Loss: 20.114\n",
      "Epoch: 111, Batch: 508/508 | Loss: 2.3906\n",
      "Epoch: 121, Batch: 508/508 | Loss: 4.3301\n",
      "Epoch: 131, Batch: 508/508 | Loss: 4.6699\n",
      "Epoch: 141, Batch: 508/508 | Loss: 3.1783\n",
      "Epoch: 150, Batch: 508/508 | Loss: 3.7646\r"
     ]
    }
   ],
   "source": [
    "# BC Train actor\n",
    "hot_actor, loss_hist = hot_start_actor(hot_actor, X, y, lr=0.001, batch_size=128, epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6ef79-102b-47a1-8401-fc7b7bd8b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot start critic\n",
    "hot_obj, hot_q_guess = hot_start_critic_q_func(hot_actor,\n",
    "                                            hot_cost_critic,\n",
    "                                            hot_risk_critic,\n",
    "                                            env,\n",
    "                                            episodes= 5000,\n",
    "                                            batch_size = 128,\n",
    "                                            lr = 0.0005,\n",
    "                                            tau=0.001,\n",
    "                                            epsilon = 60.0,\n",
    "                                            epsilon_decay = 0.9993,\n",
    "                                            discount = 1.0,\n",
    "                                            eval_freq = 100,\n",
    "                                            min_epsilon = 10.0,\n",
    "                                            noisy_exploration = True\n",
    "                                              )\n",
    "\n",
    "# Continue training\n",
    "hot_obj_train_hist, _ = DDPG_train(hot_actor,\n",
    "                                    hot_cost_critic,\n",
    "                                    hot_risk_critic,\n",
    "                                    env,\n",
    "                                    episodes= 20000,\n",
    "                                    batch_size = 128,\n",
    "                                    lr = 0.0001,\n",
    "                                    tau=0.00005,\n",
    "                                    epsilon = 10.00,\n",
    "                                    epsilon_decay = 0.9998,\n",
    "                                    discount = 1,\n",
    "                                    eval_freq = 100,\n",
    "                                    min_epsilon = 0.0,\n",
    "                                    noisy_exploration = True,\n",
    "                                    inital_buffer = 1000\n",
    "                                    )\n",
    "\n",
    "torch.save(hot_actor, f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_cost_critic, f'hot_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_risk_critic, f'hot_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'hot_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', hot_obj+hot_obj_train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642de17-d7ce-487e-8113-1c0f6997c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance of policy that does nothing every step\n",
    "n = 10000\n",
    "\n",
    "print(\"'Do Nothing' policy:\")\n",
    "_,nothing_history = evaluation().eval_policy(env, \"Nothing\", episodes = n, verbose = True)\n",
    "\n",
    "# Test analytical BS hedge\n",
    "print()\n",
    "print(\"BS policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"BS\", episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Cold Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Hot Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e55b46-5e83-45e6-a060-3e83c8ef1c06",
   "metadata": {},
   "source": [
    "#### 3M Stoch Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2716337-e5f0-4acd-9677-ea513826a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = HedgingEnv(T = 1/4, sigma0 = 0.2, kappa = 0.01, risk_aversion = 1.5, stochastic_vol=True, n_steps = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593313d-05d4-438d-8a98-86074bd6aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "cold_actor = Actor(4,1)\n",
    "cold_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "cold_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# train\n",
    "cold_obj_hist, _ = DDPG_train(cold_actor,\n",
    "                            cold_cost_critic,\n",
    "                            cold_risk_critic,\n",
    "                            env,\n",
    "                            episodes= 50000,\n",
    "                            batch_size = 128,\n",
    "                            lr = 0.0001,\n",
    "                            tau=0.00005,\n",
    "                            epsilon = 1,\n",
    "                            epsilon_decay = 0.9999,\n",
    "                            discount = 1,\n",
    "                            eval_freq = 100,\n",
    "                            min_epsilon = 0.0,\n",
    "                            noisy_exploration = False,\n",
    "                            inital_buffer = 1000\n",
    "                            )\n",
    "\n",
    "torch.save(cold_actor, f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_cost_critic, f'cold_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(cold_risk_critic, f'cold_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'cold_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', cold_obj_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02bbd31a-222e-4335-9972-d4e1f371cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samples for hot start...\n",
      "Progress: 100.00%\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "hot_actor_stoch = Actor(4,1)\n",
    "hot_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "hot_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# Generate data\n",
    "X, y = hot_start_gen_actor_samples(env,n_paths = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "993e356d-a9ee-4e1b-b129-83e4abfd6de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Actor...\n",
      "Epoch: 1, Batch: 508/508 | Loss: 78.7154\n",
      "Epoch: 11, Batch: 508/508 | Loss: 49.1207\n",
      "Epoch: 21, Batch: 508/508 | Loss: 31.1011\n",
      "Epoch: 31, Batch: 508/508 | Loss: 46.1536\n",
      "Epoch: 41, Batch: 508/508 | Loss: 26.4436\n",
      "Epoch: 51, Batch: 508/508 | Loss: 10.092\n",
      "Epoch: 61, Batch: 508/508 | Loss: 13.302\n",
      "Epoch: 71, Batch: 508/508 | Loss: 25.181\n",
      "Epoch: 81, Batch: 508/508 | Loss: 8.8741\n",
      "Epoch: 91, Batch: 508/508 | Loss: 4.3059\n",
      "Epoch: 101, Batch: 508/508 | Loss: 10.478\n",
      "Epoch: 111, Batch: 508/508 | Loss: 5.3453\n",
      "Epoch: 121, Batch: 508/508 | Loss: 11.563\n",
      "Epoch: 131, Batch: 508/508 | Loss: 5.7084\n",
      "Epoch: 141, Batch: 508/508 | Loss: 6.8103\n",
      "Epoch: 150, Batch: 508/508 | Loss: 4.1548\r"
     ]
    }
   ],
   "source": [
    "# BC Train actor\n",
    "hot_actor_stoch, loss_hist = hot_start_actor(hot_actor_stoch, X, y, lr=0.001, batch_size=128, epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f82383-3a94-40a4-89fd-37fa50d9d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot start critic\n",
    "hot_obj, hot_q_guess = hot_start_critic_q_func(hot_actor_stoch,\n",
    "                                            hot_cost_critic,\n",
    "                                            hot_risk_critic,\n",
    "                                            env,\n",
    "                                            episodes= 5000,\n",
    "                                            batch_size = 128,\n",
    "                                            lr = 0.0005,\n",
    "                                            tau=0.001,\n",
    "                                            epsilon = 60.0,\n",
    "                                            epsilon_decay = 0.9993,\n",
    "                                            discount = 1.0,\n",
    "                                            eval_freq = 100,\n",
    "                                            min_epsilon = 10.0,\n",
    "                                            noisy_exploration = True\n",
    "                                              )\n",
    "\n",
    "# Continue training\n",
    "hot_obj_train_hist, _ = DDPG_train(hot_actor_stoch,\n",
    "                                    hot_cost_critic,\n",
    "                                    hot_risk_critic,\n",
    "                                    env,\n",
    "                                    episodes= 20000,\n",
    "                                    batch_size = 128,\n",
    "                                    lr = 0.0001,\n",
    "                                    tau=0.00005,\n",
    "                                    epsilon = 10.00,\n",
    "                                    epsilon_decay = 0.9998,\n",
    "                                    discount = 1,\n",
    "                                    eval_freq = 100,\n",
    "                                    min_epsilon = 0.0,\n",
    "                                    noisy_exploration = True,\n",
    "                                    inital_buffer = 1000\n",
    "                                    )\n",
    "\n",
    "torch.save(hot_actor_stoch, f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_cost_critic, f'hot_cost_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "torch.save(hot_risk_critic, f'hot_risk_critic_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth')\n",
    "\n",
    "np.save(f'hot_training_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.npy', hot_obj+hot_obj_train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39658ec-3571-43e2-8428-427075716bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance of policy that does nothing every step\n",
    "n = 10000\n",
    "\n",
    "print(\"'Do Nothing' policy:\")\n",
    "_,nothing_history = evaluation().eval_policy(env, \"Nothing\", episodes = n, verbose = True)\n",
    "\n",
    "# Test analytical BS hedge\n",
    "print()\n",
    "print(\"BS policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"BS\", episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Cold Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'cold_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)\n",
    "\n",
    "print()\n",
    "print(\"Hot Start Actor:\")\n",
    "_,bs_history = evaluation().eval_policy(env, torch.load(f'hot_actor_{env.T*12:.1f}m_{env.n_steps}_{env.stochastic_vol}_{env.sigma0}_{env.kappa}_{env.risk_aversion}.pth'), episodes = n, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
