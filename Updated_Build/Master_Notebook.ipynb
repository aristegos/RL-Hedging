{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44084610-388f-4f87-89b8-04645197ef06",
   "metadata": {},
   "source": [
    "# RL Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569d20f-10f4-46ce-81fe-87e404152870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dab1c-9722-4295-92ec-6854affce411",
   "metadata": {},
   "source": [
    "#### Hedging Environment\n",
    "see *hedge_env.py*\n",
    "- Changed from \"cash flow PnL\" to **\"Accounting PnL\"** (see Cao 2019 for more description)\n",
    "    - general idea is now our daily reward now includes daily mark-to-market prices for options\n",
    "    - For *stationary vol*: mark-to-market is black scholes\n",
    "    - For *non-stationary vol*: mark-to-market is Hagan SABR implied vol (see Hagan 2002 paper for more)\n",
    "- Adjusted stationary vol stock dynamics to match analytical Black-Scholes analytical Euler discretization\n",
    "    - Stationary Vol:\n",
    "        - Before: $S_{t+1} = S_t (1 + \\sigma \\cdot \\sqrt{dt} \\cdot Z1)$\n",
    "        - New: Euler discretization of GBM analytical solution\n",
    "    - Stochastic Vol:\n",
    "        - Changed to SABR model with log-normal returns (e.g. beta = 1)\n",
    "- Changed the 4th state variable from \"time fraction\" to tau\n",
    "- Change it from hedging 1 option to hedging 100 options --> the bigger reward and action magnitude helps the model learn better\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74511587-9d5a-4b1e-8d90-ae88c2a5c2a8",
   "metadata": {},
   "source": [
    "#### Actor and Critic Models\n",
    "see *models.py*\n",
    "- I only have DDPG models. In my mind neither PPO nor GRPO make sense in this context given we have to re paramterize to policy to output some probability distribution over our action space. Given the high amounts of precision required to properly hedge the option, this isn't stable enough to converge (from what I've experienced)\n",
    "- Added batch normalization per layer on all networks\n",
    "- Added sigmoid activation to action output of actor\n",
    "- I use 2 seperate Q-functions (the \"cost critic\" and \"risk critic\"), each modelling $E[ \\ C_t]$ and $E[C_t^2]$ respectively. This is from Cao 2019. Can read for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d57c38-37e5-4635-9b7c-ad1b2d76efc4",
   "metadata": {},
   "source": [
    "#### Hot Start Actor\n",
    "see *hot_start_actor.py*\n",
    "- Supervised learning to teach actor either BS hedge (stationary vol) or Bartlett delta hedge (stochastic vol)\n",
    "- The trained model works well but it's hard to train a critic function on it and then proceed with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199cb94-89ed-47f0-bfa2-342bcf17025f",
   "metadata": {},
   "source": [
    "#### Hot Start Critic\n",
    "see *hot_start_critic.py*\n",
    "- Takes a pre-trained actor and uses similar DDPG method (with cost and risk critic functions) to train the 2 critic functions. Seems to have trouble learning long range dependencies. Can try mess around with it if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a5f88-fdf8-4b31-ae4d-e8c34e8c48b3",
   "metadata": {},
   "source": [
    "#### DDPG Trainer\n",
    "see *DDPG_train*\n",
    "- I rehaul the code to match Cao 2019 approach. Very gradual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc749c-04ff-426f-afc3-395a3f9c543f",
   "metadata": {},
   "source": [
    "#### Policy Evaluation Function\n",
    "see *evaluate.py*\n",
    "- Simulates paths and gets average total objective function\n",
    "- Can do simulation with BS policy, Bartlett policy, Do Nothing policy, and inputted policy\n",
    "- Will use this to compare actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6fa32-deb8-4c10-b5bd-cafca6e7a98b",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Will train 3 different actors and compare performances with Black-Scholes and Do Nothing Benchmarks: Note the costs are more than 100 times larger than our previous environment (i.e. a reward of -600 can be thought of as akin to a reward of -6 in our previous environments)\n",
    "1. Analytical Models\n",
    "2. Cold-start model\n",
    "3. Hot-started model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f296a2d3-299d-4764-9d04-90b1cbd38ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hedge_env import HedgingEnv\n",
    "from hot_start_actor import hot_start_gen_actor_samples, hot_start_actor\n",
    "from hot_start_critic import hot_start_critic_q_func, hot_start_critic_value_func\n",
    "from DDPG_train import DDPG_train\n",
    "from evaluate import evaluation\n",
    "from models import Actor, DDPG_Cost_Critic, DDPG_Risk_Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6b11b-e6d6-4eab-8c47-235e7cdf67bc",
   "metadata": {},
   "source": [
    "#### Analytical Models:\n",
    "Our analytical models are already coded into the evaluate function. Simply just need to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea1cf9d-5da2-4872-bfba-138a41299c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = HedgingEnv(T = 0.25, kappa = 0.01, risk_aversion = 1.5, stochastic_vol=False, n_steps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8f7d8b-da8f-4048-8ed2-4889d6f34585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Do Nothing' policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -864.490 | PnL: 18.010, Risk: 882.499\n",
      "Episode: 200 | Mean Objective: -818.869 | PnL: 21.839, Risk: 840.708\n",
      "Episode: 300 | Mean Objective: -931.933 | PnL: -7.004, Risk: 924.930\n",
      "Episode: 400 | Mean Objective: -908.907 | PnL: 9.934, Risk: 918.841\n",
      "Episode: 500 | Mean Objective: -920.351 | PnL: -1.969, Risk: 918.382\n",
      "Episode: 600 | Mean Objective: -908.163 | PnL: 6.843, Risk: 915.006\n",
      "Episode: 700 | Mean Objective: -922.916 | PnL: -2.266, Risk: 920.650\n",
      "Episode: 800 | Mean Objective: -918.379 | PnL: 4.640, Risk: 923.019\n",
      "Episode: 900 | Mean Objective: -978.994 | PnL: -17.242, Risk: 961.752\n",
      "Episode: 1000 | Mean Objective: -959.104 | PnL: -12.419, Risk: 946.685\n",
      "\n",
      "BS policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -452.701 | PnL: -245.073, Risk: 207.628\n",
      "Episode: 200 | Mean Objective: -462.747 | PnL: -243.250, Risk: 219.496\n",
      "Episode: 300 | Mean Objective: -451.425 | PnL: -229.559, Risk: 221.866\n",
      "Episode: 400 | Mean Objective: -453.933 | PnL: -232.897, Risk: 221.036\n",
      "Episode: 500 | Mean Objective: -462.604 | PnL: -236.685, Risk: 225.918\n",
      "Episode: 600 | Mean Objective: -462.519 | PnL: -235.573, Risk: 226.947\n",
      "Episode: 700 | Mean Objective: -461.462 | PnL: -233.839, Risk: 227.623\n",
      "Episode: 800 | Mean Objective: -459.209 | PnL: -232.970, Risk: 226.239\n",
      "Episode: 900 | Mean Objective: -452.621 | PnL: -229.599, Risk: 223.022\n",
      "Episode: 1000 | Mean Objective: -453.349 | PnL: -227.835, Risk: 225.514\n",
      "\n",
      "Bartlett policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -458.637 | PnL: -216.926, Risk: 241.711\n",
      "Episode: 200 | Mean Objective: -444.384 | PnL: -211.851, Risk: 232.534\n",
      "Episode: 300 | Mean Objective: -437.573 | PnL: -210.290, Risk: 227.283\n",
      "Episode: 400 | Mean Objective: -439.756 | PnL: -211.269, Risk: 228.487\n",
      "Episode: 500 | Mean Objective: -444.342 | PnL: -216.165, Risk: 228.177\n",
      "Episode: 600 | Mean Objective: -455.462 | PnL: -221.862, Risk: 233.599\n",
      "Episode: 700 | Mean Objective: -456.833 | PnL: -221.669, Risk: 235.165\n",
      "Episode: 800 | Mean Objective: -452.482 | PnL: -219.795, Risk: 232.687\n",
      "Episode: 900 | Mean Objective: -451.919 | PnL: -220.460, Risk: 231.459\n",
      "Episode: 1000 | Mean Objective: -453.492 | PnL: -223.410, Risk: 230.081\n"
     ]
    }
   ],
   "source": [
    "# Test performance of policy that does nothing every step\n",
    "print(\"'Do Nothing' policy:\")\n",
    "_,nothing_history = evaluation().eval_policy(env, \"Nothing\", verbose = True)\n",
    "\n",
    "# Test analytical BS hedge\n",
    "print()\n",
    "print(\"BS policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"BS\", verbose = True)\n",
    "\n",
    "# Doesnt rly make sense here (as non-stochastic vol) but also demonstrate Bartlett hedge\n",
    "print()\n",
    "print(\"Bartlett policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"Bartlett\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c1dc5-7791-42da-8bdc-9ed3c53d1cae",
   "metadata": {},
   "source": [
    "#### Cold Start Model:\n",
    "1. Initialize actor and critic functions\n",
    "2. Call DDPG train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751833e-3a39-4963-81ce-bec6dda7d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -1711.108 | PnL: -1048.616, Risk: 662.492 | e: 0.980\n",
      "Episode: 200 | Mean Objective: -1696.257 | PnL: -994.694, Risk: 701.563 | e: 0.961\n",
      "Episode: 300 | Mean Objective: -1674.942 | PnL: -982.505, Risk: 692.438 | e: 0.942\n",
      "Episode: 400 | Mean Objective: -1696.979 | PnL: -1047.649, Risk: 649.330 | e: 0.923\n",
      "Episode: 500 | Mean Objective: -1743.487 | PnL: -972.133, Risk: 771.354 | e: 0.905\n",
      "Episode: 600 | Mean Objective: -1705.085 | PnL: -997.389, Risk: 707.695 | e: 0.887\n",
      "Episode: 700 | Mean Objective: -1483.226 | PnL: -915.080, Risk: 568.146 | e: 0.869\n",
      "Episode: 800 | Mean Objective: -1670.579 | PnL: -950.000, Risk: 720.579 | e: 0.852\n",
      "Episode: 900 | Mean Objective: -1465.568 | PnL: -853.213, Risk: 612.354 | e: 0.835\n",
      "Episode: 1000 | Mean Objective: -1560.740 | PnL: -872.185, Risk: 688.556 | e: 0.819\n",
      "Episode: 1100 | Mean Objective: -1607.894 | PnL: -968.512, Risk: 639.382 | e: 0.803\n",
      "Episode: 1200 | Mean Objective: -1495.384 | PnL: -873.775, Risk: 621.608 | e: 0.787\n",
      "Episode: 1300 | Mean Objective: -1732.166 | PnL: -957.967, Risk: 774.199 | e: 0.771\n",
      "Episode: 1400 | Mean Objective: -1584.138 | PnL: -908.924, Risk: 675.214 | e: 0.756\n",
      "Episode: 1500 | Mean Objective: -1446.170 | PnL: -869.096, Risk: 577.074 | e: 0.741\n",
      "Episode: 1600 | Mean Objective: -1690.585 | PnL: -975.652, Risk: 714.933 | e: 0.726\n",
      "Episode: 1700 | Mean Objective: -1681.664 | PnL: -977.801, Risk: 703.864 | e: 0.712\n",
      "Episode: 1800 | Mean Objective: -1671.420 | PnL: -1026.551, Risk: 644.869 | e: 0.698\n",
      "Episode: 1900 | Mean Objective: -1600.080 | PnL: -954.267, Risk: 645.813 | e: 0.684\n",
      "Episode: 2000 | Mean Objective: -1612.809 | PnL: -945.746, Risk: 667.063 | e: 0.670\n",
      "Episode: 2100 | Mean Objective: -1770.456 | PnL: -1094.224, Risk: 676.232 | e: 0.657\n",
      "Episode: 2200 | Mean Objective: -1743.455 | PnL: -1011.876, Risk: 731.579 | e: 0.644\n",
      "Episode: 2300 | Mean Objective: -1788.746 | PnL: -1050.631, Risk: 738.115 | e: 0.631\n",
      "Episode: 2400 | Mean Objective: -1713.496 | PnL: -1017.206, Risk: 696.290 | e: 0.619\n",
      "Episode: 2500 | Mean Objective: -1671.753 | PnL: -1000.338, Risk: 671.414 | e: 0.607\n",
      "Episode: 2600 | Mean Objective: -1897.048 | PnL: -1101.745, Risk: 795.302 | e: 0.594\n",
      "Episode: 2700 | Mean Objective: -1819.253 | PnL: -1048.680, Risk: 770.573 | e: 0.583\n",
      "Episode: 2800 | Mean Objective: -1549.201 | PnL: -927.015, Risk: 622.187 | e: 0.571\n",
      "Episode: 2900 | Mean Objective: -1534.370 | PnL: -991.327, Risk: 543.043 | e: 0.560\n",
      "Episode: 3000 | Mean Objective: -1835.775 | PnL: -1060.405, Risk: 775.370 | e: 0.549\n",
      "Episode: 3100 | Mean Objective: -1635.393 | PnL: -956.406, Risk: 678.987 | e: 0.538\n",
      "Episode: 3200 | Mean Objective: -1781.189 | PnL: -998.193, Risk: 782.996 | e: 0.527\n",
      "Episode: 3300 | Mean Objective: -1736.664 | PnL: -1053.091, Risk: 683.573 | e: 0.517\n",
      "Episode: 3400 | Mean Objective: -1479.616 | PnL: -855.725, Risk: 623.891 | e: 0.507\n",
      "Episode: 3500 | Mean Objective: -1486.134 | PnL: -870.116, Risk: 616.018 | e: 0.497\n",
      "Episode: 3600 | Mean Objective: -1529.944 | PnL: -894.575, Risk: 635.369 | e: 0.487\n",
      "Episode: 3700 | Mean Objective: -1567.876 | PnL: -900.061, Risk: 667.815 | e: 0.477\n",
      "Episode: 3800 | Mean Objective: -1394.119 | PnL: -802.120, Risk: 591.999 | e: 0.468\n",
      "Episode: 3900 | Mean Objective: -1509.552 | PnL: -824.434, Risk: 685.118 | e: 0.458\n",
      "Episode: 4000 | Mean Objective: -1391.202 | PnL: -806.474, Risk: 584.728 | e: 0.449\n",
      "Episode: 4100 | Mean Objective: -1389.328 | PnL: -812.850, Risk: 576.478 | e: 0.440\n",
      "Episode: 4200 | Mean Objective: -1479.082 | PnL: -795.429, Risk: 683.654 | e: 0.432\n",
      "Episode: 4300 | Mean Objective: -1344.623 | PnL: -781.835, Risk: 562.789 | e: 0.423\n",
      "Episode: 4400 | Mean Objective: -1317.530 | PnL: -795.061, Risk: 522.469 | e: 0.415\n",
      "Episode: 4500 | Mean Objective: -1314.134 | PnL: -748.131, Risk: 566.003 | e: 0.407\n",
      "Episode: 4600 | Mean Objective: -1479.598 | PnL: -742.868, Risk: 736.729 | e: 0.398\n",
      "Episode: 4700 | Mean Objective: -1288.681 | PnL: -718.642, Risk: 570.039 | e: 0.391\n",
      "Episode: 4800 | Mean Objective: -1370.432 | PnL: -753.118, Risk: 617.314 | e: 0.383\n",
      "Episode: 4900 | Mean Objective: -1173.595 | PnL: -650.214, Risk: 523.382 | e: 0.375\n",
      "Episode: 5000 | Mean Objective: -1275.422 | PnL: -681.290, Risk: 594.132 | e: 0.368\n",
      "Episode: 5100 | Mean Objective: -1208.840 | PnL: -658.032, Risk: 550.808 | e: 0.361\n",
      "Episode: 5200 | Mean Objective: -1272.007 | PnL: -643.402, Risk: 628.605 | e: 0.353\n",
      "Episode: 5300 | Mean Objective: -1296.849 | PnL: -657.910, Risk: 638.940 | e: 0.346\n",
      "Episode: 5400 | Mean Objective: -1143.243 | PnL: -646.408, Risk: 496.835 | e: 0.340\n",
      "Episode: 5500 | Mean Objective: -1244.068 | PnL: -631.060, Risk: 613.009 | e: 0.333\n",
      "Episode: 5600 | Mean Objective: -1295.823 | PnL: -630.863, Risk: 664.959 | e: 0.326\n",
      "Episode: 5700 | Mean Objective: -1176.155 | PnL: -624.862, Risk: 551.292 | e: 0.320\n",
      "Episode: 5800 | Mean Objective: -1159.342 | PnL: -596.161, Risk: 563.181 | e: 0.313\n",
      "Episode: 5900 | Mean Objective: -1202.399 | PnL: -636.401, Risk: 565.997 | e: 0.307\n",
      "Episode: 6000 | Mean Objective: -1271.816 | PnL: -622.643, Risk: 649.173 | e: 0.301\n",
      "Episode: 6100 | Mean Objective: -1182.489 | PnL: -578.113, Risk: 604.376 | e: 0.295\n",
      "Episode: 6115 | Mean Objective: -1181.551 | e: 0.294\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "cold_actor = Actor(4,1)\n",
    "cold_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "cold_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# train\n",
    "cold_obj_hist, _ = DDPG_train(cold_actor,\n",
    "                            cold_cost_critic,\n",
    "                            cold_risk_critic,\n",
    "                            env,\n",
    "                            episodes= 50000,\n",
    "                            batch_size = 128,\n",
    "                            lr = 0.0001,\n",
    "                            tau=0.00001,\n",
    "                            epsilon = 1,\n",
    "                            epsilon_decay = 0.9998,\n",
    "                            discount = 1,\n",
    "                            eval_freq = 100,\n",
    "                            min_epsilon = 0.0,\n",
    "                            noisy_exploration = False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b36c-7bdb-4a69-9159-41eab4f7f0a9",
   "metadata": {},
   "source": [
    "#### Hot-Start Model:\n",
    "1. Generate Training Data\n",
    "2. Hot start actor\n",
    "3. Hot start critic (still not very good)\n",
    "4. Do DDPG on pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fc6a52-0727-4db0-af81-2a240768279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Samples for hot start...\n",
      "Training Actor...\n",
      "Epoch: 1, Batch: 156/156 | Loss: 193.156\n",
      "Epoch: 11, Batch: 156/156 | Loss: 52.9809\n",
      "Epoch: 21, Batch: 156/156 | Loss: 84.7890\n",
      "Epoch: 31, Batch: 156/156 | Loss: 43.7909\n",
      "Epoch: 41, Batch: 156/156 | Loss: 11.8410\n",
      "Epoch: 51, Batch: 156/156 | Loss: 14.1049\n",
      "Epoch: 61, Batch: 156/156 | Loss: 19.2050\n",
      "Epoch: 71, Batch: 156/156 | Loss: 6.7309\n",
      "Epoch: 81, Batch: 156/156 | Loss: 14.4409\n",
      "Epoch: 91, Batch: 156/156 | Loss: 1.52295\n",
      "Epoch: 101, Batch: 156/156 | Loss: 33.447\n",
      "Epoch: 111, Batch: 156/156 | Loss: 6.67307\n",
      "Epoch: 121, Batch: 156/156 | Loss: 25.1629\n",
      "Epoch: 131, Batch: 156/156 | Loss: 0.84589\n",
      "Epoch: 141, Batch: 156/156 | Loss: 7.78742\n",
      "Epoch: 151, Batch: 156/156 | Loss: 1.2996\n",
      "Epoch: 161, Batch: 156/156 | Loss: 3.93620\n",
      "Epoch: 171, Batch: 156/156 | Loss: 1.65184\n",
      "Epoch: 181, Batch: 156/156 | Loss: 2.3470\n",
      "Epoch: 191, Batch: 156/156 | Loss: 16.0974\n",
      "Epoch: 200, Batch: 156/156 | Loss: 5.6181\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "hot_actor = Actor(4,1)\n",
    "hot_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "hot_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# Generate data\n",
    "X, y = hot_start_gen_actor_samples(env,n_paths = 100)\n",
    "\n",
    "# BC Train actor\n",
    "hot_actor, loss_hist = hot_start_actor(hot_actor, X, y, lr=0.01, batch_size=32, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b1fac9-03d5-4b7e-8042-9d10cfe5e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -1695.656 | Guess: -129.341, Diff: 1566.315 | e: 0.905\n",
      "Episode: 200 | Mean Objective: -1314.876 | Guess: -198.921, Diff: 1115.955 | e: 0.819\n",
      "Episode: 300 | Mean Objective: -1326.176 | Guess: -208.903, Diff: 1117.272 | e: 0.741\n",
      "Episode: 400 | Mean Objective: -1046.115 | Guess: -202.690, Diff: 843.425 | e: 0.6700\n",
      "Episode: 500 | Mean Objective: -1388.094 | Guess: -215.204, Diff: 1172.890 | e: 0.606\n",
      "Episode: 600 | Mean Objective: -1090.885 | Guess: -226.710, Diff: 864.174 | e: 0.5491\n",
      "Episode: 700 | Mean Objective: -839.421 | Guess: -236.638, Diff: 602.783 | e: 0.4969\n",
      "Episode: 800 | Mean Objective: -972.599 | Guess: -247.633, Diff: 724.966 | e: 0.4491\n",
      "Episode: 900 | Mean Objective: -876.007 | Guess: -259.975, Diff: 616.032 | e: 0.406\n",
      "Episode: 1000 | Mean Objective: -800.088 | Guess: -265.295, Diff: 534.793 | e: 0.368\n",
      "Episode: 1100 | Mean Objective: -1028.005 | Guess: -266.853, Diff: 761.153 | e: 0.333\n",
      "Episode: 1200 | Mean Objective: -763.967 | Guess: -260.743, Diff: 503.225 | e: 0.3017\n",
      "Episode: 1300 | Mean Objective: -934.776 | Guess: -253.843, Diff: 680.933 | e: 0.272\n",
      "Episode: 1400 | Mean Objective: -799.347 | Guess: -243.353, Diff: 555.994 | e: 0.246\n",
      "Episode: 1500 | Mean Objective: -625.625 | Guess: -235.727, Diff: 389.898 | e: 0.223\n",
      "Episode: 1600 | Mean Objective: -859.294 | Guess: -227.972, Diff: 631.322 | e: 0.202\n",
      "Episode: 1700 | Mean Objective: -769.352 | Guess: -226.880, Diff: 542.471 | e: 0.183\n",
      "Episode: 1800 | Mean Objective: -688.754 | Guess: -225.195, Diff: 463.558 | e: 0.165\n",
      "Episode: 1900 | Mean Objective: -755.702 | Guess: -218.791, Diff: 536.910 | e: 0.149\n",
      "Episode: 2000 | Mean Objective: -645.376 | Guess: -216.400, Diff: 428.977 | e: 0.135\n",
      "Episode: 2100 | Mean Objective: -670.747 | Guess: -218.860, Diff: 451.887 | e: 0.122\n",
      "Episode: 2200 | Mean Objective: -476.220 | Guess: -215.555, Diff: 260.665 | e: 0.111\n",
      "Episode: 2300 | Mean Objective: -686.518 | Guess: -221.810, Diff: 464.708 | e: 0.100\n",
      "Episode: 2400 | Mean Objective: -534.683 | Guess: -210.982, Diff: 323.701 | e: 0.091\n",
      "Episode: 2500 | Mean Objective: -587.431 | Guess: -219.602, Diff: 367.829 | e: 0.082\n",
      "Episode: 2600 | Mean Objective: -653.862 | Guess: -225.147, Diff: 428.715 | e: 0.074\n",
      "Episode: 2700 | Mean Objective: -451.753 | Guess: -221.677, Diff: 230.076 | e: 0.067\n",
      "Episode: 2800 | Mean Objective: -527.633 | Guess: -220.076, Diff: 307.557 | e: 0.061\n",
      "Episode: 2900 | Mean Objective: -423.957 | Guess: -219.524, Diff: 204.433 | e: 0.055\n",
      "Episode: 3000 | Mean Objective: -479.599 | Guess: -214.390, Diff: 265.208 | e: 0.050\n",
      "Episode: 3100 | Mean Objective: -428.425 | Guess: -209.275, Diff: 219.151 | e: 0.045\n",
      "Episode: 3200 | Mean Objective: -434.588 | Guess: -207.480, Diff: 227.108 | e: 0.041\n",
      "Episode: 3300 | Mean Objective: -508.295 | Guess: -211.705, Diff: 296.591 | e: 0.037\n",
      "Episode: 3400 | Mean Objective: -536.565 | Guess: -216.307, Diff: 320.258 | e: 0.033\n",
      "Episode: 3500 | Mean Objective: -474.673 | Guess: -227.615, Diff: 247.058 | e: 0.030\n",
      "Episode: 3600 | Mean Objective: -447.341 | Guess: -234.703, Diff: 212.638 | e: 0.027\n",
      "Episode: 3700 | Mean Objective: -442.763 | Guess: -230.068, Diff: 212.695 | e: 0.025\n",
      "Episode: 3800 | Mean Objective: -457.945 | Guess: -227.373, Diff: 230.572 | e: 0.022\n",
      "Episode: 3900 | Mean Objective: -421.319 | Guess: -229.009, Diff: 192.310 | e: 0.020\n",
      "Episode: 4000 | Mean Objective: -470.106 | Guess: -218.674, Diff: 251.432 | e: 0.018\n",
      "Episode: 4100 | Mean Objective: -403.399 | Guess: -208.844, Diff: 194.556 | e: 0.017\n",
      "Episode: 4200 | Mean Objective: -374.589 | Guess: -213.483, Diff: 161.106 | e: 0.015\n",
      "Episode: 4300 | Mean Objective: -369.441 | Guess: -209.358, Diff: 160.083 | e: 0.014\n",
      "Episode: 4400 | Mean Objective: -350.994 | Guess: -200.571, Diff: 150.423 | e: 0.012\n",
      "Episode: 4500 | Mean Objective: -429.581 | Guess: -198.560, Diff: 231.021 | e: 0.011\n",
      "Episode: 4600 | Mean Objective: -319.423 | Guess: -204.200, Diff: 115.223 | e: 0.010\n",
      "Episode: 4700 | Mean Objective: -483.519 | Guess: -212.078, Diff: 271.441 | e: 0.009\n",
      "Episode: 4800 | Mean Objective: -419.577 | Guess: -218.223, Diff: 201.353 | e: 0.008\n",
      "Episode: 4900 | Mean Objective: -348.457 | Guess: -222.004, Diff: 126.453 | e: 0.007\n",
      "Episode: 5000 | Mean Objective: -326.396 | Guess: -219.281, Diff: 107.114 | e: 0.007\n",
      "Episode: 5100 | Mean Objective: -382.126 | Guess: -213.462, Diff: 168.664 | e: 0.006\n",
      "Episode: 5200 | Mean Objective: -363.399 | Guess: -205.759, Diff: 157.640 | e: 0.006\n",
      "Episode: 5216 | Mean Objective: -370.702 | Guess: -205.102, Diff: 165.600 | e: 0.005\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hot start critic\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hot_obj, hot_q_guess \u001b[38;5;241m=\u001b[39m \u001b[43mhot_start_critic_q_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhot_cost_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mhot_risk_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.000025\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/COS 435/Project/hot_start_critic.py:114\u001b[0m, in \u001b[0;36mhot_start_critic_q_func\u001b[0;34m(actor, cost_critic, risk_critic, env, episodes, batch_size, lr, tau, epsilon, epsilon_decay, discount, eval_freq, min_epsilon)\u001b[0m\n\u001b[1;32m    111\u001b[0m cost_critic_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()(current_cost_q, expected_cost_q)\n\u001b[1;32m    113\u001b[0m cost_critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m \u001b[43mcost_critic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m cost_critic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# update risk_critic\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Renv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Renv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Renv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hot start critic\n",
    "hot_obj, hot_q_guess = hot_start_critic_q_func(hot_actor,\n",
    "                                            hot_cost_critic,\n",
    "                                            hot_risk_critic,\n",
    "                                            env,\n",
    "                                            episodes= 10000,\n",
    "                                            batch_size = 128,\n",
    "                                            lr = 0.01,\n",
    "                                            tau=0.000025,\n",
    "                                            epsilon = 1.0,\n",
    "                                            epsilon_decay = 0.9995,\n",
    "                                            discount = 1.0,\n",
    "                                            eval_freq = 100,\n",
    "                                            min_epsilon = 0.01,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88e31c6-0a46-4135-a0d2-1f15e7a07c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -688.824 | PnL: -103.745, Risk: 585.079 | e: 0.000\n",
      "Episode: 200 | Mean Objective: -714.381 | PnL: 38.457, Risk: 752.838 | e: 0.000\n",
      "Episode: 300 | Mean Objective: -828.202 | PnL: -16.817, Risk: 811.385 | e: 0.000\n",
      "Episode: 400 | Mean Objective: -736.541 | PnL: -80.881, Risk: 655.659 | e: 0.000\n",
      "Episode: 500 | Mean Objective: -881.368 | PnL: -173.640, Risk: 707.728 | e: 0.000\n",
      "Episode: 600 | Mean Objective: -665.988 | PnL: 6.854, Risk: 672.842 | e: 0.000\n",
      "Episode: 700 | Mean Objective: -724.194 | PnL: 31.031, Risk: 755.225 | e: 0.000\n",
      "Episode: 749 | Mean Objective: -825.548 | e: 0.000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Continue training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m hot_obj_train_hist, _ \u001b[38;5;241m=\u001b[39m \u001b[43mDDPG_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mhot_cost_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mhot_risk_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9995\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/COS 435/Project/DDPG_train.py:78\u001b[0m, in \u001b[0;36mDDPG_train\u001b[0;34m(actor, cost_critic, risk_critic, env, episodes, batch_size, lr, tau, epsilon, epsilon_decay, discount, eval_freq, min_epsilon)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Update during episode if enough samples are available\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m>\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(states)\n\u001b[1;32m     80\u001b[0m     actions_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(actions)\n",
      "File \u001b[0;32m~/COS 435/Project/hot_start_critic.py:25\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m     24\u001b[0m     batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer, batch_size)\n\u001b[0;32m---> 25\u001b[0m     states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m states, actions, rewards, next_states, dones\n",
      "File \u001b[0;32m~/miniconda3/envs/Renv/lib/python3.11/site-packages/torch/_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue training\n",
    "hot_obj_train_hist, _ = DDPG_train(hot_actor,\n",
    "                                    hot_cost_critic,\n",
    "                                    hot_risk_critic,\n",
    "                                    env,\n",
    "                                    episodes= 20000,\n",
    "                                    batch_size = 128,\n",
    "                                    lr = 0.0001,\n",
    "                                    tau=0.00001,\n",
    "                                    epsilon = 5.00,\n",
    "                                    epsilon_decay = 0.9999,\n",
    "                                    discount = 1,\n",
    "                                    eval_freq = 100,\n",
    "                                    min_epsilon = 0.5,\n",
    "                                    noisy_exploration = True\n",
    "                                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
