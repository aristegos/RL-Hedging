{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44084610-388f-4f87-89b8-04645197ef06",
   "metadata": {},
   "source": [
    "# RL Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569d20f-10f4-46ce-81fe-87e404152870",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dab1c-9722-4295-92ec-6854affce411",
   "metadata": {},
   "source": [
    "#### Hedging Environment\n",
    "see *hedge_env.py*\n",
    "- Changed from \"cash flow PnL\" to **\"Accounting PnL\"** (see Cao 2019 for more description)\n",
    "    - general idea is now our daily reward now includes daily mark-to-market prices for options\n",
    "    - For *stationary vol*: mark-to-market is black scholes\n",
    "    - For *non-stationary vol*: mark-to-market is Hagan SABR implied vol (see Hagan 2002 paper for more)\n",
    "- Adjusted stationary vol stock dynamics to match analytical Black-Scholes analytical Euler discretization\n",
    "    - Stationary Vol:\n",
    "        - Before: $S_{t+1} = S_t (1 + \\sigma \\cdot \\sqrt{dt} \\cdot Z1)$\n",
    "        - New: Euler discretization of GBM analytical solution\n",
    "    - Stochastic Vol:\n",
    "        - Changed to SABR model with log-normal returns (e.g. beta = 1)\n",
    "- Changed the 4th state variable from \"time fraction\" to tau\n",
    "- Change it from hedging 1 option to hedging 100 options --> the bigger reward and action magnitude helps the model learn better\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74511587-9d5a-4b1e-8d90-ae88c2a5c2a8",
   "metadata": {},
   "source": [
    "#### Actor and Critic Models\n",
    "see *models.py*\n",
    "- I only have DDPG models. In my mind neither PPO nor GRPO make sense in this context given we have to re paramterize to policy to output some probability distribution over our action space. Given the high amounts of precision required to properly hedge the option, this isn't stable enough to converge (from what I've experienced)\n",
    "- Added batch normalization per layer on all networks\n",
    "- Added sigmoid activation to action output of actor\n",
    "- I use 2 seperate Q-functions (the \"cost critic\" and \"risk critic\"), each modelling $E[ \\ C_t]$ and $E[C_t^2]$ respectively. This is from Cao 2019. Can read for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d57c38-37e5-4635-9b7c-ad1b2d76efc4",
   "metadata": {},
   "source": [
    "#### Hot Start Actor\n",
    "see *hot_start_actor.py*\n",
    "- Supervised learning to teach actor either BS hedge (stationary vol) or Bartlett delta hedge (stochastic vol)\n",
    "- The trained model works well but it's hard to train a critic function on it and then proceed with DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199cb94-89ed-47f0-bfa2-342bcf17025f",
   "metadata": {},
   "source": [
    "#### Hot Start Critic\n",
    "see *hot_start_critic.py*\n",
    "- Takes a pre-trained actor and uses similar DDPG method (with cost and risk critic functions) to train the 2 critic functions. Seems to have trouble learning long range dependencies. Can try mess around with it if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a5f88-fdf8-4b31-ae4d-e8c34e8c48b3",
   "metadata": {},
   "source": [
    "#### DDPG Trainer\n",
    "see *DDPG_train*\n",
    "- I rehaul the code to match Cao 2019 approach. Very gradual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc749c-04ff-426f-afc3-395a3f9c543f",
   "metadata": {},
   "source": [
    "#### Policy Evaluation Function\n",
    "see *evaluate.py*\n",
    "- Simulates paths and gets average total objective function\n",
    "- Can do simulation with BS policy, Bartlett policy, Do Nothing policy, and inputted policy\n",
    "- Will use this to compare actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6fa32-deb8-4c10-b5bd-cafca6e7a98b",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Will train 3 different actors and compare performances with Black-Scholes and Do Nothing Benchmarks: Note the costs are more than 100 times larger than our previous environment (i.e. a reward of -600 can be thought of as akin to a reward of -6 in our previous environments)\n",
    "1. Analytical Models\n",
    "2. Cold-start model\n",
    "3. Hot-started model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f296a2d3-299d-4764-9d04-90b1cbd38ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hedge_env import HedgingEnv\n",
    "from hot_start_actor import hot_start_gen_actor_samples, hot_start_actor\n",
    "from hot_start_critic import hot_start_critic_q_func, hot_start_critic_value_func\n",
    "from PPO_train import PPO_train\n",
    "from DDPG_train import DDPG_train\n",
    "from evaluate import evaluation\n",
    "from models import Actor, DDPG_Cost_Critic, DDPG_Risk_Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6b11b-e6d6-4eab-8c47-235e7cdf67bc",
   "metadata": {},
   "source": [
    "#### Analytical Models:\n",
    "Our analytical models are already coded into the evaluate function. Simply just need to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8f7d8b-da8f-4048-8ed2-4889d6f34585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Do Nothing' policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -1812.241 | PnL: 80.839, Risk: 1893.080\n",
      "Episode: 200 | Mean Objective: -1896.349 | PnL: 38.335, Risk: 1934.684\n",
      "Episode: 300 | Mean Objective: -1646.584 | PnL: 91.122, Risk: 1737.706\n",
      "Episode: 400 | Mean Objective: -1664.372 | PnL: 94.468, Risk: 1758.839\n",
      "Episode: 500 | Mean Objective: -1721.654 | PnL: 101.258, Risk: 1822.911\n",
      "Episode: 600 | Mean Objective: -1769.551 | PnL: 84.089, Risk: 1853.640\n",
      "Episode: 700 | Mean Objective: -1859.789 | PnL: 56.966, Risk: 1916.755\n",
      "Episode: 800 | Mean Objective: -1896.552 | PnL: 39.904, Risk: 1936.456\n",
      "Episode: 900 | Mean Objective: -1904.885 | PnL: 31.479, Risk: 1936.364\n",
      "Episode: 1000 | Mean Objective: -1912.011 | PnL: 24.921, Risk: 1936.932\n",
      "\n",
      "BS policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -580.826 | PnL: -280.202, Risk: 300.624\n",
      "Episode: 200 | Mean Objective: -618.071 | PnL: -287.185, Risk: 330.886\n",
      "Episode: 300 | Mean Objective: -633.413 | PnL: -285.534, Risk: 347.879\n",
      "Episode: 400 | Mean Objective: -631.282 | PnL: -278.245, Risk: 353.036\n",
      "Episode: 500 | Mean Objective: -626.841 | PnL: -277.907, Risk: 348.933\n",
      "Episode: 600 | Mean Objective: -621.775 | PnL: -275.078, Risk: 346.696\n",
      "Episode: 700 | Mean Objective: -616.107 | PnL: -271.747, Risk: 344.360\n",
      "Episode: 800 | Mean Objective: -621.040 | PnL: -272.129, Risk: 348.910\n",
      "Episode: 900 | Mean Objective: -626.928 | PnL: -271.929, Risk: 355.000\n",
      "Episode: 1000 | Mean Objective: -627.240 | PnL: -272.438, Risk: 354.802\n",
      "\n",
      "Bartlett policy:\n",
      "testing...\n",
      "Episode: 100 | Mean Objective: -581.624 | PnL: -266.114, Risk: 315.509\n",
      "Episode: 200 | Mean Objective: -568.397 | PnL: -241.303, Risk: 327.094\n",
      "Episode: 300 | Mean Objective: -583.253 | PnL: -246.471, Risk: 336.782\n",
      "Episode: 400 | Mean Objective: -605.768 | PnL: -258.604, Risk: 347.164\n",
      "Episode: 500 | Mean Objective: -608.318 | PnL: -263.361, Risk: 344.957\n",
      "Episode: 600 | Mean Objective: -611.572 | PnL: -269.404, Risk: 342.168\n",
      "Episode: 700 | Mean Objective: -614.999 | PnL: -270.675, Risk: 344.325\n",
      "Episode: 800 | Mean Objective: -618.034 | PnL: -274.269, Risk: 343.765\n",
      "Episode: 900 | Mean Objective: -621.815 | PnL: -275.442, Risk: 346.372\n",
      "Episode: 1000 | Mean Objective: -620.822 | PnL: -272.053, Risk: 348.769\n"
     ]
    }
   ],
   "source": [
    "# initialize environment\n",
    "env = HedgingEnv(T = 1, kappa = 0.01, risk_aversion = 1.5, stochastic_vol=False, n_steps = 50)\n",
    "\n",
    "# Test performance of policy that does nothing every step\n",
    "print(\"'Do Nothing' policy:\")\n",
    "_,nothing_history = evaluation().eval_policy(env, \"Nothing\", verbose = True)\n",
    "\n",
    "# Test analytical BS hedge\n",
    "print()\n",
    "print(\"BS policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"BS\", verbose = True)\n",
    "\n",
    "# Doesnt rly make sense here (as non-stochastic vol) but also demonstrate Bartlett hedge\n",
    "print()\n",
    "print(\"Bartlett policy:\")\n",
    "_,bs_history = evaluation().eval_policy(env, \"Bartlett\", verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c1dc5-7791-42da-8bdc-9ed3c53d1cae",
   "metadata": {},
   "source": [
    "#### Cold Start Model:\n",
    "1. Initialize actor and critic functions\n",
    "2. Call DDPG train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751833e-3a39-4963-81ce-bec6dda7d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Critic (learning q function)...\n",
      "Episode: 100 | Mean Objective: -2828.787 | PnL: -854.519, Risk: 1974.268\n",
      "Episode: 110 | Mean Objective: -2578.046\r"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "cold_actor = Actor(4,1)\n",
    "cold_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "cold_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# train\n",
    "cold_obj_hist, _ = DDPG_train(cold_actor,\n",
    "                            cold_cost_critic,\n",
    "                            cold_risk_critic,\n",
    "                            env,\n",
    "                            episodes= 50000,\n",
    "                            batch_size = 128,\n",
    "                            lr = 0.0001,\n",
    "                            tau=0.00001,\n",
    "                            epsilon = 1,\n",
    "                            epsilon_decay = 0.998,\n",
    "                            discount = 1,\n",
    "                            eval_freq = 100,\n",
    "                            min_epsilon = 0.05,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3b36c-7bdb-4a69-9159-41eab4f7f0a9",
   "metadata": {},
   "source": [
    "#### Hot-Start Model:\n",
    "1. Generate Training Data\n",
    "2. Hot start actor\n",
    "3. Hot start critic (still not very good)\n",
    "4. Do DDPG on pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc6a52-0727-4db0-af81-2a240768279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "hot_actor = Actor(4,1)\n",
    "hot_cost_critic = DDPG_Cost_Critic(4,1)\n",
    "hot_risk_critic = DDPG_Risk_Critic(4,1)\n",
    "\n",
    "# Generate data\n",
    "X, y = hot_start_gen_actor_samples(env,n_paths = 100)\n",
    "\n",
    "# BC Train actor\n",
    "hot_actor, loss_hist = hot_start_actor(actor, X, y, lr=0.01, batch_size=32, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1fac9-03d5-4b7e-8042-9d10cfe5e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot start critic\n",
    "hot_obj, hot_q_guess = hot_start_critic_q_func(actor,\n",
    "                                        cost_critic,\n",
    "                                        risk_critic,\n",
    "                                        env,\n",
    "                                        episodes= 10000,\n",
    "                                        batch_size = 128,\n",
    "                                        lr = 0.01,\n",
    "                                        tau=0.000025,\n",
    "                                        epsilon = 0.8,\n",
    "                                        epsilon_decay = 0.995,\n",
    "                                        discount = 1,\n",
    "                                        eval_freq = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e31c6-0a46-4135-a0d2-1f15e7a07c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "hot_obj_train_hist, _ = DDPG_train(hot_actor,\n",
    "                                    hot_cost_critic,\n",
    "                                    hot_risk_critic,\n",
    "                                    env,\n",
    "                                    episodes= 20000,\n",
    "                                    batch_size = 128,\n",
    "                                    lr = 0.0001,\n",
    "                                    tau=0.00001,\n",
    "                                    epsilon = 0.1,\n",
    "                                    epsilon_decay = 0.9995,\n",
    "                                    discount = 1,\n",
    "                                    eval_freq = 100,\n",
    "                                    min_epsilon = 0.05,\n",
    "                                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
